PROJECT SNAPSHOT
==================================================


================================================================================
FILE: ./Config_Codespacd.py
================================================================================

#!/usr/bin/env python3
"""
Setup script for GitHub Codespace
Configures the Trading Application environment
"""
import os
import sys
import subprocess
import sqlite3
from pathlib import Path

def create_directories():
    """Create necessary directories"""
    directories = [
        'logs',
        'backups',
        'project_documentation',
        'updates',
        '.update_state'
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"‚úì Created {directory}/")

def update_service_paths():
    """Update paths in service files from Colab to Codespace format"""
    services = [
        'coordination_service.py',
        'security_scanner.py',
        'pattern_analysis.py',
        'technical_analysis.py',
        'paper_trading.py',
        'pattern_recognition_service.py',
        'news_service.py',
        'reporting_service.py',
        'web_dashboard.py',
        'hybrid_manager.py',
        'database_migration.py',
        'diagnostic_toolkit.py'
    ]
    
    replacements = [
        ('./trading_system.db', './trading_system.db'),
        ('./logs/', './logs/'),
        ('./backups/', './backups/'),
        ('/conten./', './'),
        ('./', './')
    ]
    
    for service in services:
        if Path(service).exists():
            with open(service, 'r') as f:
                content = f.read()
            
            original_content = content
            for old_path, new_path in replacements:
                content = content.replace(old_path, new_path)
            
            # Remove Colab-specific imports
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if 'from google.colab import' not in line and 'import google.colab' not in line:
                    filtered_lines.append(line)
                else:
                    filtered_lines.append(f"# {line}  # Removed for Codespace")
            
            content = '\n'.join(filtered_lines)
            
            if content != original_content:
                with open(service, 'w') as f:
                    f.write(content)
                print(f"‚úì Updated paths in {service}")

def initialize_database():
    """Initialize the database"""
    if Path('database_migration.py').exists():
        print("\nüîß Initializing database...")
        result = subprocess.run([sys.executable, 'database_migration.py'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úì Database initialized successfully")
        else:
            print(f"‚ö†Ô∏è  Database initialization warning: {result.stderr}")
    else:
        print("‚ö†Ô∏è  database_migration.py not found")

def create_requirements_file():
    """Create requirements.txt if it doesn't exist"""
    if not Path('requirements.txt').exists():
        requirements = """flask==3.0.0
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
scikit-learn==1.3.2
yfinance==0.2.33
alpaca-py==0.21.1
psutil==5.9.6
python-dateutil==2.8.2
pytz==2023.3
beautifulsoup4==4.12.2
"""
        with open('requirements.txt', 'w') as f:
            f.write(requirements)
        print("‚úì Created requirements.txt")

def create_startup_script():
    """Create a quick startup script"""
    startup_content = """#!/bin/bash
# Quick startup script for Trading Application

echo "üöÄ Starting Trading Application Services..."
python hybrid_manager.py
"""
    
    with open('start_trading.sh', 'w') as f:
        f.write(startup_content)
    
    os.chmod('start_trading.sh', 0o755)
    print("‚úì Created start_trading.sh")

def main():
    print("üîß Setting up Trading Application for GitHub Codespaces\n")
    
    # Check if we're in the right directory
    if not any(Path(f).exists() for f in ['coordination_service.py', 'hybrid_manager.py']):
        print("‚ö†Ô∏è  Warning: Trading application files not found in current directory")
        print("   Make sure you're in the root of your Trading_Application repository")
        response = input("\nContinue anyway? (y/n): ")
        if response.lower() != 'y':
            return
    
    create_directories()
    create_requirements_file()
    update_service_paths()
    initialize_database()
    create_startup_script()
    
    print("\n‚úÖ Codespace setup complete!")
    print("\nüìã Next steps:")
    print("1. Install requirements: pip install -r requirements.txt")
    print("2. Run diagnostic check: python diagnostic_toolkit.py --report")
    print("3. Start services: python hybrid_manager.py")
    print("   OR use: ./start_trading.sh")
    print("\nüí° Tip: The web dashboard will be available at port 8080")

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Google_drive_auth.py
================================================================================

# drive_auth.py - Google Drive Authentication Module
import json
import os
from pathlib import Path
from google.colab import userdata
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
import io
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload

class ColabDriveManager:
    def __init__(self):
        self.service = None
        self.project_folder_id = None
        self.project_root = None
        self._authenticate()
        self._setup_project_structure()

    def _authenticate(self):
        """Authenticate using service account credentials from Colab secrets"""
        try:
            # Get service account credentials from Colab secrets
            service_account_info = {
                "type": "service_account",
                "project_id": userdata.get('GOOGLE_PROJECT_ID'),
                "private_key_id": userdata.get('GOOGLE_PRIVATE_KEY_ID'),
                "private_key": userdata.get('GOOGLE_PRIVATE_KEY').replace('\\n', '\n'),
                "client_email": userdata.get('GOOGLE_CLIENT_EMAIL'),
                "client_id": userdata.get('GOOGLE_CLIENT_ID'),
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "client_x509_cert_url": f"https://www.googleapis.com/robot/v1/metadata/x509/{userdata.get('GOOGLE_CLIENT_EMAIL')}"
            }

            # Create credentials
            credentials = Credentials.from_service_account_info(
                service_account_info,
                scopes=['https://www.googleapis.com/auth/drive']
            )

            # Build the service
            self.service = build('drive', 'v3', credentials=credentials)
            print("‚úÖ Google Drive API authenticated successfully")
            return True

        except Exception as e:
            print(f"‚ùå Service account authentication failed: {e}")
            print("üîÑ Falling back to traditional Drive mounting...")
            return self._fallback_mount()

    def _fallback_mount(self):
        """Fallback to traditional Google Drive mounting"""
        try:
            from google.colab import drive
            drive.mount('./drive')

            # Set up local file system paths
            self.project_root = Path('./drive/MyDrive/TradingSystem_Phase1')
            self.project_root.mkdir(exist_ok=True)

            print("‚úÖ Google Drive mounted successfully (fallback mode)")
            return True

        except Exception as e:
            print(f"‚ùå Drive mounting failed: {e}")
            return False

    def _setup_project_structure(self):
        """Setup project folder structure"""
        if self.service:
            self._setup_api_folders()
        else:
            self._setup_local_folders()

    def _setup_api_folders(self):
        """Setup folder structure using Drive API"""
        try:
            # Find or create main project folder
            folder_name = 'TradingSystem_Phase1'

            # Search for existing folder
            results = self.service.files().list(
                q=f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder'",
                fields="files(id, name)"
            ).execute()

            if results['files']:
                self.project_folder_id = results['files'][0]['id']
                print(f"‚úÖ Found existing project folder: {folder_name}")
            else:
                # Create new folder
                folder_metadata = {
                    'name': folder_name,
                    'mimeType': 'application/vnd.google-apps.folder'
                }
                folder = self.service.files().create(body=folder_metadata).execute()
                self.project_folder_id = folder['id']
                print(f"‚úÖ Created project folder: {folder_name}")

            # Create subfolders
            subfolders = ['data', 'models', 'logs', 'config', 'backups', 'reports', 'coordination']
            for subfolder in subfolders:
                self._create_subfolder(subfolder)

        except Exception as e:
            print(f"‚ùå Error setting up API folders: {e}")

    def _create_subfolder(self, subfolder_name):
        """Create a subfolder in the project directory"""
        try:
            # Check if subfolder exists
            results = self.service.files().list(
                q=f"name='{subfolder_name}' and parents in '{self.project_folder_id}' and mimeType='application/vnd.google-apps.folder'",
                fields="files(id, name)"
            ).execute()

            if not results['files']:
                # Create subfolder
                folder_metadata = {
                    'name': subfolder_name,
                    'mimeType': 'application/vnd.google-apps.folder',
                    'parents': [self.project_folder_id]
                }
                self.service.files().create(body=folder_metadata).execute()
                print(f"   üìÅ Created subfolder: {subfolder_name}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error creating subfolder {subfolder_name}: {e}")

    def _setup_local_folders(self):
        """Setup folder structure using local file system"""
        if self.project_root:
            subfolders = ['data', 'models', 'logs', 'config', 'backups', 'reports', 'coordination']
            for subfolder in subfolders:
                (self.project_root / subfolder).mkdir(exist_ok=True)
            print("‚úÖ Local project structure created")

    def get_project_path(self, subfolder: str = None) -> Path:
        """Get path to project or subfolder"""
        if self.project_root:
            # Local file system mode
            if subfolder:
                return self.project_root / subfolder
            return self.project_root
        else:
            # API mode - return a simulated path for compatibility
            if subfolder:
                return Path(f"/drive_api/TradingSystem_Phase1/{subfolder}")
            return Path("/drive_api/TradingSystem_Phase1")

    def get_database_path(self) -> str:
        """Get the database path"""
        if self.service:
            # For API mode, we'll use a local temp file that syncs with Drive
            return "/tmp/trading.db"
        else:
            return str(self.project_root / "data" / "trading.db")

# Global instance for easy importing
drive_manager = None

def get_drive_manager():
    """Get or create global drive manager instance"""
    global drive_manager
    if drive_manager is None:
        drive_manager = ColabDriveManager()
    return drive_manager

def init_drive_connection():
    """Initialize drive connection - call this in each notebook"""
    return get_drive_manager()

# Convenience functions for easy use
def get_project_path(subfolder: str = None):
    """Get project path"""
    return get_drive_manager().get_project_path(subfolder)

def save_json(filename: str, data: dict, subfolder: str = None):
    """Save JSON data to drive"""
    dm = get_drive_manager()
    if dm.project_root:
        # Local file system mode
        if subfolder:
            path = dm.project_root / subfolder / filename
        else:
            path = dm.project_root / filename
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        return True
    return False

def load_json(filename: str, subfolder: str = None):
    """Load JSON data from drive"""
    dm = get_drive_manager()
    if dm.project_root:
        # Local file system mode
        if subfolder:
            path = dm.project_root / subfolder / filename
        else:
            path = dm.project_root / filename
        if path.exists():
            with open(path, 'r') as f:
                return json.load(f)
    return None

def get_database_path():
    """Get database path"""
    return get_drive_manager().get_database_path()

print("‚úÖ Drive Authentication Module loaded")
print("üí° Usage: drive_manager = init_drive_connection()")


================================================================================
FILE: ./README.md
================================================================================

# Trading_Application
Trading Application


================================================================================
FILE: ./Test.txt
================================================================================



================================================================================
FILE: ./coordination_service.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v106.py
Version: 1.0.6
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.6 (2025-06-22) - Made paths environment-agnostic, works in any directory
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Now works in any environment (Colab, Codespaces, local, etc.)
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time
from pathlib import Path

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path=None):
        self.app = Flask(__name__)
        self.port = port
        
        # Determine paths based on environment
        self.base_dir = Path.cwd()  # Current working directory
        
        # Set database path
        if db_path:
            self.db_path = db_path
        else:
            # Look for existing database or create in current directory
            if Path('/content/trading_system.db').exists():
                self.db_path = '/content/trading_system.db'
            elif (self.base_dir / 'trading_system.db').exists():
                self.db_path = str(self.base_dir / 'trading_system.db')
            else:
                self.db_path = str(self.base_dir / 'trading_system.db')
        
        print(f"Using database: {self.db_path}")
        
        # Setup logging with environment-appropriate paths
        self.log_dir = self.base_dir / 'logs'
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(self.db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        """Setup logging with environment-appropriate paths"""
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory in current working directory
        self.log_dir.mkdir(exist_ok=True)
        
        log_file = self.log_dir / 'coordination_service.log'
        handler = logging.FileHandler(str(log_file))
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        logger.info(f"Logging to: {log_file}")
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                'status': 'healthy', 
                'service': 'coordination',
                'environment': {
                    'base_dir': str(self.base_dir),
                    'log_dir': str(self.log_dir),
                    'db_path': self.db_path
                }
            }), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.logger.info(f"Working directory: {self.base_dir}")
        self.logger.info(f"Database path: {self.db_path}")
        self.logger.info(f"Log directory: {self.log_dir}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()

================================================================================
FILE: ./coordination_service_backup_20250622_133933.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()

================================================================================
FILE: ./coordination_service_backup_20250622_134230.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()


================================================================================
FILE: ./coordination_service_v105.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()


================================================================================
FILE: ./create_project_snapshot.py
================================================================================

import os
import fnmatch

# Define patterns to include and exclude
include_patterns = ['*.py', '*.md', '*.txt', '*.json', '*.yml', '*.yaml', 'requirements.txt', '.env.example']
exclude_dirs = ['.git', '__pycache__', 'backups', 'logs', '.vscode', 'venv', 'env']

def should_include_file(filepath):
    """Check if file should be included based on patterns"""
    filename = os.path.basename(filepath)
    return any(fnmatch.fnmatch(filename, pattern) for pattern in include_patterns)

def create_project_snapshot(output_file='project_snapshot.txt'):
    """Create a comprehensive snapshot of the project"""
    with open(output_file, 'w') as f:
        # Write header
        f.write("PROJECT SNAPSHOT\n")
        f.write("=" * 50 + "\n\n")
        
        # Walk through directory tree
        for root, dirs, files in os.walk('.'):
            # Remove excluded directories
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            # Process files
            for file in sorted(files):
                filepath = os.path.join(root, file)
                if should_include_file(filepath):
                    f.write(f"\n{'=' * 80}\n")
                    f.write(f"FILE: {filepath}\n")
                    f.write(f"{'=' * 80}\n\n")
                    
                    try:
                        with open(filepath, 'r', encoding='utf-8') as src:
                            f.write(src.read())
                    except Exception as e:
                        f.write(f"Error reading file: {e}\n")
                    
                    f.write("\n")
        
        print(f"Project snapshot created: {output_file}")

if __name__ == "__main__":
    create_project_snapshot()


================================================================================
FILE: ./database_migration.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM DATABASE MIGRATION
Service: Database Migration Script
Version: 1.0.5
Last Updated: 2025-06-20

REVISION HISTORY:
- v1.0.6 (2025-06-20) db_path ./trading_database.db
- v1.0.5 (2025-06-20) - Fixed verify_schema connection handling, ensured consistent table naming
- v1.0.4 (2025-06-19) - Enhanced startup coordination and health check improvements  
- v1.0.3 (2025-06-17) - Initial database schema with comprehensive trading tables

This script creates and manages the SQLite database schema for the trading system.
It includes tables for:
- Service coordination and registry
- Security scanning results
- Pattern analysis
- Technical indicators
- ML model predictions
- Strategy evaluations
- Risk metrics
- Orders and executions
- News sentiment analysis
"""

import sqlite3
import logging
from datetime import datetime
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('database_migration')

class DatabaseMigration:
    def __init__(self, db_path='./trading_system.db'):
        self.db_path = './trading_system.db'
        self.tables_created = []
        
    def create_connection(self):
        """Create database connection"""
        return sqlite3.connect(self.db_path)
    
    def execute_migration(self):
        """Execute all database migrations"""
        logger.info(f"Starting database migration for {self.db_path}")
        
        try:
            # Create all tables
            self.create_service_coordination_table()
            self.create_scanning_results_table()
            self.create_pattern_analysis_table()
            self.create_technical_indicators_table()
            self.create_ml_predictions_table()
            self.create_strategy_evaluations_table()
            self.create_risk_metrics_table()
            self.create_orders_table()
            self.create_news_sentiment_table()
            
            # Verify schema
            self.verify_schema()
            
            logger.info("Database migration completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Database migration failed: {e}")
            return False
    
    def create_service_coordination_table(self):
        """Create service coordination table for service registry"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS service_coordination (
                service_name TEXT PRIMARY KEY,
                host TEXT NOT NULL,
                port INTEGER NOT NULL,
                status TEXT NOT NULL,
                last_heartbeat TIMESTAMP,
                start_time TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('service_coordination')
        logger.info("Created service_coordination table")
    
    def create_scanning_results_table(self):
        """Create table for security scanner results"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scanning_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                scan_timestamp TIMESTAMP NOT NULL,
                price REAL,
                volume INTEGER,
                change_percent REAL,
                relative_volume REAL,
                market_cap REAL,
                scan_type TEXT,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes for performance
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_scanning_symbol 
            ON scanning_results(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_scanning_timestamp 
            ON scanning_results(scan_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('scanning_results')
        logger.info("Created scanning_results table")
    
    def create_pattern_analysis_table(self):
        """Create table for pattern analysis results"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pattern_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                pattern_type TEXT NOT NULL,
                pattern_name TEXT NOT NULL,
                confidence REAL,
                entry_price REAL,
                stop_loss REAL,
                target_price REAL,
                timeframe TEXT,
                detection_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pattern_symbol 
            ON pattern_analysis(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pattern_timestamp 
            ON pattern_analysis(detection_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('pattern_analysis')
        logger.info("Created pattern_analysis table")
    
    def create_technical_indicators_table(self):
        """Create table for technical indicators"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS technical_indicators (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                indicator_name TEXT NOT NULL,
                indicator_value REAL,
                signal TEXT,
                timeframe TEXT,
                calculation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_technical_symbol 
            ON technical_indicators(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_technical_indicator 
            ON technical_indicators(indicator_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('technical_indicators')
        logger.info("Created technical_indicators table")
    
    def create_ml_predictions_table(self):
        """Create table for ML model predictions"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                model_name TEXT NOT NULL,
                model_version TEXT,
                prediction_type TEXT NOT NULL,
                prediction_value REAL,
                confidence REAL,
                features_used TEXT,
                prediction_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ml_symbol 
            ON ml_predictions(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ml_model 
            ON ml_predictions(model_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('ml_predictions')
        logger.info("Created ml_predictions table")
    
    def create_strategy_evaluations_table(self):
        """Create table for strategy evaluations"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS strategy_evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                strategy_name TEXT NOT NULL,
                entry_signal TEXT,
                entry_price REAL,
                stop_loss REAL,
                target_price REAL,
                position_size INTEGER,
                risk_reward_ratio REAL,
                expected_return REAL,
                confidence_score REAL,
                evaluation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_symbol 
            ON strategy_evaluations(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_name 
            ON strategy_evaluations(strategy_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('strategy_evaluations')
        logger.info("Created strategy_evaluations table")
    
    def create_risk_metrics_table(self):
        """Create table for risk management metrics"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS risk_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT,
                portfolio_value REAL,
                position_size REAL,
                risk_amount REAL,
                risk_percentage REAL,
                max_positions INTEGER,
                current_positions INTEGER,
                daily_loss_limit REAL,
                current_daily_loss REAL,
                var_95 REAL,
                sharpe_ratio REAL,
                calculation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_risk_timestamp 
            ON risk_metrics(calculation_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('risk_metrics')
        logger.info("Created risk_metrics table")
    
    def create_orders_table(self):
        """Create table for orders and executions"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS orders (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                order_id TEXT UNIQUE NOT NULL,
                symbol TEXT NOT NULL,
                order_type TEXT NOT NULL,
                side TEXT NOT NULL,
                quantity INTEGER NOT NULL,
                price REAL,
                stop_price REAL,
                status TEXT NOT NULL,
                filled_quantity INTEGER DEFAULT 0,
                average_fill_price REAL,
                commission REAL,
                strategy_name TEXT,
                entry_reason TEXT,
                exit_reason TEXT,
                created_timestamp TIMESTAMP NOT NULL,
                updated_timestamp TIMESTAMP,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_symbol 
            ON orders(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_status 
            ON orders(status)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_order_id 
            ON orders(order_id)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('orders')
        logger.info("Created orders table")
    
    def create_news_sentiment_table(self):
        """Create table for news sentiment analysis"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_sentiment (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                headline TEXT NOT NULL,
                source TEXT,
                article_date TIMESTAMP NOT NULL,
                sentiment_score REAL,
                sentiment_label TEXT,
                relevance_score REAL,
                impact_score REAL,
                analysis_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_symbol 
            ON news_sentiment(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_date 
            ON news_sentiment(article_date)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('news_sentiment')
        logger.info("Created news_sentiment table")
    
    def verify_schema(self):
        """Verify all tables were created successfully"""
        conn = None
        try:
            conn = self.create_connection()
            cursor = conn.cursor()
            
            # Get list of tables
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
            """)
            
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            expected_tables = [
                'service_coordination',
                'scanning_results',
                'pattern_analysis',
                'technical_indicators',
                'ml_predictions',
                'strategy_evaluations',
                'risk_metrics',
                'orders',
                'news_sentiment'
            ]
            
            missing_tables = set(expected_tables) - set(existing_tables)
            
            if missing_tables:
                logger.error(f"Missing tables: {missing_tables}")
                return False
            
            logger.info(f"Schema verification successful. Found {len(existing_tables)} tables.")
            logger.info(f"Tables: {', '.join(sorted(existing_tables))}")
            
            # Verify service_coordination table structure
            cursor.execute("PRAGMA table_info(service_coordination)")
            columns = [row[1] for row in cursor.fetchall()]
            logger.info(f"service_coordination columns: {columns}")
            
            return True
            
        except Exception as e:
            logger.error(f"Schema verification failed: {e}")
            return False
        finally:
            if conn:
                conn.close()

def main():
    """Main function to run migration"""
    logger.info("Starting Trading System Database Migration v1.0.5")
    
    # Check if database already exists
    db_path = Path('trading_system.db')
    if db_path.exists():
        logger.warning(f"Database {db_path} already exists. Migration will update schema if needed.")
    
    # Run migration
    migration = DatabaseMigration()
    success = migration.execute_migration()
    
    if success:
        logger.info("Database migration completed successfully!")
        logger.info(f"Created/verified tables: {', '.join(migration.tables_created)}")
    else:
        logger.error("Database migration failed!")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())

================================================================================
FILE: ./database_utils.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM - DATABASE UTILITIES
Version: 1.0.0
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.0 (2025-06-19) - Initial implementation to fix database locking issues

Database Utilities - Provides centralized database connection management
with retry logic, WAL mode, and proper error handling to prevent locking issues
"""

import sqlite3
import time
import logging
import os
from contextlib import contextmanager
from typing import Optional, Any, List, Tuple
import threading
import random

# Global lock for database operations
_db_lock = threading.Lock()

class DatabaseManager:
    """Centralized database management with retry logic and WAL mode"""
    
    def __init__(self, db_path: str = './trading_system.db'):
        self.db_path = db_path
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Connection settings for better concurrency
        self.timeout = 30.0  # Increase timeout to 30 seconds
        self.max_retries = 5
        self.retry_delay_base = 0.1  # Base delay in seconds
        self.retry_delay_max = 2.0   # Maximum delay in seconds
        
        # Initialize database with WAL mode
        self._initialize_database()
    
    def _initialize_database(self):
        """Initialize database with WAL mode for better concurrency"""
        try:
            conn = sqlite3.connect(self.db_path, timeout=self.timeout)
            
            # Enable WAL mode for better concurrent access
            conn.execute("PRAGMA journal_mode=WAL")
            
            # Optimize for concurrent access
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            
            # Enable foreign keys
            conn.execute("PRAGMA foreign_keys=ON")
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Database initialized with WAL mode at {self.db_path}")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
            raise
    
    @contextmanager
    def get_connection(self):
        """Get a database connection with automatic retry and cleanup"""
        conn = None
        attempt = 0
        
        while attempt < self.max_retries:
            try:
                # Use the global lock for write operations
                with _db_lock:
                    conn = sqlite3.connect(self.db_path, timeout=self.timeout)
                    
                    # Set pragmas for each connection
                    conn.execute("PRAGMA foreign_keys=ON")
                    conn.execute("PRAGMA journal_mode=WAL")
                    
                    # Set row factory for dict-like access
                    conn.row_factory = sqlite3.Row
                    
                    yield conn
                    
                    # Successful completion
                    conn.commit()
                    return
                    
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e):
                    attempt += 1
                    if attempt < self.max_retries:
                        # Exponential backoff with jitter
                        delay = min(
                            self.retry_delay_base * (2 ** attempt) + random.uniform(0, 0.1),
                            self.retry_delay_max
                        )
                        self.logger.warning(
                            f"Database locked, retry {attempt}/{self.max_retries} "
                            f"after {delay:.2f}s"
                        )
                        time.sleep(delay)
                    else:
                        self.logger.error(f"Database locked after {self.max_retries} retries")
                        raise
                else:
                    raise
                    
            except Exception as e:
                self.logger.error(f"Database error: {e}")
                if conn:
                    conn.rollback()
                raise
                
            finally:
                if conn:
                    conn.close()
    
    def execute_with_retry(self, query: str, params: Optional[Tuple] = None) -> Optional[sqlite3.Cursor]:
        """Execute a query with automatic retry logic"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            return cursor
    
    def executemany_with_retry(self, query: str, params_list: List[Tuple]) -> Optional[sqlite3.Cursor]:
        """Execute many queries with automatic retry logic"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.executemany(query, params_list)
            return cursor
    
    def fetchone_with_retry(self, query: str, params: Optional[Tuple] = None) -> Optional[sqlite3.Row]:
        """Fetch one row with automatic retry logic"""
        cursor = self.execute_with_retry(query, params)
        return cursor.fetchone() if cursor else None
    
    def fetchall_with_retry(self, query: str, params: Optional[Tuple] = None) -> List[sqlite3.Row]:
        """Fetch all rows with automatic retry logic"""
        cursor = self.execute_with_retry(query, params)
        return cursor.fetchall() if cursor else []
    
    def insert_with_retry(self, table: str, data: dict) -> Optional[int]:
        """Insert data into table with retry logic"""
        columns = ', '.join(data.keys())
        placeholders = ', '.join(['?' for _ in data])
        query = f"INSERT INTO {table} ({columns}) VALUES ({placeholders})"
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(query, tuple(data.values()))
            return cursor.lastrowid
    
    def update_with_retry(self, table: str, data: dict, where_clause: str, where_params: Tuple) -> int:
        """Update data in table with retry logic"""
        set_clause = ', '.join([f"{k} = ?" for k in data.keys()])
        query = f"UPDATE {table} SET {set_clause} WHERE {where_clause}"
        params = tuple(data.values()) + where_params
        
        cursor = self.execute_with_retry(query, params)
        return cursor.rowcount if cursor else 0
    
    def table_exists(self, table_name: str) -> bool:
        """Check if a table exists"""
        query = """
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name=?
        """
        result = self.fetchone_with_retry(query, (table_name,))
        return result is not None
    
    def get_table_info(self, table_name: str) -> List[dict]:
        """Get information about table columns"""
        query = f"PRAGMA table_info({table_name})"
        rows = self.fetchall_with_retry(query)
        return [dict(row) for row in rows] if rows else []
    
    def vacuum_database(self):
        """Vacuum database to optimize performance"""
        try:
            # VACUUM cannot be run within a transaction
            conn = sqlite3.connect(self.db_path, timeout=self.timeout)
            conn.execute("VACUUM")
            conn.close()
            self.logger.info("Database vacuumed successfully")
        except Exception as e:
            self.logger.error(f"Error vacuuming database: {e}")
    
    def checkpoint_wal(self):
        """Checkpoint WAL to main database file"""
        try:
            with self.get_connection() as conn:
                conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                self.logger.info("WAL checkpoint completed")
        except Exception as e:
            self.logger.error(f"Error checkpointing WAL: {e}")


# Global database manager instance
_db_manager = None


def get_database_manager(db_path: str = './trading_system.db') -> DatabaseManager:
    """Get or create the global database manager instance"""
    global _db_manager
    
    if _db_manager is None:
        _db_manager = DatabaseManager(db_path)
    
    return _db_manager


# Convenience functions for backward compatibility
@contextmanager
def get_db_connection(db_path: str = './trading_system.db'):
    """Get a database connection with automatic retry and cleanup"""
    manager = get_database_manager(db_path)
    with manager.get_connection() as conn:
        yield conn


def execute_with_retry(query: str, params: Optional[Tuple] = None, 
                      db_path: str = './trading_system.db') -> Optional[sqlite3.Cursor]:
    """Execute a query with automatic retry logic"""
    manager = get_database_manager(db_path)
    return manager.execute_with_retry(query, params)


def save_with_retry(table: str, data: dict, 
                   db_path: str = './trading_system.db') -> Optional[int]:
    """Save data to table with automatic retry logic"""
    manager = get_database_manager(db_path)
    return manager.insert_with_retry(table, data)


# Example usage for services
class DatabaseServiceMixin:
    """Mixin class for services to use database utilities"""
    
    def __init__(self, db_path: str = './trading_system.db'):
        self.db_manager = get_database_manager(db_path)
        self.db_path = db_path
    
    def save_to_database(self, table: str, data: dict) -> bool:
        """Save data to database with retry logic"""
        try:
            row_id = self.db_manager.insert_with_retry(table, data)
            return row_id is not None
        except Exception as e:
            self.logger.error(f"Error saving to {table}: {e}")
            return False
    
    def update_database(self, table: str, data: dict, 
                       where_clause: str, where_params: Tuple) -> bool:
        """Update database with retry logic"""
        try:
            rows_affected = self.db_manager.update_with_retry(
                table, data, where_clause, where_params
            )
            return rows_affected > 0
        except Exception as e:
            self.logger.error(f"Error updating {table}: {e}")
            return False
    
    def query_database(self, query: str, params: Optional[Tuple] = None) -> List[dict]:
        """Query database with retry logic"""
        try:
            rows = self.db_manager.fetchall_with_retry(query, params)
            return [dict(row) for row in rows] if rows else []
        except Exception as e:
            self.logger.error(f"Error querying database: {e}")
            return []


if __name__ == "__main__":
    # Test the database utilities
    logging.basicConfig(level=logging.INFO)
    
    print("Testing Database Utilities...")
    
    # Initialize database manager
    db_manager = get_database_manager()
    
    # Test connection
    print("\n1. Testing connection with retry...")
    with db_manager.get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 5")
        tables = cursor.fetchall()
        print(f"   Found {len(tables)} tables")
        for table in tables:
            print(f"   - {table['name']}")
    
    # Test retry logic
    print("\n2. Testing retry logic...")
    try:
        result = db_manager.fetchone_with_retry(
            "SELECT COUNT(*) as count FROM selected_securities"
        )
        if result:
            print(f"   Selected securities count: {result['count']}")
    except Exception as e:
        print(f"   Error: {e}")
    
    print("\nDatabase utilities test completed!")


================================================================================
FILE: ./diagnose-paths.py
================================================================================

#!/usr/bin/env python3
"""
Diagnose and fix path issues in Trading Application
"""
import os
import re
from pathlib import Path

def find_path_issues():
    """Find all files with path issues"""
    print("üîç Scanning for path issues...\n")
    
    issues_found = []
    
    # Common problematic patterns
    patterns = [
        r'["\']\.?/?trading_system/',  # ./trading_system/ or /trading_system/
        r'["\']\.?/?content/',          # ../ or ./
        r'trading_system\.db',          # database references
    ]
    
    # Scan all Python files
    for py_file in Path('.').glob('*.py'):
        with open(py_file, 'r') as f:
            content = f.read()
            lines = content.split('\n')
        
        file_issues = []
        for i, line in enumerate(lines, 1):
            for pattern in patterns:
                if re.search(pattern, line):
                    file_issues.append({
                        'line_num': i,
                        'line': line.strip(),
                        'pattern': pattern
                    })
        
        if file_issues:
            issues_found.append({
                'file': str(py_file),
                'issues': file_issues
            })
    
    return issues_found

def show_issues(issues):
    """Display found issues"""
    if not issues:
        print("‚úÖ No path issues found!")
        return
    
    print(f"‚ùå Found path issues in {len(issues)} files:\n")
    
    for file_info in issues:
        print(f"\nüìÑ {file_info['file']}:")
        for issue in file_info['issues']:
            print(f"   Line {issue['line_num']}: {issue['line']}")

def suggest_fixes(issues):
    """Suggest fixes for found issues"""
    if not issues:
        return
    
    print("\n" + "="*60)
    print("üîß SUGGESTED FIXES:")
    print("="*60)
    
    print("\nThe issue is that your files are looking for a 'trading_system' subdirectory")
    print("that doesn't exist. Your files are in the current directory.\n")
    
    print("Run these commands to fix:\n")
    
    # Generate sed commands for each file
    fixed_files = set()
    for file_info in issues:
        filename = file_info['file']
        if filename not in fixed_files:
            print(f"# Fix {filename}")
            print(f"sed -i 's|trading_system/||g' {filename}")
            print(f"sed -i 's|./|./|g' {filename}")
            fixed_files.add(filename)
    
    print("\n# Or fix all at once:")
    print("for f in *.py; do")
    print("  sed -i 's|trading_system/||g' \"$f\"")
    print("  sed -i 's|./|./|g' \"$f\"")
    print("done")

def main():
    print("üè• Trading Application Path Diagnostic Tool\n")
    
    # Find issues
    issues = find_path_issues()
    
    # Show what was found
    show_issues(issues)
    
    # Suggest fixes
    suggest_fixes(issues)
    
    # Show current directory structure
    print("\n" + "="*60)
    print("üìÅ YOUR CURRENT DIRECTORY STRUCTURE:")
    print("="*60)
    print(f"Current directory: {os.getcwd()}")
    print("\nFiles in current directory:")
    for item in sorted(os.listdir('.')):
        if not item.startswith('.'):
            print(f"  - {item}")

if __name__ == "__main__":
    main()

================================================================================
FILE: ./diagnostic_toolkit.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC TOOLKIT - MAIN ORCHESTRATOR
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic toolkit orchestrator

USAGE: 
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check only
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --integration     # Service integration test only
  python diagnostic_toolkit.py --processes       # Process/port check only
  python diagnostic_toolkit.py --service NAME    # Single service focus
  python diagnostic_toolkit.py --report          # Generate detailed report

PURPOSE: Unified diagnostic toolkit for comprehensive Trading System health analysis
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Import diagnostic modules
try:
    from diagnostic_service_integration import ServiceIntegrationDiagnostic
    from diagnostic_log_analysis import LogAnalysisDiagnostic  
    from diagnostic_process_ports import ProcessPortDiagnostic
except ImportError as e:
    print(f"‚ùå Error importing diagnostic modules: {e}")
    print("   Ensure all diagnostic scripts are in the same directory")
    sys.exit(1)

class TradingSystemDiagnosticToolkit:
    """Comprehensive diagnostic toolkit orchestrator"""
    
    def __init__(self):
        self.timestamp = datetime.now()
        self.results = {}
        
        # Initialize diagnostic modules
        self.integration_diagnostic = ServiceIntegrationDiagnostic()
        self.log_diagnostic = LogAnalysisDiagnostic()
        self.process_diagnostic = ProcessPortDiagnostic()
        
        # Output directory
        self.output_dir = Path('./diagnostic_reports')
        self.output_dir.mkdir(exist_ok=True)
    
    def run_comprehensive_diagnostic(self, options: Dict) -> Dict:
        """Run complete comprehensive diagnostic"""
        print("üè• TRADING SYSTEM COMPREHENSIVE DIAGNOSTIC TOOLKIT")
        print("=" * 70)
        print(f"Started: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Mode: {'Quick' if options.get('quick') else 'Comprehensive'}")
        
        if options.get('service_filter'):
            print(f"Service Focus: {options['service_filter']}")
        
        comprehensive_results = {
            "diagnostic_metadata": {
                "timestamp": self.timestamp.isoformat(),
                "version": "1.0.0",
                "mode": "quick" if options.get('quick') else "comprehensive",
                "service_filter": options.get('service_filter'),
                "modules_run": []
            }
        }
        
        # 1. Process and Port Analysis (Always run first)
        if not options.get('logs_only') and not options.get('integration_only'):
            print(f"\n{'='*20} PHASE 1: PROCESS & PORT ANALYSIS {'='*20}")
            try:
                process_results = self.process_diagnostic.run_full_diagnostic()
                comprehensive_results["process_analysis"] = process_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("process_ports")
            except Exception as e:
                print(f"‚ùå Process analysis failed: {e}")
                comprehensive_results["process_analysis"] = {"error": str(e)}
        
        # 2. Service Integration Testing
        if not options.get('logs_only') and not options.get('processes_only'):
            print(f"\n{'='*20} PHASE 2: SERVICE INTEGRATION TESTING {'='*18}")
            try:
                integration_results = self.integration_diagnostic.run_full_diagnostic()
                comprehensive_results["integration_analysis"] = integration_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("service_integration")
            except Exception as e:
                print(f"‚ùå Integration analysis failed: {e}")
                comprehensive_results["integration_analysis"] = {"error": str(e)}
        
        # 3. Log Analysis (Skip in quick mode unless specifically requested)
        if not options.get('quick') or options.get('logs_only'):
            if not options.get('integration_only') and not options.get('processes_only'):
                print(f"\n{'='*20} PHASE 3: LOG ANALYSIS {'='*31}")
                try:
                    log_results = self.log_diagnostic.run_full_analysis(
                        service_filter=options.get('service_filter'),
                        errors_only=options.get('errors_only', False),
                        last_minutes=options.get('last_minutes')
                    )
                    comprehensive_results["log_analysis"] = log_results
                    comprehensive_results["diagnostic_metadata"]["modules_run"].append("log_analysis")
                except Exception as e:
                    print(f"‚ùå Log analysis failed: {e}")
                    comprehensive_results["log_analysis"] = {"error": str(e)}
        
        # 4. Comprehensive Analysis and Recommendations
        print(f"\n{'='*20} PHASE 4: COMPREHENSIVE ANALYSIS {'='*23}")
        comprehensive_analysis = self.generate_comprehensive_analysis(comprehensive_results)
        comprehensive_results["comprehensive_analysis"] = comprehensive_analysis
        
        # 5. Save Report (if requested)
        if options.get('save_report'):
            report_path = self.save_diagnostic_report(comprehensive_results)
            comprehensive_results["report_path"] = str(report_path)
        
        return comprehensive_results
    
    def generate_comprehensive_analysis(self, results: Dict) -> Dict:
        """Generate unified analysis across all diagnostic modules"""
        print("üß† COMPREHENSIVE SYSTEM ANALYSIS")
        print("-" * 40)
        
        analysis = {
            "overall_system_health": "unknown",
            "confidence_level": "unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": [],
            "system_scores": {},
            "health_indicators": {},
            "summary": {}
        }
        
        # Collect health indicators from each module
        process_health = self.extract_process_health(results.get("process_analysis", {}))
        integration_health = self.extract_integration_health(results.get("integration_analysis", {}))
        log_health = self.extract_log_health(results.get("log_analysis", {}))
        
        analysis["health_indicators"] = {
            "process_health": process_health,
            "integration_health": integration_health,
            "log_health": log_health
        }
        
        # Calculate weighted system scores
        scores = []
        weights = []
        
        if process_health.get("score") is not None:
            scores.append(process_health["score"])
            weights.append(0.4)  # Process health is most important
        
        if integration_health.get("score") is not None:
            scores.append(integration_health["score"])
            weights.append(0.4)  # Integration health is equally important
        
        if log_health.get("score") is not None:
            scores.append(log_health["score"])
            weights.append(0.2)  # Log health is supporting indicator
        
        # Calculate overall score
        if scores and weights:
            overall_score = sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)
            analysis["system_scores"]["overall"] = round(overall_score, 1)
        else:
            overall_score = 0
        
        # Determine overall health
        if overall_score >= 90:
            analysis["overall_system_health"] = "excellent"
            analysis["confidence_level"] = "high"
            print("üéâ OVERALL SYSTEM HEALTH: EXCELLENT")
        elif overall_score >= 75:
            analysis["overall_system_health"] = "good"
            analysis["confidence_level"] = "high"
            print("‚úÖ OVERALL SYSTEM HEALTH: GOOD")
        elif overall_score >= 60:
            analysis["overall_system_health"] = "fair"
            analysis["confidence_level"] = "medium"
            print("üü° OVERALL SYSTEM HEALTH: FAIR")
        elif overall_score >= 40:
            analysis["overall_system_health"] = "poor"
            analysis["confidence_level"] = "high"
            print("‚ö†Ô∏è OVERALL SYSTEM HEALTH: POOR")
        else:
            analysis["overall_system_health"] = "critical"
            analysis["confidence_level"] = "high"
            print("üö® OVERALL SYSTEM HEALTH: CRITICAL")
        
        print(f"System Score: {overall_score:.1f}/100")
        
        # Collect issues and recommendations from all modules
        self.collect_unified_issues_and_recommendations(results, analysis)
        
        # Generate summary
        analysis["summary"] = {
            "total_services": 9,
            "services_healthy": process_health.get("services_responding", 0),
            "workflow_functional": integration_health.get("workflow_works", False),
            "log_errors": log_health.get("total_errors", 0),
            "critical_issues_count": len(analysis["critical_issues"]),
            "recommendations_count": len(analysis["recommendations"])
        }
        
        # Display key findings
        print(f"\nKey Findings:")
        print(f"   Services Responding: {analysis['summary']['services_healthy']}/9")
        print(f"   Workflow Functional: {'Yes' if analysis['summary']['workflow_functional'] else 'No'}")
        print(f"   Log Errors: {analysis['summary']['log_errors']}")
        print(f"   Critical Issues: {analysis['summary']['critical_issues_count']}")
        
        return analysis
    
    def extract_process_health(self, process_results: Dict) -> Dict:
        """Extract health indicators from process analysis"""
        if not process_results or "error" in process_results:
            return {"score": None, "status": "unavailable"}
        
        services_total = process_results.get("services_total", 9)
        services_responding = process_results.get("services_responding", 0)
        
        # Calculate score based on service availability
        score = (services_responding / services_total) * 100 if services_total > 0 else 0
        
        return {
            "score": round(score, 1),
            "status": process_results.get("overall_status", "unknown"),
            "services_responding": services_responding,
            "services_total": services_total,
            "hybrid_manager_running": process_results.get("hybrid_manager", {}).get("status") == "running"
        }
    
    def extract_integration_health(self, integration_results: Dict) -> Dict:
        """Extract health indicators from integration analysis"""
        if not integration_results or "error" in integration_results:
            return {"score": None, "status": "unavailable"}
        
        summary = integration_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 80, "fair": 60, "poor": 30}
        
        score = health_score_map.get(summary.get("overall_health"), 0)
        workflow_result = integration_results.get("workflow", {})
        workflow_works = workflow_result.get("status") == "completed"
        
        return {
            "score": score,
            "status": summary.get("overall_health", "unknown"),
            "workflow_works": workflow_works,
            "services_functional": summary.get("services_functional", 0)
        }
    
    def extract_log_health(self, log_results: Dict) -> Dict:
        """Extract health indicators from log analysis"""
        if not log_results or "error" in log_results:
            return {"score": None, "status": "unavailable"}
        
        summary = log_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 85, "fair": 60, "poor": 20}
        
        score = health_score_map.get(summary.get("overall_log_health"), 50)
        
        return {
            "score": score,
            "status": summary.get("overall_log_health", "unknown"),
            "total_errors": summary.get("total_errors", 0),
            "total_warnings": summary.get("total_warnings", 0)
        }
    
    def collect_unified_issues_and_recommendations(self, results: Dict, analysis: Dict):
        """Collect and unify issues and recommendations from all modules"""
        # Process issues
        process_analysis = results.get("process_analysis", {})
        if process_analysis.get("issues"):
            analysis["critical_issues"].extend(process_analysis["issues"])
        if process_analysis.get("recommendations"):
            analysis["recommendations"].extend(process_analysis["recommendations"])
        
        # Integration issues
        integration_summary = results.get("integration_analysis", {}).get("summary", {})
        if integration_summary.get("critical_issues"):
            analysis["critical_issues"].extend(integration_summary["critical_issues"])
        if integration_summary.get("recommendations"):
            analysis["recommendations"].extend(integration_summary["recommendations"])
        
        # Log issues
        log_summary = results.get("log_analysis", {}).get("summary", {})
        if log_summary.get("critical_issues"):
            analysis["critical_issues"].extend(log_summary["critical_issues"])
        if log_summary.get("recommendations"):
            analysis["recommendations"].extend(log_summary["recommendations"])
        
        # Remove duplicates while preserving order
        analysis["critical_issues"] = list(dict.fromkeys(analysis["critical_issues"]))
        analysis["recommendations"] = list(dict.fromkeys(analysis["recommendations"]))
        
        # Add unified recommendations based on patterns
        if "json_serialization" in str(analysis["critical_issues"]):
            analysis["recommendations"].insert(0, "Apply JSON serialization fix to pattern_analysis.py")
        
        if "hybrid_manager_not_running" in analysis["critical_issues"]:
            analysis["recommendations"].insert(0, "Start system: python hybrid_manager.py start")
    
    def save_diagnostic_report(self, results: Dict) -> Path:
        """Save comprehensive diagnostic report to file"""
        timestamp_str = self.timestamp.strftime('%Y%m%d_%H%M%S')
        report_filename = f"trading_system_diagnostic_{timestamp_str}.json"
        report_path = self.output_dir / report_filename
        
        try:
            with open(report_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüíæ Diagnostic report saved: {report_path}")
            
            # Also create a summary text report
            summary_path = self.output_dir / f"diagnostic_summary_{timestamp_str}.txt"
            self.create_text_summary(results, summary_path)
            
            return report_path
            
        except Exception as e:
            print(f"‚ùå Error saving report: {e}")
            return None
    
    def create_text_summary(self, results: Dict, summary_path: Path):
        """Create human-readable text summary"""
        try:
            with open(summary_path, 'w') as f:
                f.write("TRADING SYSTEM DIAGNOSTIC SUMMARY\n")
                f.write("=" * 50 + "\n")
                f.write(f"Generated: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # Overall health
                comp_analysis = results.get("comprehensive_analysis", {})
                f.write(f"Overall Health: {comp_analysis.get('overall_system_health', 'unknown').upper()}\n")
                f.write(f"System Score: {comp_analysis.get('system_scores', {}).get('overall', 'N/A')}/100\n\n")
                
                # Key metrics
                summary = comp_analysis.get("summary", {})
                f.write("Key Metrics:\n")
                f.write(f"  Services Responding: {summary.get('services_healthy', 0)}/9\n")
                f.write(f"  Workflow Functional: {'Yes' if summary.get('workflow_functional') else 'No'}\n")
                f.write(f"  Log Errors: {summary.get('log_errors', 0)}\n")
                f.write(f"  Critical Issues: {summary.get('critical_issues_count', 0)}\n\n")
                
                # Critical issues
                critical_issues = comp_analysis.get("critical_issues", [])
                if critical_issues:
                    f.write("Critical Issues:\n")
                    for issue in critical_issues:
                        f.write(f"  - {issue}\n")
                    f.write("\n")
                
                # Top recommendations
                recommendations = comp_analysis.get("recommendations", [])
                if recommendations:
                    f.write("Top Recommendations:\n")
                    for i, rec in enumerate(recommendations[:5], 1):
                        f.write(f"  {i}. {rec}\n")
                
            print(f"üìÑ Text summary saved: {summary_path}")
            
        except Exception as e:
            print(f"‚ùå Error saving text summary: {e}")
    
    def run_quick_health_check(self) -> Dict:
        """Run quick health check (process + integration only)"""
        print("‚ö° QUICK HEALTH CHECK")
        print("-" * 30)
        
        return self.run_comprehensive_diagnostic({"quick": True})

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Trading System Diagnostic Toolkit',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --service scanner # Focus on scanner service
  python diagnostic_toolkit.py --report          # Save detailed report
        """
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Quick health check (skip log analysis)')
    parser.add_argument('--logs-only', action='store_true',
                       help='Run log analysis only')
    parser.add_argument('--integration-only', action='store_true',
                       help='Run integration tests only')
    parser.add_argument('--processes-only', action='store_true',
                       help='Run process/port checks only')
    parser.add_argument('--service', 
                       help='Focus on specific service')
    parser.add_argument('--errors-only', action='store_true',
                       help='Show only errors in log analysis')
    parser.add_argument('--last-minutes', type=int,
                       help='Analyze only last N minutes of logs')
    parser.add_argument('--report', action='store_true',
                       help='Save detailed diagnostic report')
    
    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_arguments()
    
    # Convert args to options dict
    options = {
        'quick': args.quick,
        'logs_only': args.logs_only,
        'integration_only': args.integration_only,
        'processes_only': args.processes_only,
        'service_filter': args.service,
        'errors_only': args.errors_only,
        'last_minutes': args.last_minutes,
        'save_report': args.report
    }
    
    # Initialize toolkit
    toolkit = TradingSystemDiagnosticToolkit()
    
    try:
        # Run diagnostic
        if args.quick:
            results = toolkit.run_quick_health_check()
        else:
            results = toolkit.run_comprehensive_diagnostic(options)
        
        # Print completion message
        print(f"\nüèÅ DIAGNOSTIC COMPLETED")
        print("=" * 30)
        
        comp_analysis = results.get("comprehensive_analysis", {})
        overall_health = comp_analysis.get("overall_system_health", "unknown")
        
        if overall_health in ["excellent", "good"]:
            print("‚úÖ Your trading system is healthy and ready for operation!")
        elif overall_health == "fair":
            print("üü° Your trading system has minor issues but is operational.")
        else:
            print("‚ùå Your trading system has significant issues requiring attention.")
        
        # Show next steps
        recommendations = comp_analysis.get("recommendations", [])
        if recommendations:
            print(f"\nüéØ Next Steps:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        return results
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Diagnostic interrupted by user")
        return None
    except Exception as e:
        print(f"\n‚ùå Diagnostic failed: {e}")
        return None

if __name__ == "__main__":
    main()

================================================================================
FILE: ./fix_coord.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: fix_coordination_service.py
Version: 1.0.0
Last Updated: 2025-06-22

PURPOSE:
Script to properly stop old coordination service and start the fixed version
"""

import os
import sys
import time
import subprocess
import psutil
from pathlib import Path

def kill_coordination_service():
    """Kill all running instances of coordination service"""
    print("üõë Stopping all coordination service instances...")
    
    killed_count = 0
    
    # Method 1: Using psutil to find and kill processes
    try:
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = proc.info.get('cmdline', [])
                if cmdline and any('coordination_service' in str(arg) for arg in cmdline):
                    print(f"  Found process: PID {proc.info['pid']}")
                    proc.terminate()
                    killed_count += 1
                    time.sleep(0.5)
                    if proc.is_running():
                        proc.kill()  # Force kill if still running
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
    except ImportError:
        print("  psutil not available, trying alternative method...")
    
    # Method 2: Using pkill command
    try:
        subprocess.run(['pkill', '-f', 'coordination_service'], capture_output=True)
        print("  Executed pkill command")
    except:
        pass
    
    # Method 3: Find processes on port 5000
    try:
        result = subprocess.run(['lsof', '-ti:5000'], capture_output=True, text=True)
        if result.stdout:
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                if pid:
                    try:
                        subprocess.run(['kill', '-9', pid], capture_output=True)
                        print(f"  Killed process on port 5000: PID {pid}")
                        killed_count += 1
                    except:
                        pass
    except:
        pass
    
    if killed_count > 0:
        print(f"‚úÖ Stopped {killed_count} coordination service instance(s)")
    else:
        print("‚ÑπÔ∏è  No running coordination service instances found")
    
    # Wait for port to be released
    time.sleep(2)
    
def check_port_availability(port=5000):
    """Check if port is available"""
    import socket
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    result = sock.connect_ex(('localhost', port))
    sock.close()
    
    if result == 0:
        print(f"‚ùå Port {port} is still in use")
        return False
    else:
        print(f"‚úÖ Port {port} is available")
        return True

def backup_old_service():
    """Backup old coordination service"""
    old_file = Path('coordination_service.py')
    if old_file.exists():
        backup_name = f'coordination_service_backup_{time.strftime("%Y%m%d_%H%M%S")}.py'
        old_file.rename(backup_name)
        print(f"üì¶ Backed up old service to: {backup_name}")

def apply_fixed_version():
    """Copy the fixed version to coordination_service.py"""
    fixed_file = Path('coordination_service_v105.py')
    target_file = Path('coordination_service.py')
    
    if not fixed_file.exists():
        print("‚ùå Fixed version (coordination_service_v105.py) not found!")
        print("   Please ensure you've created the fixed version first.")
        return False
    
    # Copy the fixed version
    target_file.write_text(fixed_file.read_text())
    print("‚úÖ Applied fixed version of coordination service")
    return True

def verify_fix():
    """Verify the fix is applied by checking the file content"""
    service_file = Path('coordination_service.py')
    
    if not service_file.exists():
        print("‚ùå coordination_service.py not found!")
        return False
    
    content = service_file.read_text()
    
    # Check for the old problematic code
    if 'service_url, service_port' in content:
        print("‚ùå Old version still in place - fix not applied!")
        return False
    
    # Check for the fixed code
    if 'host, port, status' in content:
        print("‚úÖ Fixed version verified - correct schema references found")
        return True
    
    print("‚ö†Ô∏è  Cannot verify fix - please check manually")
    return False

def start_fixed_service():
    """Start the fixed coordination service"""
    print("\nüöÄ Starting fixed coordination service...")
    
    # Start in background using subprocess
    try:
        process = subprocess.Popen(
            [sys.executable, 'coordination_service.py'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Wait a moment to see if it starts successfully
        time.sleep(3)
        
        if process.poll() is None:
            print("‚úÖ Coordination service started successfully")
            print(f"   Process PID: {process.pid}")
            
            # Check if it's responding
            time.sleep(2)
            try:
                import requests
                response = requests.get('http://localhost:5000/health', timeout=5)
                if response.status_code == 200:
                    print("‚úÖ Service is responding to health checks")
                else:
                    print("‚ö†Ô∏è  Service started but not responding properly")
            except:
                print("‚ö†Ô∏è  Cannot verify service health - check logs")
                
            return True
        else:
            print("‚ùå Service failed to start")
            stdout, stderr = process.communicate()
            if stderr:
                print(f"   Error: {stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Failed to start service: {e}")
        return False

def check_logs():
    """Display recent log entries"""
    log_file = Path('/content/logs/coordination_service.log')
    
    if log_file.exists():
        print("\nüìã Recent log entries:")
        try:
            # Get last 10 lines of log
            with open(log_file, 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-10:] if len(lines) > 10 else lines
                for line in recent_lines:
                    print(f"   {line.strip()}")
        except:
            print("   Could not read log file")
    else:
        print("\nüìã No log file found yet")

def main():
    """Main execution flow"""
    print("=" * 60)
    print("COORDINATION SERVICE FIX SCRIPT")
    print("=" * 60)
    
    # Step 1: Kill existing services
    kill_coordination_service()
    
    # Step 2: Check port availability
    if not check_port_availability():
        print("\n‚ö†Ô∏è  Port 5000 is still in use. Waiting 5 seconds...")
        time.sleep(5)
        kill_coordination_service()  # Try again
        
    # Step 3: Backup old service
    backup_old_service()
    
    # Step 4: Apply fixed version
    if not apply_fixed_version():
        print("\n‚ùå Failed to apply fix. Exiting.")
        sys.exit(1)
    
    # Step 5: Verify fix
    if not verify_fix():
        print("\n‚ö†Ô∏è  Warning: Could not verify fix was applied correctly")
        response = input("Continue anyway? (y/n): ")
        if response.lower() != 'y':
            sys.exit(1)
    
    # Step 6: Start fixed service
    if start_fixed_service():
        print("\n‚úÖ SUCCESS: Fixed coordination service is running!")
        
        # Step 7: Check logs
        check_logs()
        
        print("\nüìå Next steps:")
        print("   1. Monitor logs: tail -f /content/logs/coordination_service.log")
        print("   2. Check health: curl http://localhost:5000/health")
        print("   3. Verify services: curl http://localhost:5000/services")
    else:
        print("\n‚ùå FAILED: Could not start fixed service")
        print("   Check the logs for more information")
        check_logs()

if __name__ == "__main__":
    # Install psutil if not available
    try:
        import psutil
    except ImportError:
        print("Installing psutil for better process management...")
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'psutil', '-q'], capture_output=True)
        
    main()

================================================================================
FILE: ./google_drive_service.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: google_drive_service.py
Version: 1.0.1
Last Updated: 2025-01-11

REVISION HISTORY:
- v1.0.1 (2025-01-11) - Refactored to remove fallback, conform to project methodology
- v1.0.0 (2024-12-28) - Initial version with fallback support

PURPOSE:
Provides unified Google Drive access for Trading System services.
Used by TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py and enhanced_startup_manager.py.
"""

import json
import os
import logging
from pathlib import Path
from typing import Dict, Optional, Any, List
from datetime import datetime
from google.colab import userdata
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.service_account import Credentials
import io
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaIoBaseUpload

class GoogleDriveService:
    """
    Unified Google Drive service for Trading System.
    Provides API access to Google Drive without fallback mechanisms.
    """
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """Initialize Google Drive service"""
        self.service = None
        self.project_folder_id = None
        self.logger = logger or self._setup_default_logger()
        
        # Project configuration
        self.project_name = "TradingSystem_Phase1"
        self.subfolders = [
            'data', 'models', 'logs', 'config', 
            'backups', 'reports', 'coordination', 
            'project_documentation', 'updates'
        ]
        
        # Initialize service
        self._authenticate()
        self._setup_project_structure()
    
    def _setup_default_logger(self) -> logging.Logger:
        """Setup default logger if none provided"""
        logger = logging.getLogger('GoogleDriveService')
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger
    
    def _authenticate(self):
        """Authenticate using service account credentials from Colab secrets"""
        try:
            # Build service account info from Colab secrets
            service_account_info = {
                "type": "service_account",
                "project_id": userdata.get('GOOGLE_PROJECT_ID'),
                "private_key_id": userdata.get('GOOGLE_PRIVATE_KEY_ID'),
                "private_key": userdata.get('GOOGLE_PRIVATE_KEY').replace('\\n', '\n'),
                "client_email": userdata.get('GOOGLE_CLIENT_EMAIL'),
                "client_id": userdata.get('GOOGLE_CLIENT_ID'),
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "client_x509_cert_url": f"https://www.googleapis.com/robot/v1/metadata/x509/{userdata.get('GOOGLE_CLIENT_EMAIL')}"
            }
            
            # Validate required fields
            required_fields = ['project_id', 'private_key', 'client_email', 'client_id']
            for field in required_fields:
                if not service_account_info.get(field):
                    raise ValueError(f"Missing required field: GOOGLE_{field.upper()}")
            
            # Create credentials
            credentials = Credentials.from_service_account_info(
                service_account_info,
                scopes=['https://www.googleapis.com/auth/drive']
            )
            
            # Build the service
            self.service = build('drive', 'v3', credentials=credentials)
            
            # Test authentication
            self.service.about().get(fields="user").execute()
            
            self.logger.info("‚úÖ Google Drive API authenticated successfully")
            
        except Exception as e:
            self.logger.error(f"‚ùå Google Drive authentication failed: {e}")
            raise RuntimeError(f"Failed to authenticate with Google Drive: {e}")
    
    def _setup_project_structure(self):
        """Setup project folder structure in Google Drive"""
        try:
            # Find or create main project folder
            self.project_folder_id = self._find_or_create_folder(
                self.project_name, 
                parent_id=None
            )
            self.logger.info(f"‚úÖ Project folder ready: {self.project_name}")
            
            # Create subfolders
            for subfolder in self.subfolders:
                self._find_or_create_folder(subfolder, self.project_folder_id)
                
        except Exception as e:
            self.logger.error(f"‚ùå Error setting up project structure: {e}")
            raise
    
    def _find_or_create_folder(self, folder_name: str, parent_id: Optional[str] = None) -> str:
        """Find existing folder or create new one"""
        try:
            # Build query
            query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder'"
            if parent_id:
                query += f" and parents in '{parent_id}'"
            query += " and trashed=false"
            
            # Search for existing folder
            results = self.service.files().list(
                q=query,
                fields="files(id, name)",
                pageSize=1
            ).execute()
            
            if results.get('files'):
                folder_id = results['files'][0]['id']
                self.logger.debug(f"   üìÅ Found existing folder: {folder_name}")
                return folder_id
            
            # Create new folder
            folder_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder'
            }
            if parent_id:
                folder_metadata['parents'] = [parent_id]
                
            folder = self.service.files().create(
                body=folder_metadata,
                fields='id'
            ).execute()
            
            self.logger.info(f"   üìÅ Created folder: {folder_name}")
            return folder['id']
            
        except Exception as e:
            self.logger.error(f"Error with folder {folder_name}: {e}")
            raise
    
    def get_subfolder_id(self, subfolder: str) -> str:
        """Get the ID of a subfolder"""
        if not self.project_folder_id:
            raise RuntimeError("Project folder not initialized")
            
        return self._find_or_create_folder(subfolder, self.project_folder_id)
    
    def list_files(self, folder_name: Optional[str] = None, mime_type: Optional[str] = None) -> List[Dict]:
        """List files in a folder"""
        try:
            # Determine parent folder
            if folder_name:
                parent_id = self.get_subfolder_id(folder_name)
            else:
                parent_id = self.project_folder_id
            
            # Build query
            query = f"parents in '{parent_id}' and trashed=false"
            if mime_type:
                query += f" and mimeType='{mime_type}'"
            
            # Get files
            results = self.service.files().list(
                q=query,
                fields="files(id, name, mimeType, modifiedTime, size)",
                orderBy="modifiedTime desc"
            ).execute()
            
            return results.get('files', [])
            
        except Exception as e:
            self.logger.error(f"Error listing files: {e}")
            return []
    
    def read_file(self, filename: str, subfolder: Optional[str] = None) -> bytes:
        """Read file content from Google Drive"""
        try:
            # Find the file
            file_id = self._find_file(filename, subfolder)
            if not file_id:
                raise FileNotFoundError(f"File not found: {filename}")
            
            # Download file
            request = self.service.files().get_media(fileId=file_id)
            file_content = io.BytesIO()
            downloader = MediaIoBaseDownload(file_content, request)
            
            done = False
            while not done:
                status, done = downloader.next_chunk()
                
            file_content.seek(0)
            return file_content.read()
            
        except Exception as e:
            self.logger.error(f"Error reading file {filename}: {e}")
            raise
    
    def write_file(self, filename: str, content: bytes, subfolder: Optional[str] = None, 
                   mime_type: str = 'application/octet-stream') -> str:
        """Write file to Google Drive"""
        try:
            # Get parent folder
            if subfolder:
                parent_id = self.get_subfolder_id(subfolder)
            else:
                parent_id = self.project_folder_id
            
            # Check if file exists
            existing_file_id = self._find_file(filename, subfolder)
            
            # Prepare content
            media = MediaIoBaseUpload(
                io.BytesIO(content),
                mimetype=mime_type,
                resumable=True
            )
            
            if existing_file_id:
                # Update existing file
                file_metadata = {'name': filename}
                updated_file = self.service.files().update(
                    fileId=existing_file_id,
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                self.logger.info(f"Updated file: {filename}")
                return updated_file['id']
            else:
                # Create new file
                file_metadata = {
                    'name': filename,
                    'parents': [parent_id]
                }
                created_file = self.service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                self.logger.info(f"Created file: {filename}")
                return created_file['id']
                
        except Exception as e:
            self.logger.error(f"Error writing file {filename}: {e}")
            raise
    
    def _find_file(self, filename: str, subfolder: Optional[str] = None) -> Optional[str]:
        """Find file ID by name"""
        try:
            # Get parent folder
            if subfolder:
                parent_id = self.get_subfolder_id(subfolder)
            else:
                parent_id = self.project_folder_id
            
            # Search for file
            query = f"name='{filename}' and parents in '{parent_id}' and trashed=false"
            results = self.service.files().list(
                q=query,
                fields="files(id)",
                pageSize=1
            ).execute()
            
            files = results.get('files', [])
            return files[0]['id'] if files else None
            
        except Exception as e:
            self.logger.error(f"Error finding file {filename}: {e}")
            return None
    
    def delete_file(self, filename: str, subfolder: Optional[str] = None) -> bool:
        """Delete a file from Google Drive"""
        try:
            file_id = self._find_file(filename, subfolder)
            if not file_id:
                self.logger.warning(f"File not found for deletion: {filename}")
                return False
            
            self.service.files().delete(fileId=file_id).execute()
            self.logger.info(f"Deleted file: {filename}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error deleting file {filename}: {e}")
            return False
    
    def create_backup(self, source_folder: str, backup_name: str) -> str:
        """Create a backup of a folder"""
        try:
            # Create backup folder
            backup_folder_id = self._find_or_create_folder(
                backup_name, 
                self.get_subfolder_id('backups')
            )
            
            # Get source folder contents
            source_files = self.list_files(source_folder)
            
            # Copy files to backup
            for file_info in source_files:
                if file_info['mimeType'] != 'application/vnd.google-apps.folder':
                    # Copy file
                    copy_metadata = {
                        'name': file_info['name'],
                        'parents': [backup_folder_id]
                    }
                    self.service.files().copy(
                        fileId=file_info['id'],
                        body=copy_metadata
                    ).execute()
                    
            self.logger.info(f"Created backup: {backup_name}")
            return backup_folder_id
            
        except Exception as e:
            self.logger.error(f"Error creating backup: {e}")
            raise
    
    # Convenience methods for JSON operations
    def save_json(self, filename: str, data: Dict, subfolder: Optional[str] = None) -> bool:
        """Save JSON data to Google Drive"""
        try:
            json_content = json.dumps(data, indent=2, default=str)
            self.write_file(
                filename, 
                json_content.encode('utf-8'), 
                subfolder,
                mime_type='application/json'
            )
            return True
        except Exception as e:
            self.logger.error(f"Error saving JSON {filename}: {e}")
            return False
    
    def load_json(self, filename: str, subfolder: Optional[str] = None) -> Optional[Dict]:
        """Load JSON data from Google Drive"""
        try:
            content = self.read_file(filename, subfolder)
            return json.loads(content.decode('utf-8'))
        except FileNotFoundError:
            return None
        except Exception as e:
            self.logger.error(f"Error loading JSON {filename}: {e}")
            return None
    
    def get_database_info(self) -> Dict[str, Any]:
        """Get information about the database file"""
        try:
            db_info = {
                'exists': False,
                'size': 0,
                'last_modified': None,
                'file_id': None
            }
            
            file_id = self._find_file('trading.db', 'data')
            if file_id:
                # Get file metadata
                file_meta = self.service.files().get(
                    fileId=file_id,
                    fields='size,modifiedTime'
                ).execute()
                
                db_info.update({
                    'exists': True,
                    'size': int(file_meta.get('size', 0)),
                    'last_modified': file_meta.get('modifiedTime'),
                    'file_id': file_id
                })
                
            return db_info
            
        except Exception as e:
            self.logger.error(f"Error getting database info: {e}")
            return {'exists': False, 'error': str(e)}


# Global instance management
_drive_service_instance = None

def get_drive_service(logger: Optional[logging.Logger] = None) -> GoogleDriveService:
    """Get or create global drive service instance"""
    global _drive_service_instance
    if _drive_service_instance is None:
        _drive_service_instance = GoogleDriveService(logger)
    return _drive_service_instance

def init_drive_service(logger: Optional[logging.Logger] = None) -> GoogleDriveService:
    """Initialize drive service - call this in each notebook/script"""
    return get_drive_service(logger)

# Legacy compatibility functions
def get_project_path(subfolder: Optional[str] = None) -> str:
    """Legacy: Get virtual project path for compatibility"""
    base = f"/drive_api/{get_drive_service().project_name}"
    return f"{base}/{subfolder}" if subfolder else base

def get_database_path() -> str:
    """Legacy: Get virtual database path for compatibility"""
    return f"/drive_api/{get_drive_service().project_name}/data/trading.db"

# Direct export for convenience
save_json = lambda filename, data, subfolder=None: get_drive_service().save_json(filename, data, subfolder)
load_json = lambda filename, subfolder=None: get_drive_service().load_json(filename, subfolder)

if __name__ == "__main__":
    # Test the service
    print("üöÄ Google Drive Service v1.0.1")
    print("================================")
    
    try:
        service = init_drive_service()
        print("‚úÖ Service initialized successfully")
        
        # Test listing files
        print("\nüìÅ Project structure:")
        for folder in service.subfolders:
            files = service.list_files(folder)
            print(f"   {folder}/: {len(files)} files")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


================================================================================
FILE: ./hybrid_manager.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1 - ENHANCED VERSION (CLI ONLY)
Service: Hybrid Service Manager (CLI Service Control)
Version: 3.0.2-CLI
Last Updated: 2025-06-20

REVISION HISTORY:
- v3.0.2-CLI (2025-06-20) - Removed GUI for headless operation in Google Colab
- v3.0.2 (2025-06-20) - Added database table verification, startup delays, health check verification
- v3.0.1 (2025-06-19) - Enhanced with architecture implementation requirements
- v3.0.0 (2025-06-18) - Major refactor with GUI, enhanced monitoring and orchestration

This CLI version provides:
1. Service lifecycle management
2. Real-time health monitoring
3. System metrics and performance tracking
4. Database table verification before startup
5. Command-line interface for Google Colab
"""

import subprocess
import psutil
import requests
import json
import time
import threading
import os
import sys
import sqlite3
from datetime import datetime
from pathlib import Path
import logging
import signal

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('hybrid_manager')

class ServiceManager:
    def __init__(self):
        # Updated service list to match v3.0.2
        self.services = [
            {"name": "coordination_service", "port": 5000, "process": None, "status": "Stopped", "critical": True, "startup_delay": 5},
            {"name": "security_scanner", "port": 5001, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "pattern_analysis", "port": 5002, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "technical_analysis", "port": 5003, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "paper_trading", "port": 5005, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "pattern_recognition_service", "port": 5006, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "news_service", "port": 5008, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3},
            {"name": "reporting_service", "port": 5009, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3},
            {"name": "web_dashboard", "port": 8080, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3}
        ]
        self.monitoring_active = False
        self.db_path = "./trading_system.db"
        self.running = True
        
        # Set up signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals"""
        print("\nüõë Shutdown signal received. Stopping services...")
        self.running = False
        self.stop_all_services()
        sys.exit(0)
        
    def verify_database_tables(self):
        """Verify all required database tables exist"""
        try:
            if not Path(self.db_path).exists():
                logger.error(f"Database {self.db_path} not found!")
                return False
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # List of required tables from v3.0.2
            required_tables = [
                'service_registry',
                'trading_schedule',
                'news_articles',
                'technical_indicators',
                'patterns',
                'signals',
                'trades',
                'risk_metrics',
                'ml_predictions',
                'system_events'
            ]
            
            # Check each table
            missing_tables = []
            for table in required_tables:
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,))
                if not cursor.fetchone():
                    missing_tables.append(table)
            
            conn.close()
            
            if missing_tables:
                logger.warning(f"Missing database tables: {missing_tables}")
                logger.info("Running database migration...")
                return self.run_database_migration()
            
            logger.info("All required database tables verified ‚úì")
            return True
            
        except Exception as e:
            logger.error(f"Database verification failed: {str(e)}")
            return False
    
    def run_database_migration(self):
        """Run database migration if needed"""
        try:
            migration_path = Path("./database_migration.py")
            if migration_path.exists():
                result = subprocess.run([sys.executable, str(migration_path)], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    logger.info("Database migration completed successfully")
                    return True
                else:
                    logger.error(f"Database migration failed: {result.stderr}")
                    return False
            else:
                logger.error("database_migration.py not found")
                return False
        except Exception as e:
            logger.error(f"Migration error: {str(e)}")
            return False
    
    def start_service(self, service):
        """Start an individual service with health check verification"""
        try:
            if service["process"] and service["process"].poll() is None:
                logger.info(f"{service['name']} is already running")
                return True
                
            logger.info(f"Starting {service['name']}...")
            
            # Build command based on service name
            if service["name"] == "web_dashboard":
                cmd = [sys.executable, f"./web_dashboard_service.py"]
            else:
                cmd = [sys.executable, f"./{service['name']}.py"]
                
            # Start the process
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
            
            service["process"] = process
            service["status"] = "Starting"
            service["start_time"] = datetime.now()
            
            # Wait for startup delay
            logger.info(f"Waiting {service['startup_delay']}s for {service['name']} to initialize...")
            time.sleep(service['startup_delay'])
            
            # Verify service is healthy
            if self.check_service_health(service):
                service["status"] = "Running"
                logger.info(f"‚úì {service['name']} started successfully (PID: {process.pid})")
                return True
            else:
                service["status"] = "Failed"
                if process.poll() is None:
                    process.terminate()
                    process.wait()
                logger.error(f"‚úó {service['name']} failed health check")
                return False
                
        except Exception as e:
            logger.error(f"Failed to start {service['name']}: {str(e)}")
            service["status"] = "Error"
            return False
    
    def check_service_health(self, service, max_retries=10):
        """Check if a service is healthy via HTTP endpoint"""
        url = f"http://localhost:{service['port']}/health"
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    return True
            except:
                pass
            
            if attempt < max_retries - 1:
                time.sleep(1)
        
        return False
    
    def stop_service(self, service):
        """Stop an individual service"""
        try:
            if service["process"] and service["process"].poll() is None:
                logger.info(f"Stopping {service['name']}...")
                service["process"].terminate()
                service["process"].wait(timeout=5)
                service["status"] = "Stopped"
                service["process"] = None
                logger.info(f"‚úì {service['name']} stopped")
            else:
                service["status"] = "Stopped"
                service["process"] = None
        except subprocess.TimeoutExpired:
            logger.warning(f"Force killing {service['name']}")
            service["process"].kill()
            service["process"].wait()
            service["status"] = "Stopped"
            service["process"] = None
        except Exception as e:
            logger.error(f"Error stopping {service['name']}: {str(e)}")
    
    def start_all_services(self):
        """Start all services in the correct order"""
        print("\nüöÄ STARTING TRADING SYSTEM SERVICES")
        print("=" * 60)
        
        # Verify database first
        if not self.verify_database_tables():
            print("‚ùå Database verification failed. Cannot start services.")
            return False
        
        # Start coordination service first (it's the master)
        coord_service = next(s for s in self.services if s["name"] == "coordination_service")
        if not self.start_service(coord_service):
            print("‚ùå Failed to start coordination service. Aborting.")
            return False
        
        # Start other services
        for service in self.services:
            if service["name"] != "coordination_service":
                self.start_service(service)
        
        # Print status
        self.print_status()
        
        # Start monitoring
        if self.monitoring_active:
            self.start_monitoring()
        
        return True
    
    def stop_all_services(self):
        """Stop all services"""
        print("\nüõë STOPPING ALL SERVICES")
        print("=" * 60)
        
        for service in reversed(self.services):  # Stop in reverse order
            self.stop_service(service)
        
        print("‚úì All services stopped")
    
    def print_status(self):
        """Print current service status"""
        print("\nüìä SERVICE STATUS")
        print("=" * 60)
        print(f"{'Service':<25} {'Status':<12} {'Port':<6} {'PID':<8} {'Uptime'}")
        print("-" * 60)
        
        for service in self.services:
            if service["process"] and service["process"].poll() is None:
                pid = service["process"].pid
                uptime = str(datetime.now() - service.get("start_time", datetime.now())).split('.')[0]
            else:
                pid = "-"
                uptime = "-"
            
            status_icon = {
                "Running": "‚úì",
                "Stopped": "‚úó",
                "Starting": "‚è≥",
                "Failed": "‚ùå",
                "Error": "‚ö†Ô∏è"
            }.get(service["status"], "?")
            
            print(f"{service['name']:<25} {status_icon} {service['status']:<10} {service['port']:<6} {pid:<8} {uptime}")
        
        print("=" * 60)
        
        # System metrics
        print("\nüìà SYSTEM METRICS")
        print("=" * 60)
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            print(f"CPU Usage: {cpu_percent}%")
            print(f"Memory: {memory.percent}% ({memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB)")
            print(f"Disk: {disk.percent}% ({disk.used / (1024**3):.1f}GB / {disk.total / (1024**3):.1f}GB)")
        except Exception as e:
            print(f"Error getting system metrics: {e}")
        print("=" * 60)
    
    def monitor_services(self):
        """Monitor services and restart if needed"""
        monitor_count = 0
        while self.running and self.monitoring_active:
            monitor_count += 1
            
            for service in self.services:
                if service["process"] and service["process"].poll() is not None:
                    # Service crashed
                    logger.warning(f"{service['name']} crashed! Restarting...")
                    service["status"] = "Crashed"
                    self.start_service(service)
            
            # Print status every 10 checks (5 minutes)
            if monitor_count % 10 == 0:
                print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Monitor check #{monitor_count}")
                self.print_status()
            
            time.sleep(30)  # Check every 30 seconds
    
    def start_monitoring(self):
        """Start the monitoring thread"""
        self.monitoring_active = True
        monitor_thread = threading.Thread(target=self.monitor_services, daemon=True)
        monitor_thread.start()
        logger.info("Service monitoring started")


def main():
    """Main entry point"""
    manager = ServiceManager()
    
    # Parse command line arguments
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "start":
            manager.monitoring_active = True
            if manager.start_all_services():
                print("\n‚úÖ Trading System Started Successfully!")
                print("Press Ctrl+C to stop the system\n")
                
                # Keep running
                try:
                    while manager.running:
                        time.sleep(1)
                except KeyboardInterrupt:
                    pass
            else:
                print("\n‚ùå Failed to start trading system")
                sys.exit(1)
                
        elif command == "stop":
            manager.stop_all_services()
            
        elif command == "status":
            manager.print_status()
            
        elif command == "restart":
            manager.stop_all_services()
            time.sleep(2)
            manager.monitoring_active = True
            manager.start_all_services()
            
        elif command == "help":
            print("Trading System Hybrid Manager v3.0.2-CLI")
            print("\nUsage: python hybrid_manager.py [command]")
            print("\nCommands:")
            print("  start    - Start all services with monitoring")
            print("  stop     - Stop all services")
            print("  status   - Show current service status")
            print("  restart  - Restart all services")
            print("  help     - Show this help message")
        else:
            print(f"Unknown command: {command}")
            print("Use 'python hybrid_manager.py help' for usage")
    else:
        # Default: start with monitoring
        print("Starting Trading System (use 'python hybrid_manager.py help' for options)")
        manager.monitoring_active = True
        if manager.start_all_services():
            print("\n‚úÖ Trading System Started Successfully!")
            print("Press Ctrl+C to stop the system\n")
            
            try:
                while manager.running:
                    time.sleep(1)
            except KeyboardInterrupt:
                pass
        else:
            print("\n‚ùå Failed to start trading system")
            sys.exit(1)


if __name__ == "__main__":
    main()


================================================================================
FILE: ./install-requirements.py
================================================================================

#!/usr/bin/env python3
"""
Robust requirement installer for Trading Application
Handles common pip issues in Codespaces
"""

import subprocess
import sys

def run_command(cmd):
    """Run a command and return success status"""
    print(f"Running: {cmd}")
    result = subprocess.run(cmd, shell=True)
    return result.returncode == 0

def main():
    print("üîß Installing Trading Application Requirements\n")
    
    # Step 1: Upgrade pip and core tools
    print("Step 1: Upgrading pip and setuptools...")
    if not run_command(f"{sys.executable} -m pip install --upgrade pip setuptools wheel"):
        print("‚ùå Failed to upgrade pip/setuptools")
        return 1
    
    # Step 2: Install requirements one by one to identify issues
    requirements = [
        "flask==3.0.0",
        "requests==2.31.0",
        "pandas==2.1.4",
        "numpy==1.26.2",
        "scikit-learn==1.3.2",
        "yfinance==0.2.33",
        "psutil==5.9.6",
        "python-dateutil==2.8.2",
        "pytz==2023.3",
        "beautifulsoup4==4.12.2",
        "lxml==4.9.3"
    ]
    
    print("\nStep 2: Installing packages...")
    failed = []
    
    for req in requirements:
        print(f"\nInstalling {req}...")
        if not run_command(f"{sys.executable} -m pip install {req}"):
            failed.append(req)
            print(f"‚ö†Ô∏è  Failed to install {req}")
        else:
            print(f"‚úÖ {req} installed")
    
    # Step 3: Try to install Alpaca separately (often causes issues)
    print("\nStep 3: Attempting Alpaca installation...")
    if not run_command(f"{sys.executable} -m pip install alpaca-py==0.21.1"):
        print("‚ö†Ô∏è  Alpaca installation failed - you can run without it")
        print("   Paper trading will use mock trades instead")
    else:
        print("‚úÖ Alpaca installed successfully")
    
    # Summary
    print("\n" + "="*50)
    print("üìä Installation Summary:")
    print(f"‚úÖ Successful: {len(requirements) - len(failed)} packages")
    
    if failed:
        print(f"‚ùå Failed: {len(failed)} packages")
        for pkg in failed:
            print(f"   - {pkg}")
        print("\nYour trading system can still run, but some features may be limited.")
    else:
        print("\nüéâ All packages installed successfully!")
    
    print("\nüöÄ Next step: python setup_codespace.py")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================================================================================
FILE: ./make_web.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: make_web.py
Version: 1.0.1
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.1 (2025-06-22) - Fixed CSS syntax error
- v1.0.0 (2025-06-22) - Initial version

PURPOSE:
Creates web interface for Trading Application following project standards.
"""

import os
import sys
from pathlib import Path
from datetime import datetime

def create_web_interface():
    """Create the web interface with proper CSS embedding"""
    
    # HTML template with CSS properly embedded in string
    html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>Trading Application</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .header {
            background: #333;
            color: white;
            padding: 20px;
            margin: -20px -20px 20px -20px;
            border-radius: 10px 10px 0 0;
        }
        
        .nav {
            background: #444;
            padding: 10px;
            margin: 0 -20px 20px -20px;
        }
        
        .nav a {
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            display: inline-block;
        }
        
        .nav a:hover {
            background: #555;
        }
        
        .content {
            padding: 20px 0;
        }
        
        .status-card {
            background: #f8f9fa;
            padding: 20px;
            margin: 10px 0;
            border-radius: 8px;
            border: 1px solid #dee2e6;
        }
        
        .status-healthy {
            border-left: 4px solid #28a745;
        }
        
        .status-error {
            border-left: 4px solid #dc3545;
        }
        
        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .metric {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #dee2e6;
            text-align: center;
        }
        
        .metric h3 {
            margin: 0 0 10px 0;
            color: #495057;
        }
        
        .metric .value {
            font-size: 2em;
            font-weight: bold;
            color: #007bff;
        }
        
        .button {
            background: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin: 5px;
        }
        
        .button:hover {
            background: #0056b3;
        }
        
        .button.success {
            background: #28a745;
        }
        
        .button.success:hover {
            background: #218838;
        }
        
        .button.danger {
            background: #dc3545;
        }
        
        .button.danger:hover {
            background: #c82333;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #dee2e6;
        }
        
        th {
            background: #f8f9fa;
            font-weight: bold;
        }
        
        .footer {
            margin-top: 40px;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #dee2e6;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Trading Application Dashboard</h1>
            <p>Real-time trading system management</p>
        </div>
        
        <div class="nav">
            <a href="#overview">Overview</a>
            <a href="#services">Services</a>
            <a href="#trading">Trading</a>
            <a href="#performance">Performance</a>
            <a href="#logs">Logs</a>
        </div>
        
        <div class="content">
            <section id="overview">
                <h2>System Overview</h2>
                
                <div class="metrics">
                    <div class="metric">
                        <h3>System Status</h3>
                        <div class="value">Active</div>
                    </div>
                    <div class="metric">
                        <h3>Active Services</h3>
                        <div class="value">8/8</div>
                    </div>
                    <div class="metric">
                        <h3>Trading Mode</h3>
                        <div class="value">Paper</div>
                    </div>
                    <div class="metric">
                        <h3>Database Status</h3>
                        <div class="value">Connected</div>
                    </div>
                </div>
                
                <div class="status-card status-healthy">
                    <h3>Quick Actions</h3>
                    <button class="button success">Start Trading</button>
                    <button class="button">View Positions</button>
                    <button class="button">Check Performance</button>
                    <button class="button danger">Stop All Services</button>
                </div>
            </section>
            
            <section id="services">
                <h2>Service Status</h2>
                
                <table>
                    <thead>
                        <tr>
                            <th>Service</th>
                            <th>Port</th>
                            <th>Status</th>
                            <th>Uptime</th>
                            <th>Actions</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Trading Engine</td>
                            <td>5000</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Market Data</td>
                            <td>5001</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Pattern Analysis</td>
                            <td>5002</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Risk Management</td>
                            <td>5003</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>News Service</td>
                            <td>5004</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Portfolio Manager</td>
                            <td>5005</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Scheduler</td>
                            <td>5007</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>ML Service</td>
                            <td>5009</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                    </tbody>
                </table>
            </section>
        </div>
        
        <div class="footer">
            <p>Trading Application v1.0.1 | Last Updated: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</p>
        </div>
    </div>
</body>
</html>
"""
    
    # Write the HTML file
    output_path = Path("trading_dashboard.html")
    
    try:
        with open(output_path, 'w') as f:
            f.write(html_content)
        
        print(f"‚úÖ Successfully created web interface: {output_path}")
        print(f"üìÅ File location: {output_path.absolute()}")
        
        # If running in Colab, provide additional instructions
        if 'google.colab' in sys.modules:
            print("\nüåê To view in Google Colab:")
            print("1. Run: from IPython.display import HTML")
            print("2. Run: HTML(filename='trading_dashboard.html')")
            
    except Exception as e:
        print(f"‚ùå Error creating web interface: {e}")
        return False
    
    return True

def main():
    """Main entry point"""
    print("Creating Trading Application Web Interface...")
    print("-" * 50)
    
    success = create_web_interface()
    
    if success:
        print("\n‚úÖ Web interface created successfully!")
    else:
        print("\n‚ùå Failed to create web interface")
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================================================
FILE: ./news_service.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM NEWS SERVICE
Version: 1.0.5
Last Updated: 2025-06-21
REVISION HISTORY:
v1.0.5 (2025-06-21) - Fixed NOT NULL constraint by ensuring article_date is always populated
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Enhanced sentiment analysis
v1.0.2 (2025-06-15) - Initial version
v1.0.1 (2025-06-15) - Original implementation

News Service - Provides news sentiment analysis for securities
Uses multiple NLP models for comprehensive sentiment analysis
Fixed to handle yfinance websockets dependency issues gracefully
Fixed NOT NULL constraint error for article_date field
"""

import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

# Import TextBlob for sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    print("‚úÖ TextBlob imported successfully")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    print("‚ö†Ô∏è TextBlob not available - using keyword-based sentiment only")

# Import database utilities if available
try:
    from database_utils import DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found, using direct SQLite connections")

class NewsService(DatabaseServiceMixin if USE_DB_UTILS else object):
    def __init__(self, db_path='./trading_system.db'):
        if USE_DB_UTILS:
            super().__init__(db_path)
        else:
            self.db_path = db_path
        
        self.app = Flask(__name__)
        self.setup_routes()
        self.setup_logging()
        
        # Sentiment keywords for basic analysis
        self.positive_keywords = [
            'upgrade', 'buy', 'strong', 'outperform', 'positive', 'growth',
            'beat', 'exceed', 'surge', 'rally', 'gain', 'profit', 'revenue',
            'bullish', 'optimistic', 'successful', 'breakthrough', 'innovation'
        ]
        
        self.negative_keywords = [
            'downgrade', 'sell', 'weak', 'underperform', 'negative', 'loss',
            'miss', 'decline', 'fall', 'drop', 'cut', 'layoff', 'lawsuit',
            'bearish', 'pessimistic', 'failure', 'concern', 'risk', 'warning'
        ]
        
        self.logger.info("News Service initialized with database utilities" if USE_DB_UTILS else "News Service initialized")
        
    def setup_logging(self):
        """Setup logging configuration"""
        self.logger = logging.getLogger('news_service')
        self.logger.setLevel(logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        self.logger.addHandler(handler)
        
    def setup_routes(self):
        """Setup Flask routes"""
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({"status": "healthy", "service": "news_service"})
            
        @self.app.route('/analyze_sentiment', methods=['POST'])
        def analyze_sentiment():
            data = request.json
            symbol = data.get('symbol')
            
            if not symbol:
                return jsonify({"error": "Symbol required"}), 400
                
            analysis = self._analyze_sentiment(symbol)
            return jsonify(analysis)
            
    def _get_news_data(self, symbol: str) -> List[Dict]:
        """Fetch news data for a symbol"""
        news_items = []
        
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                news = ticker.news
                
                for item in news[:10]:  # Get latest 10 news items
                    news_items.append({
                        'title': item.get('title', ''),
                        'publisher': item.get('publisher', ''),
                        'link': item.get('link', ''),
                        'timestamp': item.get('providerPublishTime', 0)
                    })
                    
            except Exception as e:
                self.logger.error(f"Error fetching news for {symbol}: {e}")
                
        else:
            # Simulated news data when yfinance not available
            news_items = [
                {
                    'title': f"{symbol} Shows Strong Performance in Q4",
                    'publisher': 'Financial Times',
                    'link': 'https://example.com',
                    'timestamp': int(datetime.now().timestamp())
                },
                {
                    'title': f"Analysts Upgrade {symbol} to Buy Rating",
                    'publisher': 'Reuters',
                    'link': 'https://example.com',
                    'timestamp': int(datetime.now().timestamp())
                }
            ]
            
        return news_items
        
    def _calculate_sentiment(self, text: str) -> tuple:
        """Calculate sentiment score and label"""
        if TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                
                if polarity > 0.1:
                    label = 'positive'
                elif polarity < -0.1:
                    label = 'negative'
                else:
                    label = 'neutral'
                    
                return polarity, label
                
            except Exception as e:
                self.logger.error(f"TextBlob error: {e}")
                
        # Fallback to keyword-based sentiment
        text_lower = text.lower()
        positive_count = sum(1 for word in self.positive_keywords if word in text_lower)
        negative_count = sum(1 for word in self.negative_keywords if word in text_lower)
        
        if positive_count > negative_count:
            score = 0.5
            label = 'positive'
        elif negative_count > positive_count:
            score = -0.5
            label = 'negative'
        else:
            score = 0.0
            label = 'neutral'
            
        return score, label
        
    def _analyze_sentiment(self, symbol: str) -> Dict:
        """Analyze sentiment for a symbol"""
        news_items = self._get_news_data(symbol)
        
        if not news_items:
            return {
                'symbol': symbol,
                'sentiment_score': 0.0,
                'sentiment_label': 'neutral',
                'news_count': 0,
                'analysis_time': datetime.now().isoformat()
            }
            
        sentiments = []
        positive_count = 0
        negative_count = 0
        
        for item in news_items:
            score, label = self._calculate_sentiment(item['title'])
            sentiments.append(score)
            
            if label == 'positive':
                positive_count += 1
            elif label == 'negative':
                negative_count += 1
                
        # Calculate average sentiment
        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0.0
        
        # Determine overall label
        if avg_sentiment > 0.1:
            overall_label = 'positive'
        elif avg_sentiment < -0.1:
            overall_label = 'negative'
        else:
            overall_label = 'neutral'
            
        analysis_result = {
            'symbol': symbol,
            'sentiment_score': round(avg_sentiment, 3),
            'sentiment_label': overall_label,
            'news_count': len(news_items),
            'positive_articles': positive_count,
            'negative_articles': negative_count,
            'neutral_articles': len(news_items) - positive_count - negative_count,
            'latest_news': news_items[:5],  # Return top 5 news items
            'analysis_time': datetime.now().isoformat()
        }
        
        # Save to database
        self._save_sentiment_analysis(analysis_result)
        
        return analysis_result
        
    def _save_sentiment_analysis(self, sentiment_data: Dict):
        """Save sentiment analysis to database"""
        try:
            # Ensure article_date is always populated
            analysis_time = sentiment_data.get('analysis_time', datetime.now().isoformat())
            
            # Convert ISO format to datetime object then back to ensure consistency
            if isinstance(analysis_time, str):
                article_date = datetime.fromisoformat(analysis_time.replace('Z', '+00:00')).isoformat()
            else:
                article_date = datetime.now().isoformat()
            
            if USE_DB_UTILS:
                # Use database utilities with retry logic
                with self.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    cursor.execute('''
                        INSERT INTO news_sentiment 
                        (symbol, article_date, sentiment_score, sentiment_label, 
                         news_count, positive_articles, negative_articles, metadata, analysis_timestamp)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        sentiment_data['symbol'],
                        article_date,  # FIXED: Always populated
                        sentiment_data['sentiment_score'],
                        sentiment_data['sentiment_label'],
                        sentiment_data['news_count'],
                        sentiment_data.get('positive_articles', 0),
                        sentiment_data.get('negative_articles', 0),
                        json.dumps(sentiment_data),
                        datetime.now().isoformat()
                    ))
                    
                    conn.commit()
                    
            else:
                # Direct SQLite connection
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO news_sentiment 
                    (symbol, article_date, sentiment_score, sentiment_label, 
                     news_count, positive_articles, negative_articles, metadata, analysis_timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    sentiment_data['symbol'],
                    article_date,  # FIXED: Always populated
                    sentiment_data['sentiment_score'],
                    sentiment_data['sentiment_label'],
                    sentiment_data['news_count'],
                    sentiment_data.get('positive_articles', 0),
                    sentiment_data.get('negative_articles', 0),
                    json.dumps(sentiment_data),
                    datetime.now().isoformat()
                ))
                
                conn.commit()
                conn.close()
                
            self.logger.info(f"Saved sentiment analysis for {sentiment_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving sentiment analysis: {e}")
            # Log the specific data that caused the error for debugging
            self.logger.error(f"Failed data: {sentiment_data}")
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        textblob_status = "with TextBlob" if TEXTBLOB_AVAILABLE else "keyword-based only"
        db_mode = "with retry logic" if USE_DB_UTILS else "direct connection"
        
        self.logger.info(f"Starting News Service on port 5008 {mode}, {textblob_status}, {db_mode}")
        self.app.run(host='0.0.0.0', port=5008, debug=False)

if __name__ == "__main__":
    service = NewsService()
    service.run()


================================================================================
FILE: ./paper_trading.py
================================================================================

"""
Name of Service: TRADING SYSTEM PHASE 1 - PAPER TRADING
Version: 1.0.2
Last Updated: 2025-06-15
REVISION HISTORY:
v1.0.2 (2025-06-15) - Updated to use ALPACA_PAPER_API_KEY and ALPACA_PAPER_API_SECRET environment variables
v1.0.1 (2025-06-15) - Updated header format and moved API credentials to environment variables
v1.0.0 (2025-06-15) - Initial release with Alpaca paper trading integration

Paper Trading Service - Executes trades using Alpaca paper trading API
Receives trading signals and executes them via Alpaca
"""

import os
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Note: You'll need to install alpaca-trade-api
# !pip install alpaca-trade-api
try:
    import alpaca_trade_api as tradeapi
    ALPACA_AVAILABLE = True
except ImportError:
    ALPACA_AVAILABLE = False

class PaperTradingService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        # Alpaca configuration from environment variables - UPDATED TO USE PAPER TRADING VARS
        self.api_key = os.environ.get('ALPACA_PAPER_API_KEY', '')
        self.api_secret = os.environ.get('ALPACA_PAPER_API_SECRET', '')
        self.base_url = os.environ.get('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
        
        # Log configuration status (without exposing secrets)
        if self.api_key and self.api_secret:
            self.logger.info("Alpaca Paper Trading API credentials loaded from environment")
        else:
            self.logger.warning("Alpaca Paper Trading API credentials not found in environment variables")
            self.logger.info("Set ALPACA_PAPER_API_KEY and ALPACA_PAPER_API_SECRET environment variables")
        
        self.alpaca_api = None
        self._setup_alpaca_api()
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PaperTradingService')
        
        handler = logging.FileHandler('./logs/paper_trading_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_alpaca_api(self):
        """Setup Alpaca API connection"""
        if not ALPACA_AVAILABLE:
            self.logger.warning("Alpaca API not available - install alpaca-trade-api")
            return
        
        if not self.api_key or not self.api_secret:
            self.logger.warning("Alpaca Paper Trading API credentials not configured - running in simulation mode")
            return
        
        try:
            self.alpaca_api = tradeapi.REST(
                self.api_key,
                self.api_secret,
                self.base_url,
                api_version='v2'
            )
            
            # Test connection
            account = self.alpaca_api.get_account()
            self.logger.info(f"Connected to Alpaca Paper Trading API. Account status: {account.status}")
            self.logger.info(f"Paper Trading Account - Buying Power: ${float(account.buying_power):,.2f}")
            
        except Exception as e:
            self.logger.error(f"Error setting up Alpaca Paper Trading API: {e}")
            self.alpaca_api = None
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "paper_trading",
                "alpaca_connected": self.alpaca_api is not None,
                "trading_mode": "paper" if self.alpaca_api else "simulation"
            })
        
        @self.app.route('/execute_trades', methods=['POST'])
        def execute_trades_endpoint():
            signals = request.json.get('signals', [])
            trades = self._execute_trades(signals)
            return jsonify(trades)
        
        @self.app.route('/account', methods=['GET'])
        def get_account():
            account_info = self._get_account_info()
            return jsonify(account_info)
        
        @self.app.route('/positions', methods=['GET'])
        def get_positions():
            positions = self._get_positions()
            return jsonify(positions)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "paper_trading",
                "port": 5005
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _execute_trades(self, trading_signals: List[Dict]) -> List[Dict]:
        """Execute trades based on trading signals"""
        self.logger.info(f"Executing trades for {len(trading_signals)} signals")
        
        executed_trades = []
        
        for signal in trading_signals:
            try:
                if signal.get('signal') not in ['BUY', 'SELL']:
                    continue
                
                trade_result = self._execute_single_trade(signal)
                if trade_result:
                    executed_trades.append(trade_result)
                
            except Exception as e:
                self.logger.error(f"Error executing trade for {signal.get('symbol', 'unknown')}: {e}")
        
        self.logger.info(f"Trade execution completed: {len(executed_trades)} trades executed")
        return executed_trades
    
    def _execute_single_trade(self, signal: Dict) -> Optional[Dict]:
        """Execute a single trade"""
        symbol = signal['symbol']
        signal_type = signal['signal']
        quantity = signal.get('quantity', 100)
        
        try:
            if self.alpaca_api is None:
                # Simulate trade execution for demo
                return self._simulate_trade_execution(signal)
            
            # Check account
            account = self.alpaca_api.get_account()
            if signal_type == 'BUY' and float(account.buying_power) < 1000:
                self.logger.warning(f"Insufficient buying power for {symbol}")
                return None
            
            # Get current price
            try:
                latest_trade = self.alpaca_api.get_latest_trade(symbol)
                current_price = latest_trade.price
            except:
                # Fallback to last close price
                current_price = signal.get('current_price', 100)
            
            # Submit order
            side = 'buy' if signal_type == 'BUY' else 'sell'
            
            order = self.alpaca_api.submit_order(
                symbol=symbol,
                qty=quantity,
                side=side,
                type='market',
                time_in_force='day'
            )
            
            # Create trade record
            trade_record = {
                'symbol': symbol,
                'signal': signal_type,
                'quantity': quantity,
                'entry_price': current_price,
                'confidence': signal.get('confidence', 0.0),
                'reason': signal.get('reason', 'Technical analysis signal'),
                'alpaca_order_id': order.id,
                'status': 'executed',
                'timestamp': datetime.now().isoformat()
            }
            
            # Save to database
            self._save_trade_record(trade_record)
            
            self.logger.info(f"Executed {signal_type} trade for {symbol}: {quantity} shares at ${current_price:.2f}")
            return trade_record
            
        except Exception as e:
            self.logger.error(f"Error executing trade for {symbol}: {e}")
            return None
    
    def _simulate_trade_execution(self, signal: Dict) -> Dict:
        """Simulate trade execution when Alpaca API is not available"""
        symbol = signal['symbol']
        signal_type = signal['signal']
        quantity = signal.get('quantity', 100)
        current_price = signal.get('current_price', 100)
        
        trade_record = {
            'symbol': symbol,
            'signal': signal_type,
            'quantity': quantity,
            'entry_price': current_price,
            'confidence': signal.get('confidence', 0.0),
            'reason': signal.get('reason', 'Simulated trade'),
            'alpaca_order_id': f"SIM_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'status': 'simulated',
            'timestamp': datetime.now().isoformat()
        }
        
        # Save to database
        self._save_trade_record(trade_record)
        
        self.logger.info(f"Simulated {signal_type} trade for {symbol}: {quantity} shares at ${current_price:.2f}")
        return trade_record
    
    def _save_trade_record(self, trade_data: Dict):
        """Save trade record to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trades 
                (symbol, signal_type, quantity, entry_price, confidence, 
                 trade_reason, alpaca_order_id, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                trade_data['symbol'],
                trade_data['signal'],
                trade_data['quantity'],
                trade_data['entry_price'],
                trade_data['confidence'],
                trade_data['reason'],
                trade_data['alpaca_order_id'],
                trade_data['status'],
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved trade record for {trade_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving trade record: {e}")
    
    def _get_account_info(self) -> Dict:
        """Get account information"""
        try:
            if self.alpaca_api is None:
                return {
                    "status": "simulated", 
                    "buying_power": 100000, 
                    "portfolio_value": 100000,
                    "mode": "simulation"
                }
            
            account = self.alpaca_api.get_account()
            
            return {
                "account_status": account.status,
                "buying_power": float(account.buying_power),
                "portfolio_value": float(account.portfolio_value),
                "cash": float(account.cash),
                "equity": float(account.equity),
                "mode": "paper_trading",
                "pattern_day_trader": account.pattern_day_trader,
                "trading_blocked": account.trading_blocked,
                "transfers_blocked": account.transfers_blocked
            }
            
        except Exception as e:
            self.logger.error(f"Error getting account info: {e}")
            return {"error": str(e)}
    
    def _get_positions(self) -> List[Dict]:
        """Get current positions"""
        try:
            if self.alpaca_api is None:
                return []
            
            positions = self.alpaca_api.list_positions()
            
            position_list = []
            for position in positions:
                position_list.append({
                    "symbol": position.symbol,
                    "quantity": int(position.qty),
                    "side": "long" if int(position.qty) > 0 else "short",
                    "market_value": float(position.market_value),
                    "unrealized_pl": float(position.unrealized_pl),
                    "unrealized_plpc": float(position.unrealized_plpc),
                    "avg_entry_price": float(position.avg_entry_price),
                    "current_price": float(position.current_price) if hasattr(position, 'current_price') else None
                })
            
            return position_list
            
        except Exception as e:
            self.logger.error(f"Error getting positions: {e}")
            return []
    
    def run(self):
        self.logger.info("Starting Paper Trading Service on port 5005")
        if self.alpaca_api:
            self.logger.info("Alpaca Paper Trading API connected and ready")
        else:
            self.logger.info("Running in simulation mode (no Alpaca connection)")
        self.app.run(host='0.0.0.0', port=5005, debug=False)

if __name__ == "__main__":
    service = PaperTradingService()
    service.run()


================================================================================
FILE: ./pattern_analysis.py
================================================================================

# ================================================================
# 3. pattern_analysis.py (Port 5002) - FIXED VERSION
# ================================================================
"""
Name of Service: TRADING SYSTEM PATTERN ANALYSIS - FIXED VERSION
Version: 1.0.5
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.5 (2025-06-17) - CRITICAL FIX: JSON serialization error with boolean values
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Removed TA-Lib dependency, using only manual pattern detection
v1.0.2 (2025-06-15) - Fixed version with TA-Lib fallback
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Pattern Analysis Service - Analyzes technical patterns using manual calculation methods
CRITICAL BUG FIX: Fixed JSON serialization error when saving patterns with boolean values
"""

import numpy as np
import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

class PatternAnalysisService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        self.pattern_recognition_url = "http://localhost:5006"
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PatternAnalysisService')
        
        handler = logging.FileHandler('./logs/pattern_analysis_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "pattern_analysis", 
                "implementation": "manual_algorithms",
                "yfinance_available": YFINANCE_AVAILABLE,
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated"
            })
        
        @self.app.route('/analyze_patterns/<symbol>', methods=['GET'])
        def analyze_patterns_endpoint(symbol):
            analysis = self._analyze_patterns(symbol)
            return jsonify(analysis)
        
        @self.app.route('/supported_patterns', methods=['GET'])
        def get_supported_patterns():
            return jsonify(self._get_supported_patterns())
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "pattern_analysis", 
                "port": 5002
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _analyze_patterns(self, symbol: str) -> Dict:
        """Main pattern analysis logic"""
        self.logger.info(f"Starting pattern analysis for {symbol}")
        
        try:
            # Get market data
            hist_data = self._get_historical_data(symbol)
            if hist_data is None:
                return {'symbol': symbol, 'patterns': [], 'error': 'No data available'}
            
            # Manual pattern analysis
            basic_patterns = self._detect_basic_patterns(symbol, hist_data)
            
            # Get enhanced patterns from pattern recognition service
            enhanced_patterns = self._get_enhanced_patterns(symbol)
            
            # Combine results
            all_patterns = basic_patterns + enhanced_patterns.get('candlestick_patterns', [])
            
            # Calculate overall confidence
            confidence_score = self._calculate_confidence(all_patterns)
            
            combined_analysis = {
                'symbol': symbol,
                'basic_patterns': basic_patterns,
                'enhanced_patterns': enhanced_patterns,
                'patterns': all_patterns,
                'confidence_score': confidence_score,
                'pattern_count': len(all_patterns),
                'analysis_timestamp': datetime.now().isoformat(),
                'implementation': 'manual_algorithms',
                'data_source': 'yfinance' if YFINANCE_AVAILABLE else 'simulated'
            }
            
            # Save to database - FIXED VERSION with proper JSON handling
            self._save_pattern_analysis(symbol, combined_analysis)
            
            self.logger.info(f"Pattern analysis completed for {symbol}: {len(all_patterns)} patterns found")
            return combined_analysis
            
        except Exception as e:
            self.logger.error(f"Error in pattern analysis for {symbol}: {e}")
            return {'symbol': symbol, 'patterns': [], 'error': str(e)}
    
    def _get_historical_data(self, symbol: str, period: str = "30d") -> Optional[pd.DataFrame]:
        """Get historical data for pattern analysis"""
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period=period)
                
                if len(hist) < 20:
                    return self._generate_simulated_data(symbol)
                
                return hist
                
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using simulated data: {e}")
                return self._generate_simulated_data(symbol)
        else:
            return self._generate_simulated_data(symbol)
    
    def _generate_simulated_data(self, symbol: str) -> pd.DataFrame:
        """Generate simulated OHLCV data for pattern analysis"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data
        random.seed(hash(symbol) + int(time.time() / 86400))
        
        # Generate 30 days of simulated data
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        
        # Start with a base price
        base_price = random.uniform(10, 200)
        prices = []
        volumes = []
        
        current_price = base_price
        for i in range(30):
            # Random daily change
            change_pct = random.uniform(-0.05, 0.05)  # -5% to +5%
            current_price *= (1 + change_pct)
            
            # Generate OHLC
            open_price = current_price * random.uniform(0.98, 1.02)
            close_price = current_price
            high_price = max(open_price, close_price) * random.uniform(1.0, 1.05)
            low_price = min(open_price, close_price) * random.uniform(0.95, 1.0)
            volume = random.randint(100000, 1000000)
            
            prices.append([open_price, high_price, low_price, close_price])
            volumes.append(volume)
        
        # Create DataFrame
        data = pd.DataFrame(prices, columns=['Open', 'High', 'Low', 'Close'], index=dates)
        data['Volume'] = volumes
        
        return data
    
    def _detect_basic_patterns(self, symbol: str, data: pd.DataFrame) -> List[Dict]:
        """Detect basic candlestick patterns using manual calculations"""
        patterns = []
        
        try:
            close_prices = data['Close'].values
            open_prices = data['Open'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            
            # Check last 5 days for patterns
            for i in range(-5, 0):
                if abs(i) > len(close_prices):
                    continue
                    
                current_open = open_prices[i]
                current_close = close_prices[i]
                current_high = high_prices[i]
                current_low = low_prices[i]
                
                # Calculate body and shadows
                body = abs(current_close - current_open)
                lower_shadow = min(current_open, current_close) - current_low
                upper_shadow = current_high - max(current_open, current_close)
                total_range = current_high - current_low
                
                # Avoid division by zero
                if total_range == 0:
                    continue
                
                # Doji pattern (open ‚âà close) - FIXED: Convert booleans properly
                if body < (total_range * 0.1):
                    patterns.append({
                        'pattern_type': 'doji',
                        'signal_strength': 100,
                        'confidence_score': 0.7,
                        'bullish': None,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Small body indicates indecision'
                    })
                
                # Hammer pattern (small body at top, long lower shadow)
                if lower_shadow > body * 2 and upper_shadow < body * 0.5 and body > 0:
                    patterns.append({
                        'pattern_type': 'hammer',
                        'signal_strength': 100,
                        'confidence_score': 0.6,
                        'bullish': True,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Long lower shadow suggests buying pressure'
                    })
                
                # Shooting star (small body at bottom, long upper shadow)
                if upper_shadow > body * 2 and lower_shadow < body * 0.5 and body > 0:
                    patterns.append({
                        'pattern_type': 'shooting_star',
                        'signal_strength': -100,
                        'confidence_score': 0.6,
                        'bullish': False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Long upper shadow suggests selling pressure'
                    })
                
                # Engulfing patterns (requires previous candle)
                if i > -len(close_prices) and abs(i) > 1:
                    prev_open = open_prices[i-1]
                    prev_close = close_prices[i-1]
                    prev_body = abs(prev_close - prev_open)
                    
                    # Bullish engulfing
                    if (current_close > current_open and  # Current candle is bullish
                        prev_close < prev_open and        # Previous candle is bearish
                        current_open < prev_close and     # Current opens below previous close
                        current_close > prev_open and     # Current closes above previous open
                        body > prev_body * 1.1):          # Current body is larger
                        
                        patterns.append({
                            'pattern_type': 'bullish_engulfing',
                            'signal_strength': 100,
                            'confidence_score': 0.8,
                            'bullish': True,  # Will be converted to string
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_calculation',
                            'description': 'Bullish candle engulfs previous bearish candle'
                        })
                    
                    # Bearish engulfing
                    elif (current_close < current_open and  # Current candle is bearish
                          prev_close > prev_open and        # Previous candle is bullish
                          current_open > prev_close and     # Current opens above previous close
                          current_close < prev_open and     # Current closes below previous open
                          body > prev_body * 1.1):          # Current body is larger
                        
                        patterns.append({
                            'pattern_type': 'bearish_engulfing',
                            'signal_strength': -100,
                            'confidence_score': 0.8,
                            'bullish': False,  # Will be converted to string
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_calculation',
                            'description': 'Bearish candle engulfs previous bullish candle'
                        })
            
            # Trend detection using linear regression
            if len(close_prices) >= 10:
                recent_trend = np.polyfit(range(10), close_prices[-10:], 1)[0]
                price_std = np.std(close_prices[-20:]) if len(close_prices) >= 20 else np.std(close_prices)
                
                if abs(recent_trend) > price_std * 0.01:  # Significant trend
                    patterns.append({
                        'pattern_type': 'trend_detected',
                        'signal_strength': 100 if recent_trend > 0 else -100,
                        'confidence_score': min(abs(recent_trend) * 100, 0.9),
                        'bullish': True if recent_trend > 0 else False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f"{'Upward' if recent_trend > 0 else 'Downward'} trend detected"
                    })
            
            # Support and resistance levels
            if len(close_prices) >= 20:
                recent_high = np.max(high_prices[-20:])
                recent_low = np.min(low_prices[-20:])
                current_price = close_prices[-1]
                
                # Near resistance
                if current_price >= recent_high * 0.98:
                    patterns.append({
                        'pattern_type': 'near_resistance',
                        'signal_strength': -50,
                        'confidence_score': 0.7,
                        'bullish': False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f'Price near resistance level: ${recent_high:.2f}'
                    })
                
                # Near support
                if current_price <= recent_low * 1.02:
                    patterns.append({
                        'pattern_type': 'near_support',
                        'signal_strength': 50,
                        'confidence_score': 0.7,
                        'bullish': True,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f'Price near support level: ${recent_low:.2f}'
                    })
            
        except Exception as e:
            self.logger.error(f"Error in manual pattern detection: {e}")
        
        return patterns
    
    def _get_enhanced_patterns(self, symbol: str) -> Dict:
        """Get enhanced patterns from pattern recognition service"""
        try:
            response = requests.get(f"{self.pattern_recognition_url}/detect_advanced_patterns/{symbol}", 
                                  timeout=30)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.warning(f"Could not get enhanced patterns for {symbol}")
                return {}
        except Exception as e:
            self.logger.warning(f"Error getting enhanced patterns for {symbol}: {e}")
            return {}
    
    def _calculate_confidence(self, patterns: List[Dict]) -> float:
        """Calculate overall pattern confidence"""
        if not patterns:
            return 0.0
        
        total_confidence = sum([p.get('confidence_score', 0) for p in patterns])
        return min(total_confidence / len(patterns), 1.0)
    
    def _save_pattern_analysis(self, symbol: str, analysis_data: Dict):
        """Save pattern analysis to database - FIXED VERSION with proper JSON serialization"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # CRITICAL FIX: Convert all values to JSON-serializable format
            json_safe_data = self._make_json_serializable(analysis_data)
            
            cursor.execute('''
                INSERT INTO pattern_analysis 
                (symbol, analysis_date, pattern_type, pattern_name, confidence_score, additional_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                symbol,
                datetime.now().date().isoformat(),
                'combined_analysis',
                f"{len(analysis_data.get('patterns', []))} patterns detected",
                analysis_data.get('confidence_score', 0.0),
                json.dumps(json_safe_data),  # Now safe to serialize
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved pattern analysis for {symbol}")
            
        except Exception as e:
            self.logger.error(f"Error saving pattern analysis for {symbol}: {e}")
    
    def _make_json_serializable(self, obj):
        """CRITICAL FIX: Convert object to JSON-serializable format"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, bool):
            return str(obj).lower()  # Convert True/False to "true"/"false"
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)  # Convert numpy types to standard Python types
        elif obj is None:
            return "null"  # Convert None to string
        else:
            return obj  # Return as-is for strings, numbers, etc.
    
    def _get_supported_patterns(self) -> List[str]:
        """Get list of supported pattern types"""
        return [
            'doji', 'hammer', 'shooting_star', 
            'bullish_engulfing', 'bearish_engulfing',
            'trend_detected', 'near_resistance', 'near_support',
            'advanced_chart_patterns', 'advanced_volume_patterns'
        ]
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        self.logger.info(f"Starting Pattern Analysis Service on port 5002 {mode}")
        self.app.run(host='0.0.0.0', port=5002, debug=False)

if __name__ == "__main__":
    service = PatternAnalysisService()
    service.run()

================================================================================
FILE: ./pattern_recognition_service.py
================================================================================

# ================================================================
# 6. pattern_recognition_service.py (Port 5006) - CORRECTED VERSION
# ================================================================
"""
Name of Service: TRADING SYSTEM ADVANCED PATTERN RECOGNITION - CORRECTED VERSION
Version: 1.0.5
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.5 (2025-06-19) - Integrated database utilities to fix database locking issues
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Removed TA-Lib dependency, using only manual pattern detection algorithms
v1.0.2 (2025-06-15) - Enhanced pattern detection
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Advanced Pattern Recognition Service - Enhanced pattern detection using manual ML algorithms
Complements the basic pattern analysis with advanced mathematical algorithms
Now uses database utilities with retry logic to prevent locking issues
"""

import numpy as np
import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Import database utilities
try:
    from database_utils import get_database_manager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found, using direct SQLite connections")

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

class PatternRecognitionService(DatabaseServiceMixin if USE_DB_UTILS else object):
    def __init__(self, db_path='./trading_system.db'):
        # Initialize database utilities if available
        if USE_DB_UTILS:
            super().__init__(db_path)
        else:
            self.db_path = db_path
            
        self.app = Flask(__name__)
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PatternRecognitionService')
        
        handler = logging.FileHandler('./logs/pattern_recognition_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "pattern_recognition", 
                "implementation": "manual_algorithms",
                "yfinance_available": YFINANCE_AVAILABLE,
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated",
                "database_mode": "with_retry" if USE_DB_UTILS else "direct"
            })
        
        @self.app.route('/detect_advanced_patterns/<symbol>', methods=['GET'])
        def detect_advanced_patterns(symbol):
            patterns = self._detect_advanced_patterns(symbol)
            return jsonify(patterns)
        
        @self.app.route('/candlestick_patterns/<symbol>', methods=['GET'])
        def candlestick_patterns(symbol):
            patterns = self._detect_candlestick_patterns(symbol)
            return jsonify(patterns)
        
        @self.app.route('/chart_patterns/<symbol>', methods=['GET'])
        def chart_patterns(symbol):
            patterns = self._detect_chart_patterns(symbol)
            return jsonify(patterns)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "pattern_recognition",
                "port": 5006
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _detect_advanced_patterns(self, symbol: str) -> Dict:
        """Detect advanced patterns using multiple manual techniques"""
        try:
            data = self._get_market_data(symbol)
            if data.empty:
                return {"error": "No data available"}
            
            # Different pattern types using manual algorithms
            candlestick_patterns = self._detect_candlestick_patterns(symbol, data)
            chart_patterns = self._detect_chart_patterns(symbol, data)
            volume_patterns = self._detect_volume_patterns(symbol, data)
            
            # Calculate overall pattern score
            pattern_score = self._calculate_pattern_score(candlestick_patterns, chart_patterns, volume_patterns)
            
            result = {
                'symbol': symbol,
                'candlestick_patterns': candlestick_patterns,
                'chart_patterns': chart_patterns,
                'volume_patterns': volume_patterns,
                'overall_pattern_score': pattern_score,
                'analysis_time': datetime.now().isoformat(),
                'implementation': 'manual_algorithms',
                'data_source': 'yfinance' if YFINANCE_AVAILABLE else 'simulated'
            }
            
            # Save to database with retry logic
            self._save_pattern_analysis(result)
            
            self.logger.info(f"Advanced pattern analysis completed for {symbol}: score {pattern_score}")
            return result
            
        except Exception as e:
            self.logger.error(f"Error in advanced pattern detection for {symbol}: {e}")
            return {"error": str(e)}
    
    def _get_market_data(self, symbol: str, period: str = "30d") -> pd.DataFrame:
        """Get market data for pattern analysis"""
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period=period)
                
                if len(hist) < 20:
                    return self._generate_simulated_data(symbol)
                
                return hist
                
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using simulated data: {e}")
                return self._generate_simulated_data(symbol)
        else:
            return self._generate_simulated_data(symbol)
    
    def _generate_simulated_data(self, symbol: str) -> pd.DataFrame:
        """Generate simulated OHLCV data for pattern analysis"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data
        random.seed(hash(symbol) + int(time.time() / 86400))
        
        # Generate 30 days of simulated data
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        
        # Start with a base price
        base_price = random.uniform(10, 200)
        prices = []
        volumes = []
        
        current_price = base_price
        for i in range(30):
            # Random daily change
            change_pct = random.uniform(-0.05, 0.05)  # -5% to +5%
            current_price *= (1 + change_pct)
            
            # Generate OHLC
            open_price = current_price * random.uniform(0.98, 1.02)
            close_price = current_price
            high_price = max(open_price, close_price) * random.uniform(1.0, 1.05)
            low_price = min(open_price, close_price) * random.uniform(0.95, 1.0)
            volume = random.randint(100000, 1000000)
            
            prices.append([open_price, high_price, low_price, close_price])
            volumes.append(volume)
        
        # Create DataFrame
        data = pd.DataFrame(prices, columns=['Open', 'High', 'Low', 'Close'], index=dates)
        data['Volume'] = volumes
        
        return data
    
    def _detect_candlestick_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect candlestick patterns using manual mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            open_prices = data['Open'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            close_prices = data['Close'].values
            
            # Analyze last 10 days for patterns
            for i in range(max(-10, -len(close_prices)), 0):
                if abs(i) > len(close_prices) or abs(i-1) > len(close_prices):
                    continue
                
                # Current candle
                curr_open = open_prices[i]
                curr_high = high_prices[i]
                curr_low = low_prices[i]
                curr_close = close_prices[i]
                
                # Calculate current candle properties
                curr_body = abs(curr_close - curr_open)
                curr_range = curr_high - curr_low
                curr_upper_shadow = curr_high - max(curr_open, curr_close)
                curr_lower_shadow = min(curr_open, curr_close) - curr_low
                
                if curr_range == 0:
                    continue
                
                # Previous candle (for multi-candle patterns)
                if abs(i-1) < len(close_prices):
                    prev_open = open_prices[i-1]
                    prev_high = high_prices[i-1]
                    prev_low = low_prices[i-1] 
                    prev_close = close_prices[i-1]
                    prev_body = abs(prev_close - prev_open)
                
                # DOJI Pattern
                if curr_body < curr_range * 0.1:
                    patterns.append({
                        'pattern_name': 'doji',
                        'signal_strength': 0,  # Neutral
                        'confidence_score': 0.7,
                        'bullish': None,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Indecision pattern - open equals close'
                    })
                
                # HAMMER Pattern
                if (curr_lower_shadow > curr_body * 2 and 
                    curr_upper_shadow < curr_body * 0.3 and 
                    curr_body > 0):
                    patterns.append({
                        'pattern_name': 'hammer',
                        'signal_strength': 80,
                        'confidence_score': 0.75,
                        'bullish': True,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Bullish reversal - long lower shadow'
                    })
                
                # HANGING MAN Pattern (like hammer but at top of uptrend)
                if (curr_lower_shadow > curr_body * 2 and 
                    curr_upper_shadow < curr_body * 0.3 and
                    curr_body > 0 and
                    i > -5):  # Check if in potential uptrend
                    recent_trend = np.mean(close_prices[i-3:i]) if abs(i-3) < len(close_prices) else curr_close
                    if curr_close > recent_trend * 1.02:  # In uptrend
                        patterns.append({
                            'pattern_name': 'hanging_man',
                            'signal_strength': -60,
                            'confidence_score': 0.65,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Bearish reversal at top of uptrend'
                        })
                
                # SHOOTING STAR Pattern
                if (curr_upper_shadow > curr_body * 2 and 
                    curr_lower_shadow < curr_body * 0.3 and 
                    curr_body > 0):
                    patterns.append({
                        'pattern_name': 'shooting_star',
                        'signal_strength': -80,
                        'confidence_score': 0.75,
                        'bullish': False,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Bearish reversal - long upper shadow'
                    })
                
                # ENGULFING Patterns (requires previous candle)
                if abs(i-1) < len(close_prices):
                    # Bullish Engulfing
                    if (curr_close > curr_open and  # Current is bullish
                        prev_close < prev_open and  # Previous is bearish
                        curr_open < prev_close and  # Opens below prev close
                        curr_close > prev_open and  # Closes above prev open
                        curr_body > prev_body * 1.1):  # Larger body
                        
                        patterns.append({
                            'pattern_name': 'bullish_engulfing',
                            'signal_strength': 90,
                            'confidence_score': 0.85,
                            'bullish': True,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bullish reversal pattern'
                        })
                    
                    # Bearish Engulfing
                    elif (curr_close < curr_open and  # Current is bearish
                          prev_close > prev_open and  # Previous is bullish
                          curr_open > prev_close and  # Opens above prev close
                          curr_close < prev_open and  # Closes below prev open
                          curr_body > prev_body * 1.1):  # Larger body
                        
                        patterns.append({
                            'pattern_name': 'bearish_engulfing',
                            'signal_strength': -90,
                            'confidence_score': 0.85,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bearish reversal pattern'
                        })
                
                # SPINNING TOP Pattern
                if (curr_body < curr_range * 0.3 and
                    curr_upper_shadow > curr_body * 0.5 and
                    curr_lower_shadow > curr_body * 0.5):
                    patterns.append({
                        'pattern_name': 'spinning_top',
                        'signal_strength': 0,
                        'confidence_score': 0.6,
                        'bullish': None,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Indecision with long shadows both ways'
                    })
            
            # THREE WHITE SOLDIERS Pattern (3 consecutive bullish candles)
            if len(close_prices) >= 3:
                for i in range(-3, 0):
                    if abs(i-2) >= len(close_prices):
                        continue
                    
                    candle1_bull = close_prices[i-2] > open_prices[i-2]
                    candle2_bull = close_prices[i-1] > open_prices[i-1] 
                    candle3_bull = close_prices[i] > open_prices[i]
                    
                    if (candle1_bull and candle2_bull and candle3_bull and
                        close_prices[i-1] > close_prices[i-2] and
                        close_prices[i] > close_prices[i-1]):
                        
                        patterns.append({
                            'pattern_name': 'three_white_soldiers',
                            'signal_strength': 85,
                            'confidence_score': 0.8,
                            'bullish': True,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bullish continuation pattern'
                        })
                        break  # Only detect once
            
            # THREE BLACK CROWS Pattern (3 consecutive bearish candles)
            if len(close_prices) >= 3:
                for i in range(-3, 0):
                    if abs(i-2) >= len(close_prices):
                        continue
                    
                    candle1_bear = close_prices[i-2] < open_prices[i-2]
                    candle2_bear = close_prices[i-1] < open_prices[i-1]
                    candle3_bear = close_prices[i] < open_prices[i]
                    
                    if (candle1_bear and candle2_bear and candle3_bear and
                        close_prices[i-1] < close_prices[i-2] and
                        close_prices[i] < close_prices[i-1]):
                        
                        patterns.append({
                            'pattern_name': 'three_black_crows',
                            'signal_strength': -85,
                            'confidence_score': 0.8,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bearish continuation pattern'
                        })
                        break  # Only detect once
                        
        except Exception as e:
            self.logger.error(f"Error in candlestick pattern detection: {e}")
        
        return patterns
    
    def _detect_chart_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect chart patterns like support/resistance, trends using mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            close_prices = data['Close'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            
            # Trend detection using linear regression
            if len(close_prices) >= 20:
                # Short-term trend (10 days)
                short_x = np.arange(10)
                short_trend = np.polyfit(short_x, close_prices[-10:], 1)[0]
                
                # Long-term trend (20 days) 
                long_x = np.arange(20)
                long_trend = np.polyfit(long_x, close_prices[-20:], 1)[0]
                
                # Calculate trend strength
                price_std = np.std(close_prices[-20:])
                short_strength = abs(short_trend) / price_std if price_std > 0 else 0
                long_strength = abs(long_trend) / price_std if price_std > 0 else 0
                
                # Strong uptrend
                if short_trend > 0 and long_trend > 0 and short_strength > 0.02:
                    patterns.append({
                        'pattern_name': 'strong_uptrend',
                        'strength': min(short_strength * 10, 1.0),
                        'timeframe': 'short_and_long_term',
                        'confidence_score': 0.8,
                        'trend_slope': short_trend,
                        'description': 'Sustained upward price movement'
                    })
                
                # Strong downtrend
                elif short_trend < 0 and long_trend < 0 and short_strength > 0.02:
                    patterns.append({
                        'pattern_name': 'strong_downtrend',
                        'strength': min(short_strength * 10, 1.0),
                        'timeframe': 'short_and_long_term', 
                        'confidence_score': 0.8,
                        'trend_slope': short_trend,
                        'description': 'Sustained downward price movement'
                    })
                
                # Trend reversal
                elif (short_trend > 0 and long_trend < 0) or (short_trend < 0 and long_trend > 0):
                    patterns.append({
                        'pattern_name': 'trend_reversal',
                        'strength': min(abs(short_trend - long_trend) / price_std, 1.0),
                        'timeframe': 'reversal_detected',
                        'confidence_score': 0.7,
                        'description': 'Recent trend change detected'
                    })
            
            # Support and Resistance Levels
            current_price = close_prices[-1]
            
            # Calculate recent highs and lows
            if len(close_prices) >= 20:
                highs_20 = np.max(high_prices[-20:])
                lows_20 = np.min(low_prices[-20:])
                
                # Resistance level analysis
                if current_price >= highs_20 * 0.98:  # Within 2% of resistance
                    resistance_touches = np.sum(high_prices[-20:] >= highs_20 * 0.99)
                    patterns.append({
                        'pattern_name': 'near_resistance',
                        'resistance_level': highs_20,
                        'current_price': current_price,
                        'distance_pct': ((current_price - highs_20) / highs_20) * 100,
                        'touches': int(resistance_touches),
                        'confidence_score': min(0.9, 0.5 + resistance_touches * 0.1),
                        'description': f'Price near resistance at ${highs_20:.2f}'
                    })
                
                # Support level analysis
                if current_price <= lows_20 * 1.02:  # Within 2% of support
                    support_touches = np.sum(low_prices[-20:] <= lows_20 * 1.01)
                    patterns.append({
                        'pattern_name': 'near_support',
                        'support_level': lows_20,
                        'current_price': current_price,
                        'distance_pct': ((current_price - lows_20) / lows_20) * 100,
                        'touches': int(support_touches),
                        'confidence_score': min(0.9, 0.5 + support_touches * 0.1),
                        'description': f'Price near support at ${lows_20:.2f}'
                    })
            
            # Consolidation/Triangle patterns
            if len(close_prices) >= 15:
                recent_highs = high_prices[-15:]
                recent_lows = low_prices[-15:]
                
                # Linear regression on highs and lows
                x_vals = np.arange(len(recent_highs))
                high_trend = np.polyfit(x_vals, recent_highs, 1)[0]
                low_trend = np.polyfit(x_vals, recent_lows, 1)[0]
                
                # Consolidation (horizontal movement)
                if abs(high_trend) < np.std(recent_highs) * 0.1 and abs(low_trend) < np.std(recent_lows) * 0.1:
                    patterns.append({
                        'pattern_name': 'consolidation',
                        'high_trend': high_trend,
                        'low_trend': low_trend,
                        'range_size': (np.max(recent_highs) - np.min(recent_lows)) / current_price * 100,
                        'confidence_score': 0.75,
                        'description': 'Sideways price movement - consolidation phase'
                    })
                
                # Ascending Triangle
                elif abs(high_trend) < np.std(recent_highs) * 0.1 and low_trend > 0:
                    patterns.append({
                        'pattern_name': 'ascending_triangle',
                        'pattern_type': 'continuation',
                        'bias': 'bullish',
                        'confidence_score': 0.7,
                        'description': 'Rising lows with horizontal resistance'
                    })
                
                # Descending Triangle  
                elif abs(low_trend) < np.std(recent_lows) * 0.1 and high_trend < 0:
                    patterns.append({
                        'pattern_name': 'descending_triangle',
                        'pattern_type': 'continuation',
                        'bias': 'bearish',
                        'confidence_score': 0.7,
                        'description': 'Falling highs with horizontal support'
                    })
                
        except Exception as e:
            self.logger.error(f"Error in chart pattern detection: {e}")
        
        return patterns
    
    def _detect_volume_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect volume-based patterns using mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            volume = data['Volume'].values
            close_prices = data['Close'].values
            
            if len(volume) >= 20:
                avg_volume_20 = np.mean(volume[-20:])
                current_volume = volume[-1]
                
                # High volume analysis
                if current_volume > avg_volume_20 * 2:
                    price_change = (close_prices[-1] - close_prices[-2]) / close_prices[-2] * 100
                    
                    patterns.append({
                        'pattern_name': 'high_volume_breakout',
                        'volume_ratio': current_volume / avg_volume_20,
                        'price_change_pct': price_change,
                        'bullish': price_change > 0,
                        'confidence_score': min(0.9, 0.5 + (current_volume / avg_volume_20) * 0.1),
                        'description': f'Volume spike ({current_volume/avg_volume_20:.1f}x average)'
                    })
                
                # Volume trend analysis
                if len(volume) >= 10:
                    volume_trend = np.polyfit(range(10), volume[-10:], 1)[0]
                    volume_strength = abs(volume_trend) / avg_volume_20
                    
                    if volume_strength > 0.1:
                        patterns.append({
                            'pattern_name': 'volume_trend',
                            'trend_direction': 'increasing' if volume_trend > 0 else 'decreasing',
                            'trend_strength': volume_strength,
                            'confidence_score': min(0.8, 0.4 + volume_strength),
                            'description': f'{"Increasing" if volume_trend > 0 else "Decreasing"} volume trend'
                        })
                
                # On-Balance Volume (OBV) analysis
                obv = self._calculate_obv(close_prices, volume)
                if len(obv) >= 10:
                    obv_trend = np.polyfit(range(min(10, len(obv))), obv[-10:], 1)[0]
                    obv_strength = abs(obv_trend) / np.std(obv[-20:]) if len(obv) >= 20 else 0
                    
                    if obv_strength > 0.1:
                        patterns.append({
                            'pattern_name': 'obv_divergence',
                            'obv_trend': 'bullish' if obv_trend > 0 else 'bearish',
                            'strength': obv_strength,
                            'confidence_score': min(0.8, 0.5 + obv_strength),
                            'description': f'OBV showing {"accumulation" if obv_trend > 0 else "distribution"}'
                        })
                
                # Volume Price Trend (VPT) analysis
                vpt = self._calculate_vpt(close_prices, volume)
                if len(vpt) >= 10:
                    vpt_trend = np.polyfit(range(min(10, len(vpt))), vpt[-10:], 1)[0]
                    
                    if abs(vpt_trend) > np.std(vpt[-20:]) * 0.1 if len(vpt) >= 20 else 0:
                        patterns.append({
                            'pattern_name': 'volume_price_trend',
                            'vpt_direction': 'positive' if vpt_trend > 0 else 'negative',
                            'strength': abs(vpt_trend),
                            'confidence_score': 0.7,
                            'description': f'Volume-price trend is {"positive" if vpt_trend > 0 else "negative"}'
                        })
                
                # Low volume warning
                if current_volume < avg_volume_20 * 0.5:
                    patterns.append({
                        'pattern_name': 'low_volume_warning',
                        'volume_ratio': current_volume / avg_volume_20,
                        'confidence_score': 0.6,
                        'description': 'Unusually low volume - reduced reliability'
                    })
                    
        except Exception as e:
            self.logger.error(f"Error in volume pattern detection: {e}")
        
        return patterns
    
    def _calculate_obv(self, prices: np.array, volumes: np.array) -> np.array:
        """Calculate On-Balance Volume manually"""
        obv = np.zeros(len(prices))
        if len(prices) == 0:
            return obv
            
        obv[0] = volumes[0]
        
        for i in range(1, len(prices)):
            if prices[i] > prices[i-1]:
                obv[i] = obv[i-1] + volumes[i]
            elif prices[i] < prices[i-1]:
                obv[i] = obv[i-1] - volumes[i]
            else:
                obv[i] = obv[i-1]
        
        return obv
    
    def _calculate_vpt(self, prices: np.array, volumes: np.array) -> np.array:
        """Calculate Volume Price Trend manually"""
        vpt = np.zeros(len(prices))
        if len(prices) <= 1:
            return vpt
            
        vpt[0] = volumes[0]
        
        for i in range(1, len(prices)):
            price_change_pct = (prices[i] - prices[i-1]) / prices[i-1] if prices[i-1] != 0 else 0
            vpt[i] = vpt[i-1] + (volumes[i] * price_change_pct)
        
        return vpt
    
    def _calculate_pattern_score(self, candlestick: List, chart: List, volume: List) -> float:
        """Calculate overall pattern strength score"""
        score = 0.0
        
        # Candlestick patterns contribute 40%
        if candlestick:
            candlestick_score = sum([p.get('confidence_score', 0) for p in candlestick]) / len(candlestick)
            score += candlestick_score * 0.4
        
        # Chart patterns contribute 35%
        if chart:
            chart_score = sum([p.get('confidence_score', 0) for p in chart]) / len(chart)
            score += chart_score * 0.35
        
        # Volume patterns contribute 25%
        if volume:
            volume_score = sum([p.get('confidence_score', 0) for p in volume]) / len(volume)
            score += volume_score * 0.25
        
        return min(score, 1.0)
    
    def _save_pattern_analysis(self, analysis_data: Dict):
        """Save pattern analysis to database with retry logic"""
        try:
            data = {
                'symbol': analysis_data['symbol'],
                'detection_date': datetime.now().isoformat(),
                'pattern_category': 'advanced_combined',
                'pattern_score': analysis_data['overall_pattern_score'],
                'candlestick_patterns_count': len(analysis_data.get('candlestick_patterns', [])),
                'chart_patterns_count': len(analysis_data.get('chart_patterns', [])),
                'volume_patterns_count': len(analysis_data.get('volume_patterns', [])),
                'pattern_metadata': json.dumps(analysis_data),
                'created_at': datetime.now().isoformat()
            }
            
            if USE_DB_UTILS:
                # Use database utilities with retry logic
                success = self.save_to_database('advanced_patterns', data)
                if success:
                    self.logger.info(f"Pattern analysis saved with retry logic for {analysis_data['symbol']}")
                else:
                    self.logger.error(f"Failed to save pattern analysis for {analysis_data['symbol']}")
            else:
                # Fallback to direct connection
                conn = sqlite3.connect(self.db_path, timeout=30)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO advanced_patterns 
                    (symbol, detection_date, pattern_category, pattern_score, 
                     candlestick_patterns_count, chart_patterns_count, volume_patterns_count,
                     pattern_metadata, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data['symbol'],
                    data['detection_date'],
                    data['pattern_category'],
                    data['pattern_score'],
                    data['candlestick_patterns_count'],
                    data['chart_patterns_count'],
                    data['volume_patterns_count'],
                    data['pattern_metadata'],
                    data['created_at']
                ))
                
                conn.commit()
                conn.close()
                self.logger.info(f"Pattern analysis saved for {analysis_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving pattern analysis: {e}")
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        db_mode = "with retry logic" if USE_DB_UTILS else "direct connection"
        self.logger.info(f"Starting Pattern Recognition Service on port 5006 {mode}, database {db_mode}")
        self.app.run(host='0.0.0.0', port=5006, debug=False)

if __name__ == "__main__":
    service = PatternRecognitionService()
    service.run()


================================================================================
FILE: ./project_snapshot.txt
================================================================================

PROJECT SNAPSHOT
==================================================


================================================================================
FILE: ./Config_Codespacd.py
================================================================================

#!/usr/bin/env python3
"""
Setup script for GitHub Codespace
Configures the Trading Application environment
"""
import os
import sys
import subprocess
import sqlite3
from pathlib import Path

def create_directories():
    """Create necessary directories"""
    directories = [
        'logs',
        'backups',
        'project_documentation',
        'updates',
        '.update_state'
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"‚úì Created {directory}/")

def update_service_paths():
    """Update paths in service files from Colab to Codespace format"""
    services = [
        'coordination_service.py',
        'security_scanner.py',
        'pattern_analysis.py',
        'technical_analysis.py',
        'paper_trading.py',
        'pattern_recognition_service.py',
        'news_service.py',
        'reporting_service.py',
        'web_dashboard.py',
        'hybrid_manager.py',
        'database_migration.py',
        'diagnostic_toolkit.py'
    ]
    
    replacements = [
        ('./trading_system.db', './trading_system.db'),
        ('./logs/', './logs/'),
        ('./backups/', './backups/'),
        ('/conten./', './'),
        ('./', './')
    ]
    
    for service in services:
        if Path(service).exists():
            with open(service, 'r') as f:
                content = f.read()
            
            original_content = content
            for old_path, new_path in replacements:
                content = content.replace(old_path, new_path)
            
            # Remove Colab-specific imports
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if 'from google.colab import' not in line and 'import google.colab' not in line:
                    filtered_lines.append(line)
                else:
                    filtered_lines.append(f"# {line}  # Removed for Codespace")
            
            content = '\n'.join(filtered_lines)
            
            if content != original_content:
                with open(service, 'w') as f:
                    f.write(content)
                print(f"‚úì Updated paths in {service}")

def initialize_database():
    """Initialize the database"""
    if Path('database_migration.py').exists():
        print("\nüîß Initializing database...")
        result = subprocess.run([sys.executable, 'database_migration.py'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úì Database initialized successfully")
        else:
            print(f"‚ö†Ô∏è  Database initialization warning: {result.stderr}")
    else:
        print("‚ö†Ô∏è  database_migration.py not found")

def create_requirements_file():
    """Create requirements.txt if it doesn't exist"""
    if not Path('requirements.txt').exists():
        requirements = """flask==3.0.0
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
scikit-learn==1.3.2
yfinance==0.2.33
alpaca-py==0.21.1
psutil==5.9.6
python-dateutil==2.8.2
pytz==2023.3
beautifulsoup4==4.12.2
"""
        with open('requirements.txt', 'w') as f:
            f.write(requirements)
        print("‚úì Created requirements.txt")

def create_startup_script():
    """Create a quick startup script"""
    startup_content = """#!/bin/bash
# Quick startup script for Trading Application

echo "üöÄ Starting Trading Application Services..."
python hybrid_manager.py
"""
    
    with open('start_trading.sh', 'w') as f:
        f.write(startup_content)
    
    os.chmod('start_trading.sh', 0o755)
    print("‚úì Created start_trading.sh")

def main():
    print("üîß Setting up Trading Application for GitHub Codespaces\n")
    
    # Check if we're in the right directory
    if not any(Path(f).exists() for f in ['coordination_service.py', 'hybrid_manager.py']):
        print("‚ö†Ô∏è  Warning: Trading application files not found in current directory")
        print("   Make sure you're in the root of your Trading_Application repository")
        response = input("\nContinue anyway? (y/n): ")
        if response.lower() != 'y':
            return
    
    create_directories()
    create_requirements_file()
    update_service_paths()
    initialize_database()
    create_startup_script()
    
    print("\n‚úÖ Codespace setup complete!")
    print("\nüìã Next steps:")
    print("1. Install requirements: pip install -r requirements.txt")
    print("2. Run diagnostic check: python diagnostic_toolkit.py --report")
    print("3. Start services: python hybrid_manager.py")
    print("   OR use: ./start_trading.sh")
    print("\nüí° Tip: The web dashboard will be available at port 8080")

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Google_drive_auth.py
================================================================================

# drive_auth.py - Google Drive Authentication Module
import json
import os
from pathlib import Path
from google.colab import userdata
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
import io
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload

class ColabDriveManager:
    def __init__(self):
        self.service = None
        self.project_folder_id = None
        self.project_root = None
        self._authenticate()
        self._setup_project_structure()

    def _authenticate(self):
        """Authenticate using service account credentials from Colab secrets"""
        try:
            # Get service account credentials from Colab secrets
            service_account_info = {
                "type": "service_account",
                "project_id": userdata.get('GOOGLE_PROJECT_ID'),
                "private_key_id": userdata.get('GOOGLE_PRIVATE_KEY_ID'),
                "private_key": userdata.get('GOOGLE_PRIVATE_KEY').replace('\\n', '\n'),
                "client_email": userdata.get('GOOGLE_CLIENT_EMAIL'),
                "client_id": userdata.get('GOOGLE_CLIENT_ID'),
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "client_x509_cert_url": f"https://www.googleapis.com/robot/v1/metadata/x509/{userdata.get('GOOGLE_CLIENT_EMAIL')}"
            }

            # Create credentials
            credentials = Credentials.from_service_account_info(
                service_account_info,
                scopes=['https://www.googleapis.com/auth/drive']
            )

            # Build the service
            self.service = build('drive', 'v3', credentials=credentials)
            print("‚úÖ Google Drive API authenticated successfully")
            return True

        except Exception as e:
            print(f"‚ùå Service account authentication failed: {e}")
            print("üîÑ Falling back to traditional Drive mounting...")
            return self._fallback_mount()

    def _fallback_mount(self):
        """Fallback to traditional Google Drive mounting"""
        try:
            from google.colab import drive
            drive.mount('./drive')

            # Set up local file system paths
            self.project_root = Path('./drive/MyDrive/TradingSystem_Phase1')
            self.project_root.mkdir(exist_ok=True)

            print("‚úÖ Google Drive mounted successfully (fallback mode)")
            return True

        except Exception as e:
            print(f"‚ùå Drive mounting failed: {e}")
            return False

    def _setup_project_structure(self):
        """Setup project folder structure"""
        if self.service:
            self._setup_api_folders()
        else:
            self._setup_local_folders()

    def _setup_api_folders(self):
        """Setup folder structure using Drive API"""
        try:
            # Find or create main project folder
            folder_name = 'TradingSystem_Phase1'

            # Search for existing folder
            results = self.service.files().list(
                q=f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder'",
                fields="files(id, name)"
            ).execute()

            if results['files']:
                self.project_folder_id = results['files'][0]['id']
                print(f"‚úÖ Found existing project folder: {folder_name}")
            else:
                # Create new folder
                folder_metadata = {
                    'name': folder_name,
                    'mimeType': 'application/vnd.google-apps.folder'
                }
                folder = self.service.files().create(body=folder_metadata).execute()
                self.project_folder_id = folder['id']
                print(f"‚úÖ Created project folder: {folder_name}")

            # Create subfolders
            subfolders = ['data', 'models', 'logs', 'config', 'backups', 'reports', 'coordination']
            for subfolder in subfolders:
                self._create_subfolder(subfolder)

        except Exception as e:
            print(f"‚ùå Error setting up API folders: {e}")

    def _create_subfolder(self, subfolder_name):
        """Create a subfolder in the project directory"""
        try:
            # Check if subfolder exists
            results = self.service.files().list(
                q=f"name='{subfolder_name}' and parents in '{self.project_folder_id}' and mimeType='application/vnd.google-apps.folder'",
                fields="files(id, name)"
            ).execute()

            if not results['files']:
                # Create subfolder
                folder_metadata = {
                    'name': subfolder_name,
                    'mimeType': 'application/vnd.google-apps.folder',
                    'parents': [self.project_folder_id]
                }
                self.service.files().create(body=folder_metadata).execute()
                print(f"   üìÅ Created subfolder: {subfolder_name}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è Error creating subfolder {subfolder_name}: {e}")

    def _setup_local_folders(self):
        """Setup folder structure using local file system"""
        if self.project_root:
            subfolders = ['data', 'models', 'logs', 'config', 'backups', 'reports', 'coordination']
            for subfolder in subfolders:
                (self.project_root / subfolder).mkdir(exist_ok=True)
            print("‚úÖ Local project structure created")

    def get_project_path(self, subfolder: str = None) -> Path:
        """Get path to project or subfolder"""
        if self.project_root:
            # Local file system mode
            if subfolder:
                return self.project_root / subfolder
            return self.project_root
        else:
            # API mode - return a simulated path for compatibility
            if subfolder:
                return Path(f"/drive_api/TradingSystem_Phase1/{subfolder}")
            return Path("/drive_api/TradingSystem_Phase1")

    def get_database_path(self) -> str:
        """Get the database path"""
        if self.service:
            # For API mode, we'll use a local temp file that syncs with Drive
            return "/tmp/trading.db"
        else:
            return str(self.project_root / "data" / "trading.db")

# Global instance for easy importing
drive_manager = None

def get_drive_manager():
    """Get or create global drive manager instance"""
    global drive_manager
    if drive_manager is None:
        drive_manager = ColabDriveManager()
    return drive_manager

def init_drive_connection():
    """Initialize drive connection - call this in each notebook"""
    return get_drive_manager()

# Convenience functions for easy use
def get_project_path(subfolder: str = None):
    """Get project path"""
    return get_drive_manager().get_project_path(subfolder)

def save_json(filename: str, data: dict, subfolder: str = None):
    """Save JSON data to drive"""
    dm = get_drive_manager()
    if dm.project_root:
        # Local file system mode
        if subfolder:
            path = dm.project_root / subfolder / filename
        else:
            path = dm.project_root / filename
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        return True
    return False

def load_json(filename: str, subfolder: str = None):
    """Load JSON data from drive"""
    dm = get_drive_manager()
    if dm.project_root:
        # Local file system mode
        if subfolder:
            path = dm.project_root / subfolder / filename
        else:
            path = dm.project_root / filename
        if path.exists():
            with open(path, 'r') as f:
                return json.load(f)
    return None

def get_database_path():
    """Get database path"""
    return get_drive_manager().get_database_path()

print("‚úÖ Drive Authentication Module loaded")
print("üí° Usage: drive_manager = init_drive_connection()")


================================================================================
FILE: ./README.md
================================================================================

# Trading_Application
Trading Application


================================================================================
FILE: ./Test.txt
================================================================================



================================================================================
FILE: ./coordination_service.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v106.py
Version: 1.0.6
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.6 (2025-06-22) - Made paths environment-agnostic, works in any directory
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Now works in any environment (Colab, Codespaces, local, etc.)
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time
from pathlib import Path

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path=None):
        self.app = Flask(__name__)
        self.port = port
        
        # Determine paths based on environment
        self.base_dir = Path.cwd()  # Current working directory
        
        # Set database path
        if db_path:
            self.db_path = db_path
        else:
            # Look for existing database or create in current directory
            if Path('/content/trading_system.db').exists():
                self.db_path = '/content/trading_system.db'
            elif (self.base_dir / 'trading_system.db').exists():
                self.db_path = str(self.base_dir / 'trading_system.db')
            else:
                self.db_path = str(self.base_dir / 'trading_system.db')
        
        print(f"Using database: {self.db_path}")
        
        # Setup logging with environment-appropriate paths
        self.log_dir = self.base_dir / 'logs'
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(self.db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        """Setup logging with environment-appropriate paths"""
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory in current working directory
        self.log_dir.mkdir(exist_ok=True)
        
        log_file = self.log_dir / 'coordination_service.log'
        handler = logging.FileHandler(str(log_file))
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        logger.info(f"Logging to: {log_file}")
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                'status': 'healthy', 
                'service': 'coordination',
                'environment': {
                    'base_dir': str(self.base_dir),
                    'log_dir': str(self.log_dir),
                    'db_path': self.db_path
                }
            }), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.logger.info(f"Working directory: {self.base_dir}")
        self.logger.info(f"Database path: {self.db_path}")
        self.logger.info(f"Log directory: {self.log_dir}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()

================================================================================
FILE: ./coordination_service_backup_20250622_133933.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()

================================================================================
FILE: ./coordination_service_backup_20250622_134230.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()


================================================================================
FILE: ./coordination_service_v105.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: coordination_service_v105.py
Version: 1.0.5
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.5 (2025-06-22) - Fixed schema mismatch: use host/port from service_coordination table
- v1.0.4 (2025-06-20) - Fixed table name mismatch (use service_coordination not service_registry)
- v1.0.3 (2025-06-19) - Added trading schedule endpoints for automated trading
- v1.0.2 (2025-06-19) - Enhanced with persistent registration and auto-registration
- v1.0.1 (2025-06-11) - Implemented database utilities for retry logic
- v1.0.0 (2025-06-11) - Initial release with standardized authentication

PURPOSE:
Coordination Service - Central orchestrator for all trading system services
Manages service discovery, workflow coordination, and health monitoring
Fixed to match actual database schema from database_migration.py
"""

import os
import requests
import logging
import sqlite3
import threading
import json
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional
import time

# Try to import database utilities
try:
    from database_utils import DatabaseManager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found. Running without retry logic.")

class CoordinationService:
    def __init__(self, port=5000, db_path='/content/trading_system.db'):
        self.app = Flask(__name__)
        self.port = port
        self.db_path = db_path
        self.logger = self._setup_logging()
        
        # Initialize database utilities if available
        if USE_DB_UTILS:
            self.db_manager = DatabaseManager(db_path)
        
        # Service registry - in memory for fast access
        self.service_registry = {}
        
        # Trading schedule configuration
        self.schedule_config = {
            "enabled": False,
            "interval_minutes": 30,
            "market_hours_only": True,
            "start_time": "09:30",
            "end_time": "16:00",
            "timezone": "America/New_York",
            "excluded_days": ["Saturday", "Sunday"],
            "last_run": None,
            "next_run": None
        }
        
        # Initialize database tables
        self._init_database()
        
        # Load configuration from database
        self._load_schedule_config()
        
        # Setup routes and background tasks
        self._setup_routes()
        self._load_service_registry()
        self._start_background_tasks()
        
    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('CoordinationService')
        
        # Create logs directory if it doesn't exist
        os.makedirs('/content/logs', exist_ok=True)
        
        handler = logging.FileHandler('/content/logs/coordination_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_database(self):
        """Initialize database tables"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Create service_coordination table (matching database_migration.py schema)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS service_coordination (
                    service_name TEXT PRIMARY KEY,
                    host TEXT NOT NULL,
                    port INTEGER NOT NULL,
                    status TEXT NOT NULL,
                    last_heartbeat TIMESTAMP,
                    start_time TIMESTAMP,
                    metadata TEXT
                )
            ''')
            
            # Create trading_schedule_config table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_schedule_config (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    config TEXT NOT NULL
                )
            ''')
            
            # Create trading_cycles table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS trading_cycles (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    cycle_id TEXT UNIQUE NOT NULL,
                    status TEXT NOT NULL,
                    start_time TIMESTAMP,
                    end_time TIMESTAMP,
                    securities_scanned INTEGER DEFAULT 0,
                    patterns_found INTEGER DEFAULT 0,
                    trades_executed INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("Database tables initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
        
    def _load_service_registry(self):
        """Load service registry from database on startup - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Load from service_coordination table with correct column names
            cursor.execute('SELECT service_name, host, port, status, last_heartbeat FROM service_coordination')
            for row in cursor.fetchall():
                service_name = row[0]
                host = row[1]
                port = row[2]
                status = row[3]
                last_heartbeat = row[4]
                
                # Build URL from host and port
                url = f"http://{host}:{port}"
                
                self.service_registry[service_name] = {
                    'url': url,
                    'host': host,
                    'port': port,
                    'status': status,
                    'last_heartbeat': last_heartbeat
                }
                
            conn.close()
            self.logger.info(f"Loaded {len(self.service_registry)} services from database")
            
        except Exception as e:
            self.logger.error(f"Error loading service registry: {e}")
            
    def _save_schedule_config(self):
        """Save schedule configuration to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Save config
            cursor.execute('''
                INSERT OR REPLACE INTO trading_schedule_config (id, config)
                VALUES (1, ?)
            ''', (json.dumps(self.schedule_config),))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error saving schedule config: {e}")
            
    def _load_schedule_config(self):
        """Load schedule configuration from database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT config FROM trading_schedule_config WHERE id = 1')
            row = cursor.fetchone()
            
            if row:
                self.schedule_config = json.loads(row[0])
                
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error loading schedule config: {e}")
            
    def _persist_service_registration(self, service_name: str, port: int):
        """Persist service registration to database - FIXED TO MATCH SCHEMA"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Insert or update with correct column names
            cursor.execute('''
                INSERT OR REPLACE INTO service_coordination 
                (service_name, host, port, status, last_heartbeat, start_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                service_name,
                'localhost',  # host
                port,         # port
                'active',     # status
                datetime.now().isoformat(),  # last_heartbeat
                datetime.now().isoformat()   # start_time
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error persisting service registration: {e}")
            
    def _setup_routes(self):
        """Setup Flask routes"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy', 'service': 'coordination'}), 200
            
        @self.app.route('/register', methods=['POST'])
        def register_service():
            data = request.json
            service_name = data.get('service_name')
            port = data.get('port')
            
            if not service_name or not port:
                return jsonify({'error': 'service_name and port required'}), 400
                
            # Register service
            self.service_registry[service_name] = {
                'url': f"http://localhost:{port}",
                'host': 'localhost',
                'port': port,
                'status': 'active',
                'last_heartbeat': datetime.now().isoformat()
            }
            
            # Persist to database
            self._persist_service_registration(service_name, port)
            
            self.logger.info(f"Registered service: {service_name} on port {port}")
            return jsonify({'status': 'registered'}), 200
            
        @self.app.route('/services', methods=['GET'])
        def get_services():
            return jsonify(self.service_registry), 200
            
        @self.app.route('/services/status', methods=['GET'])
        def get_services_status():
            """Get comprehensive status of all known services"""
            comprehensive_status = []
            
            # Check all known services including those not registered
            all_services = {
                "coordination": 5000,
                "scanner": 5001,
                "pattern": 5002,
                "technical": 5003,
                "trading": 5005,
                "pattern_rec": 5006,
                "news": 5008,
                "reporting": 5009,
                "dashboard": 5004
            }
            
            for service_name, default_port in all_services.items():
                if service_name in self.service_registry:
                    # Service is registered
                    info = self.service_registry[service_name]
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    comprehensive_status.append({
                        'name': service_name,
                        'url': info['url'],
                        'port': port,
                        'registered': True,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': info.get('last_heartbeat', 'unknown')
                    })
                else:
                    # Not registered, check if it's running
                    is_healthy = self._check_service_health(service_name, default_port)
                    
                    # Auto-register if healthy
                    if is_healthy:
                        self.service_registry[service_name] = {
                            'url': f"http://localhost:{default_port}",
                            'host': 'localhost',
                            'port': default_port,
                            'status': 'active',
                            'last_heartbeat': datetime.now().isoformat()
                        }
                        self._persist_service_registration(service_name, default_port)
                        
                    comprehensive_status.append({
                        'name': service_name,
                        'url': f"http://localhost:{default_port}",
                        'port': default_port,
                        'registered': is_healthy,
                        'status': 'active' if is_healthy else 'inactive',
                        'healthy': is_healthy,
                        'last_heartbeat': datetime.now().isoformat() if is_healthy else 'never'
                    })
                    
            return jsonify(comprehensive_status), 200
            
        @self.app.route('/trigger/scan', methods=['POST'])
        def trigger_scan():
            """Trigger a security scan across the system"""
            try:
                # Start a new trading cycle
                cycle_id = f"CYCLE_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Record cycle start
                self._start_trading_cycle(cycle_id)
                
                # Trigger scanner service
                if 'scanner' in self.service_registry:
                    scanner_url = self.service_registry['scanner']['url']
                    response = requests.post(f"{scanner_url}/scan/all", timeout=5)
                    
                    if response.status_code == 200:
                        return jsonify({
                            'status': 'scan_initiated',
                            'cycle_id': cycle_id,
                            'timestamp': datetime.now().isoformat()
                        }), 200
                        
                return jsonify({'error': 'Scanner service not available'}), 503
                
            except Exception as e:
                self.logger.error(f"Error triggering scan: {e}")
                return jsonify({'error': str(e)}), 500
                
        @self.app.route('/schedule', methods=['GET'])
        def get_schedule():
            """Get current trading schedule configuration"""
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/schedule', methods=['PUT'])
        def update_schedule():
            """Update trading schedule configuration"""
            data = request.json
            
            # Update configuration
            for key in ['enabled', 'interval_minutes', 'market_hours_only', 
                       'start_time', 'end_time', 'timezone', 'excluded_days']:
                if key in data:
                    self.schedule_config[key] = data[key]
                    
            # Save to database
            self._save_schedule_config()
            
            # Calculate next run time if enabled
            if self.schedule_config['enabled']:
                self.schedule_config['next_run'] = self._calculate_next_run()
                
            return jsonify(self.schedule_config), 200
            
        @self.app.route('/cycles', methods=['GET'])
        def get_trading_cycles():
            """Get recent trading cycles"""
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    SELECT cycle_id, status, start_time, end_time, 
                           securities_scanned, patterns_found, trades_executed, error_count
                    FROM trading_cycles
                    ORDER BY created_at DESC
                    LIMIT 20
                ''')
                
                cycles = []
                for row in cursor.fetchall():
                    cycles.append({
                        'cycle_id': row[0],
                        'status': row[1],
                        'start_time': row[2],
                        'end_time': row[3],
                        'securities_scanned': row[4],
                        'patterns_found': row[5],
                        'trades_executed': row[6],
                        'error_count': row[7]
                    })
                    
                conn.close()
                return jsonify(cycles), 200
                
            except Exception as e:
                self.logger.error(f"Error getting trading cycles: {e}")
                return jsonify({'error': str(e)}), 500
                
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
            
    def _start_trading_cycle(self, cycle_id: str):
        """Record the start of a trading cycle"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_cycles (cycle_id, status, start_time)
                VALUES (?, ?, ?)
            ''', (cycle_id, 'RUNNING', datetime.now().isoformat()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"Error starting trading cycle: {e}")
            
    def _calculate_next_run(self) -> str:
        """Calculate next scheduled run time"""
        now = datetime.now()
        next_run = now + timedelta(minutes=self.schedule_config['interval_minutes'])
        
        # If market hours only, adjust to next market open
        if self.schedule_config['market_hours_only']:
            # Simple implementation - would need timezone handling in production
            if next_run.weekday() >= 5:  # Weekend
                days_until_monday = 7 - next_run.weekday()
                next_run = next_run + timedelta(days=days_until_monday)
                next_run = next_run.replace(hour=9, minute=30, second=0)
                
        return next_run.isoformat()
        
    def _scheduled_trading_cycle(self):
        """Execute scheduled trading cycles"""
        while True:
            try:
                if self.schedule_config['enabled']:
                    # Check if it's time to run
                    now = datetime.now()
                    next_run_str = self.schedule_config.get('next_run')
                    
                    if next_run_str:
                        next_run = datetime.fromisoformat(next_run_str)
                        
                        if now >= next_run:
                            # Trigger trading cycle
                            self.logger.info("Executing scheduled trading cycle")
                            
                            # Make internal request to trigger scan
                            requests.post(f"http://localhost:{self.port}/trigger/scan")
                            
                            # Update last run and calculate next run
                            self.schedule_config['last_run'] = now.isoformat()
                            self.schedule_config['next_run'] = self._calculate_next_run()
                            self._save_schedule_config()
                            
                # Sleep for 30 seconds before checking again
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in scheduled trading cycle: {e}")
                time.sleep(60)  # Wait longer on error
                
    def _monitor_services(self):
        """Background task to monitor service health"""
        while True:
            try:
                for service_name, info in list(self.service_registry.items()):
                    port = info['port']
                    is_healthy = self._check_service_health(service_name, port)
                    
                    if is_healthy:
                        info['status'] = 'active'
                        info['last_heartbeat'] = datetime.now().isoformat()
                    else:
                        info['status'] = 'inactive'
                        
                # Sleep for 30 seconds
                time.sleep(30)
                
            except Exception as e:
                self.logger.error(f"Error in service monitoring: {e}")
                time.sleep(60)
                
    def _start_background_tasks(self):
        """Start background monitoring tasks"""
        # Start service monitor
        monitor_thread = threading.Thread(target=self._monitor_services)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start scheduled trading
        schedule_thread = threading.Thread(target=self._scheduled_trading_cycle)
        schedule_thread.daemon = True
        schedule_thread.start()
        
        self.logger.info("Background tasks started")
        
    def run(self):
        """Start the Flask application"""
        self.logger.info(f"Starting Coordination Service on port {self.port}")
        self.app.run(host='0.0.0.0', port=self.port, debug=False)

if __name__ == "__main__":
    service = CoordinationService()
    service.run()


================================================================================
FILE: ./create_project_snapshot.py
================================================================================

import os
import fnmatch

# Define patterns to include and exclude
include_patterns = ['*.py', '*.md', '*.txt', '*.json', '*.yml', '*.yaml', 'requirements.txt', '.env.example']
exclude_dirs = ['.git', '__pycache__', 'backups', 'logs', '.vscode', 'venv', 'env']

def should_include_file(filepath):
    """Check if file should be included based on patterns"""
    filename = os.path.basename(filepath)
    return any(fnmatch.fnmatch(filename, pattern) for pattern in include_patterns)

def create_project_snapshot(output_file='project_snapshot.txt'):
    """Create a comprehensive snapshot of the project"""
    with open(output_file, 'w') as f:
        # Write header
        f.write("PROJECT SNAPSHOT\n")
        f.write("=" * 50 + "\n\n")
        
        # Walk through directory tree
        for root, dirs, files in os.walk('.'):
            # Remove excluded directories
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            # Process files
            for file in sorted(files):
                filepath = os.path.join(root, file)
                if should_include_file(filepath):
                    f.write(f"\n{'=' * 80}\n")
                    f.write(f"FILE: {filepath}\n")
                    f.write(f"{'=' * 80}\n\n")
                    
                    try:
                        with open(filepath, 'r', encoding='utf-8') as src:
                            f.write(src.read())
                    except Exception as e:
                        f.write(f"Error reading file: {e}\n")
                    
                    f.write("\n")
        
        print(f"Project snapshot created: {output_file}")

if __name__ == "__main__":
    create_project_snapshot()


================================================================================
FILE: ./database_migration.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM DATABASE MIGRATION
Service: Database Migration Script
Version: 1.0.5
Last Updated: 2025-06-20

REVISION HISTORY:
- v1.0.6 (2025-06-20) db_path ./trading_database.db
- v1.0.5 (2025-06-20) - Fixed verify_schema connection handling, ensured consistent table naming
- v1.0.4 (2025-06-19) - Enhanced startup coordination and health check improvements  
- v1.0.3 (2025-06-17) - Initial database schema with comprehensive trading tables

This script creates and manages the SQLite database schema for the trading system.
It includes tables for:
- Service coordination and registry
- Security scanning results
- Pattern analysis
- Technical indicators
- ML model predictions
- Strategy evaluations
- Risk metrics
- Orders and executions
- News sentiment analysis
"""

import sqlite3
import logging
from datetime import datetime
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('database_migration')

class DatabaseMigration:
    def __init__(self, db_path='./trading_system.db'):
        self.db_path = './trading_system.db'
        self.tables_created = []
        
    def create_connection(self):
        """Create database connection"""
        return sqlite3.connect(self.db_path)
    
    def execute_migration(self):
        """Execute all database migrations"""
        logger.info(f"Starting database migration for {self.db_path}")
        
        try:
            # Create all tables
            self.create_service_coordination_table()
            self.create_scanning_results_table()
            self.create_pattern_analysis_table()
            self.create_technical_indicators_table()
            self.create_ml_predictions_table()
            self.create_strategy_evaluations_table()
            self.create_risk_metrics_table()
            self.create_orders_table()
            self.create_news_sentiment_table()
            
            # Verify schema
            self.verify_schema()
            
            logger.info("Database migration completed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Database migration failed: {e}")
            return False
    
    def create_service_coordination_table(self):
        """Create service coordination table for service registry"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS service_coordination (
                service_name TEXT PRIMARY KEY,
                host TEXT NOT NULL,
                port INTEGER NOT NULL,
                status TEXT NOT NULL,
                last_heartbeat TIMESTAMP,
                start_time TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('service_coordination')
        logger.info("Created service_coordination table")
    
    def create_scanning_results_table(self):
        """Create table for security scanner results"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scanning_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                scan_timestamp TIMESTAMP NOT NULL,
                price REAL,
                volume INTEGER,
                change_percent REAL,
                relative_volume REAL,
                market_cap REAL,
                scan_type TEXT,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes for performance
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_scanning_symbol 
            ON scanning_results(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_scanning_timestamp 
            ON scanning_results(scan_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('scanning_results')
        logger.info("Created scanning_results table")
    
    def create_pattern_analysis_table(self):
        """Create table for pattern analysis results"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pattern_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                pattern_type TEXT NOT NULL,
                pattern_name TEXT NOT NULL,
                confidence REAL,
                entry_price REAL,
                stop_loss REAL,
                target_price REAL,
                timeframe TEXT,
                detection_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pattern_symbol 
            ON pattern_analysis(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pattern_timestamp 
            ON pattern_analysis(detection_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('pattern_analysis')
        logger.info("Created pattern_analysis table")
    
    def create_technical_indicators_table(self):
        """Create table for technical indicators"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS technical_indicators (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                indicator_name TEXT NOT NULL,
                indicator_value REAL,
                signal TEXT,
                timeframe TEXT,
                calculation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_technical_symbol 
            ON technical_indicators(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_technical_indicator 
            ON technical_indicators(indicator_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('technical_indicators')
        logger.info("Created technical_indicators table")
    
    def create_ml_predictions_table(self):
        """Create table for ML model predictions"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ml_predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                model_name TEXT NOT NULL,
                model_version TEXT,
                prediction_type TEXT NOT NULL,
                prediction_value REAL,
                confidence REAL,
                features_used TEXT,
                prediction_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ml_symbol 
            ON ml_predictions(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ml_model 
            ON ml_predictions(model_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('ml_predictions')
        logger.info("Created ml_predictions table")
    
    def create_strategy_evaluations_table(self):
        """Create table for strategy evaluations"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS strategy_evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                strategy_name TEXT NOT NULL,
                entry_signal TEXT,
                entry_price REAL,
                stop_loss REAL,
                target_price REAL,
                position_size INTEGER,
                risk_reward_ratio REAL,
                expected_return REAL,
                confidence_score REAL,
                evaluation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_symbol 
            ON strategy_evaluations(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_name 
            ON strategy_evaluations(strategy_name)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('strategy_evaluations')
        logger.info("Created strategy_evaluations table")
    
    def create_risk_metrics_table(self):
        """Create table for risk management metrics"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS risk_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT,
                portfolio_value REAL,
                position_size REAL,
                risk_amount REAL,
                risk_percentage REAL,
                max_positions INTEGER,
                current_positions INTEGER,
                daily_loss_limit REAL,
                current_daily_loss REAL,
                var_95 REAL,
                sharpe_ratio REAL,
                calculation_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_risk_timestamp 
            ON risk_metrics(calculation_timestamp)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('risk_metrics')
        logger.info("Created risk_metrics table")
    
    def create_orders_table(self):
        """Create table for orders and executions"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS orders (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                order_id TEXT UNIQUE NOT NULL,
                symbol TEXT NOT NULL,
                order_type TEXT NOT NULL,
                side TEXT NOT NULL,
                quantity INTEGER NOT NULL,
                price REAL,
                stop_price REAL,
                status TEXT NOT NULL,
                filled_quantity INTEGER DEFAULT 0,
                average_fill_price REAL,
                commission REAL,
                strategy_name TEXT,
                entry_reason TEXT,
                exit_reason TEXT,
                created_timestamp TIMESTAMP NOT NULL,
                updated_timestamp TIMESTAMP,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_symbol 
            ON orders(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_status 
            ON orders(status)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_orders_order_id 
            ON orders(order_id)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('orders')
        logger.info("Created orders table")
    
    def create_news_sentiment_table(self):
        """Create table for news sentiment analysis"""
        conn = self.create_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_sentiment (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                headline TEXT NOT NULL,
                source TEXT,
                article_date TIMESTAMP NOT NULL,
                sentiment_score REAL,
                sentiment_label TEXT,
                relevance_score REAL,
                impact_score REAL,
                analysis_timestamp TIMESTAMP NOT NULL,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_symbol 
            ON news_sentiment(symbol)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_date 
            ON news_sentiment(article_date)
        ''')
        
        conn.commit()
        conn.close()
        self.tables_created.append('news_sentiment')
        logger.info("Created news_sentiment table")
    
    def verify_schema(self):
        """Verify all tables were created successfully"""
        conn = None
        try:
            conn = self.create_connection()
            cursor = conn.cursor()
            
            # Get list of tables
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
            """)
            
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            expected_tables = [
                'service_coordination',
                'scanning_results',
                'pattern_analysis',
                'technical_indicators',
                'ml_predictions',
                'strategy_evaluations',
                'risk_metrics',
                'orders',
                'news_sentiment'
            ]
            
            missing_tables = set(expected_tables) - set(existing_tables)
            
            if missing_tables:
                logger.error(f"Missing tables: {missing_tables}")
                return False
            
            logger.info(f"Schema verification successful. Found {len(existing_tables)} tables.")
            logger.info(f"Tables: {', '.join(sorted(existing_tables))}")
            
            # Verify service_coordination table structure
            cursor.execute("PRAGMA table_info(service_coordination)")
            columns = [row[1] for row in cursor.fetchall()]
            logger.info(f"service_coordination columns: {columns}")
            
            return True
            
        except Exception as e:
            logger.error(f"Schema verification failed: {e}")
            return False
        finally:
            if conn:
                conn.close()

def main():
    """Main function to run migration"""
    logger.info("Starting Trading System Database Migration v1.0.5")
    
    # Check if database already exists
    db_path = Path('trading_system.db')
    if db_path.exists():
        logger.warning(f"Database {db_path} already exists. Migration will update schema if needed.")
    
    # Run migration
    migration = DatabaseMigration()
    success = migration.execute_migration()
    
    if success:
        logger.info("Database migration completed successfully!")
        logger.info(f"Created/verified tables: {', '.join(migration.tables_created)}")
    else:
        logger.error("Database migration failed!")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())

================================================================================
FILE: ./database_utils.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM - DATABASE UTILITIES
Version: 1.0.0
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.0 (2025-06-19) - Initial implementation to fix database locking issues

Database Utilities - Provides centralized database connection management
with retry logic, WAL mode, and proper error handling to prevent locking issues
"""

import sqlite3
import time
import logging
import os
from contextlib import contextmanager
from typing import Optional, Any, List, Tuple
import threading
import random

# Global lock for database operations
_db_lock = threading.Lock()

class DatabaseManager:
    """Centralized database management with retry logic and WAL mode"""
    
    def __init__(self, db_path: str = './trading_system.db'):
        self.db_path = db_path
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Connection settings for better concurrency
        self.timeout = 30.0  # Increase timeout to 30 seconds
        self.max_retries = 5
        self.retry_delay_base = 0.1  # Base delay in seconds
        self.retry_delay_max = 2.0   # Maximum delay in seconds
        
        # Initialize database with WAL mode
        self._initialize_database()
    
    def _initialize_database(self):
        """Initialize database with WAL mode for better concurrency"""
        try:
            conn = sqlite3.connect(self.db_path, timeout=self.timeout)
            
            # Enable WAL mode for better concurrent access
            conn.execute("PRAGMA journal_mode=WAL")
            
            # Optimize for concurrent access
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            
            # Enable foreign keys
            conn.execute("PRAGMA foreign_keys=ON")
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Database initialized with WAL mode at {self.db_path}")
            
        except Exception as e:
            self.logger.error(f"Error initializing database: {e}")
            raise
    
    @contextmanager
    def get_connection(self):
        """Get a database connection with automatic retry and cleanup"""
        conn = None
        attempt = 0
        
        while attempt < self.max_retries:
            try:
                # Use the global lock for write operations
                with _db_lock:
                    conn = sqlite3.connect(self.db_path, timeout=self.timeout)
                    
                    # Set pragmas for each connection
                    conn.execute("PRAGMA foreign_keys=ON")
                    conn.execute("PRAGMA journal_mode=WAL")
                    
                    # Set row factory for dict-like access
                    conn.row_factory = sqlite3.Row
                    
                    yield conn
                    
                    # Successful completion
                    conn.commit()
                    return
                    
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e):
                    attempt += 1
                    if attempt < self.max_retries:
                        # Exponential backoff with jitter
                        delay = min(
                            self.retry_delay_base * (2 ** attempt) + random.uniform(0, 0.1),
                            self.retry_delay_max
                        )
                        self.logger.warning(
                            f"Database locked, retry {attempt}/{self.max_retries} "
                            f"after {delay:.2f}s"
                        )
                        time.sleep(delay)
                    else:
                        self.logger.error(f"Database locked after {self.max_retries} retries")
                        raise
                else:
                    raise
                    
            except Exception as e:
                self.logger.error(f"Database error: {e}")
                if conn:
                    conn.rollback()
                raise
                
            finally:
                if conn:
                    conn.close()
    
    def execute_with_retry(self, query: str, params: Optional[Tuple] = None) -> Optional[sqlite3.Cursor]:
        """Execute a query with automatic retry logic"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            if params:
                cursor.execute(query, params)
            else:
                cursor.execute(query)
            return cursor
    
    def executemany_with_retry(self, query: str, params_list: List[Tuple]) -> Optional[sqlite3.Cursor]:
        """Execute many queries with automatic retry logic"""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.executemany(query, params_list)
            return cursor
    
    def fetchone_with_retry(self, query: str, params: Optional[Tuple] = None) -> Optional[sqlite3.Row]:
        """Fetch one row with automatic retry logic"""
        cursor = self.execute_with_retry(query, params)
        return cursor.fetchone() if cursor else None
    
    def fetchall_with_retry(self, query: str, params: Optional[Tuple] = None) -> List[sqlite3.Row]:
        """Fetch all rows with automatic retry logic"""
        cursor = self.execute_with_retry(query, params)
        return cursor.fetchall() if cursor else []
    
    def insert_with_retry(self, table: str, data: dict) -> Optional[int]:
        """Insert data into table with retry logic"""
        columns = ', '.join(data.keys())
        placeholders = ', '.join(['?' for _ in data])
        query = f"INSERT INTO {table} ({columns}) VALUES ({placeholders})"
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(query, tuple(data.values()))
            return cursor.lastrowid
    
    def update_with_retry(self, table: str, data: dict, where_clause: str, where_params: Tuple) -> int:
        """Update data in table with retry logic"""
        set_clause = ', '.join([f"{k} = ?" for k in data.keys()])
        query = f"UPDATE {table} SET {set_clause} WHERE {where_clause}"
        params = tuple(data.values()) + where_params
        
        cursor = self.execute_with_retry(query, params)
        return cursor.rowcount if cursor else 0
    
    def table_exists(self, table_name: str) -> bool:
        """Check if a table exists"""
        query = """
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name=?
        """
        result = self.fetchone_with_retry(query, (table_name,))
        return result is not None
    
    def get_table_info(self, table_name: str) -> List[dict]:
        """Get information about table columns"""
        query = f"PRAGMA table_info({table_name})"
        rows = self.fetchall_with_retry(query)
        return [dict(row) for row in rows] if rows else []
    
    def vacuum_database(self):
        """Vacuum database to optimize performance"""
        try:
            # VACUUM cannot be run within a transaction
            conn = sqlite3.connect(self.db_path, timeout=self.timeout)
            conn.execute("VACUUM")
            conn.close()
            self.logger.info("Database vacuumed successfully")
        except Exception as e:
            self.logger.error(f"Error vacuuming database: {e}")
    
    def checkpoint_wal(self):
        """Checkpoint WAL to main database file"""
        try:
            with self.get_connection() as conn:
                conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                self.logger.info("WAL checkpoint completed")
        except Exception as e:
            self.logger.error(f"Error checkpointing WAL: {e}")


# Global database manager instance
_db_manager = None


def get_database_manager(db_path: str = './trading_system.db') -> DatabaseManager:
    """Get or create the global database manager instance"""
    global _db_manager
    
    if _db_manager is None:
        _db_manager = DatabaseManager(db_path)
    
    return _db_manager


# Convenience functions for backward compatibility
@contextmanager
def get_db_connection(db_path: str = './trading_system.db'):
    """Get a database connection with automatic retry and cleanup"""
    manager = get_database_manager(db_path)
    with manager.get_connection() as conn:
        yield conn


def execute_with_retry(query: str, params: Optional[Tuple] = None, 
                      db_path: str = './trading_system.db') -> Optional[sqlite3.Cursor]:
    """Execute a query with automatic retry logic"""
    manager = get_database_manager(db_path)
    return manager.execute_with_retry(query, params)


def save_with_retry(table: str, data: dict, 
                   db_path: str = './trading_system.db') -> Optional[int]:
    """Save data to table with automatic retry logic"""
    manager = get_database_manager(db_path)
    return manager.insert_with_retry(table, data)


# Example usage for services
class DatabaseServiceMixin:
    """Mixin class for services to use database utilities"""
    
    def __init__(self, db_path: str = './trading_system.db'):
        self.db_manager = get_database_manager(db_path)
        self.db_path = db_path
    
    def save_to_database(self, table: str, data: dict) -> bool:
        """Save data to database with retry logic"""
        try:
            row_id = self.db_manager.insert_with_retry(table, data)
            return row_id is not None
        except Exception as e:
            self.logger.error(f"Error saving to {table}: {e}")
            return False
    
    def update_database(self, table: str, data: dict, 
                       where_clause: str, where_params: Tuple) -> bool:
        """Update database with retry logic"""
        try:
            rows_affected = self.db_manager.update_with_retry(
                table, data, where_clause, where_params
            )
            return rows_affected > 0
        except Exception as e:
            self.logger.error(f"Error updating {table}: {e}")
            return False
    
    def query_database(self, query: str, params: Optional[Tuple] = None) -> List[dict]:
        """Query database with retry logic"""
        try:
            rows = self.db_manager.fetchall_with_retry(query, params)
            return [dict(row) for row in rows] if rows else []
        except Exception as e:
            self.logger.error(f"Error querying database: {e}")
            return []


if __name__ == "__main__":
    # Test the database utilities
    logging.basicConfig(level=logging.INFO)
    
    print("Testing Database Utilities...")
    
    # Initialize database manager
    db_manager = get_database_manager()
    
    # Test connection
    print("\n1. Testing connection with retry...")
    with db_manager.get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 5")
        tables = cursor.fetchall()
        print(f"   Found {len(tables)} tables")
        for table in tables:
            print(f"   - {table['name']}")
    
    # Test retry logic
    print("\n2. Testing retry logic...")
    try:
        result = db_manager.fetchone_with_retry(
            "SELECT COUNT(*) as count FROM selected_securities"
        )
        if result:
            print(f"   Selected securities count: {result['count']}")
    except Exception as e:
        print(f"   Error: {e}")
    
    print("\nDatabase utilities test completed!")


================================================================================
FILE: ./diagnose-paths.py
================================================================================

#!/usr/bin/env python3
"""
Diagnose and fix path issues in Trading Application
"""
import os
import re
from pathlib import Path

def find_path_issues():
    """Find all files with path issues"""
    print("üîç Scanning for path issues...\n")
    
    issues_found = []
    
    # Common problematic patterns
    patterns = [
        r'["\']\.?/?trading_system/',  # ./trading_system/ or /trading_system/
        r'["\']\.?/?content/',          # ../ or ./
        r'trading_system\.db',          # database references
    ]
    
    # Scan all Python files
    for py_file in Path('.').glob('*.py'):
        with open(py_file, 'r') as f:
            content = f.read()
            lines = content.split('\n')
        
        file_issues = []
        for i, line in enumerate(lines, 1):
            for pattern in patterns:
                if re.search(pattern, line):
                    file_issues.append({
                        'line_num': i,
                        'line': line.strip(),
                        'pattern': pattern
                    })
        
        if file_issues:
            issues_found.append({
                'file': str(py_file),
                'issues': file_issues
            })
    
    return issues_found

def show_issues(issues):
    """Display found issues"""
    if not issues:
        print("‚úÖ No path issues found!")
        return
    
    print(f"‚ùå Found path issues in {len(issues)} files:\n")
    
    for file_info in issues:
        print(f"\nüìÑ {file_info['file']}:")
        for issue in file_info['issues']:
            print(f"   Line {issue['line_num']}: {issue['line']}")

def suggest_fixes(issues):
    """Suggest fixes for found issues"""
    if not issues:
        return
    
    print("\n" + "="*60)
    print("üîß SUGGESTED FIXES:")
    print("="*60)
    
    print("\nThe issue is that your files are looking for a 'trading_system' subdirectory")
    print("that doesn't exist. Your files are in the current directory.\n")
    
    print("Run these commands to fix:\n")
    
    # Generate sed commands for each file
    fixed_files = set()
    for file_info in issues:
        filename = file_info['file']
        if filename not in fixed_files:
            print(f"# Fix {filename}")
            print(f"sed -i 's|trading_system/||g' {filename}")
            print(f"sed -i 's|./|./|g' {filename}")
            fixed_files.add(filename)
    
    print("\n# Or fix all at once:")
    print("for f in *.py; do")
    print("  sed -i 's|trading_system/||g' \"$f\"")
    print("  sed -i 's|./|./|g' \"$f\"")
    print("done")

def main():
    print("üè• Trading Application Path Diagnostic Tool\n")
    
    # Find issues
    issues = find_path_issues()
    
    # Show what was found
    show_issues(issues)
    
    # Suggest fixes
    suggest_fixes(issues)
    
    # Show current directory structure
    print("\n" + "="*60)
    print("üìÅ YOUR CURRENT DIRECTORY STRUCTURE:")
    print("="*60)
    print(f"Current directory: {os.getcwd()}")
    print("\nFiles in current directory:")
    for item in sorted(os.listdir('.')):
        if not item.startswith('.'):
            print(f"  - {item}")

if __name__ == "__main__":
    main()

================================================================================
FILE: ./diagnostic_toolkit.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC TOOLKIT - MAIN ORCHESTRATOR
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic toolkit orchestrator

USAGE: 
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check only
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --integration     # Service integration test only
  python diagnostic_toolkit.py --processes       # Process/port check only
  python diagnostic_toolkit.py --service NAME    # Single service focus
  python diagnostic_toolkit.py --report          # Generate detailed report

PURPOSE: Unified diagnostic toolkit for comprehensive Trading System health analysis
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Import diagnostic modules
try:
    from diagnostic_service_integration import ServiceIntegrationDiagnostic
    from diagnostic_log_analysis import LogAnalysisDiagnostic  
    from diagnostic_process_ports import ProcessPortDiagnostic
except ImportError as e:
    print(f"‚ùå Error importing diagnostic modules: {e}")
    print("   Ensure all diagnostic scripts are in the same directory")
    sys.exit(1)

class TradingSystemDiagnosticToolkit:
    """Comprehensive diagnostic toolkit orchestrator"""
    
    def __init__(self):
        self.timestamp = datetime.now()
        self.results = {}
        
        # Initialize diagnostic modules
        self.integration_diagnostic = ServiceIntegrationDiagnostic()
        self.log_diagnostic = LogAnalysisDiagnostic()
        self.process_diagnostic = ProcessPortDiagnostic()
        
        # Output directory
        self.output_dir = Path('./diagnostic_reports')
        self.output_dir.mkdir(exist_ok=True)
    
    def run_comprehensive_diagnostic(self, options: Dict) -> Dict:
        """Run complete comprehensive diagnostic"""
        print("üè• TRADING SYSTEM COMPREHENSIVE DIAGNOSTIC TOOLKIT")
        print("=" * 70)
        print(f"Started: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Mode: {'Quick' if options.get('quick') else 'Comprehensive'}")
        
        if options.get('service_filter'):
            print(f"Service Focus: {options['service_filter']}")
        
        comprehensive_results = {
            "diagnostic_metadata": {
                "timestamp": self.timestamp.isoformat(),
                "version": "1.0.0",
                "mode": "quick" if options.get('quick') else "comprehensive",
                "service_filter": options.get('service_filter'),
                "modules_run": []
            }
        }
        
        # 1. Process and Port Analysis (Always run first)
        if not options.get('logs_only') and not options.get('integration_only'):
            print(f"\n{'='*20} PHASE 1: PROCESS & PORT ANALYSIS {'='*20}")
            try:
                process_results = self.process_diagnostic.run_full_diagnostic()
                comprehensive_results["process_analysis"] = process_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("process_ports")
            except Exception as e:
                print(f"‚ùå Process analysis failed: {e}")
                comprehensive_results["process_analysis"] = {"error": str(e)}
        
        # 2. Service Integration Testing
        if not options.get('logs_only') and not options.get('processes_only'):
            print(f"\n{'='*20} PHASE 2: SERVICE INTEGRATION TESTING {'='*18}")
            try:
                integration_results = self.integration_diagnostic.run_full_diagnostic()
                comprehensive_results["integration_analysis"] = integration_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("service_integration")
            except Exception as e:
                print(f"‚ùå Integration analysis failed: {e}")
                comprehensive_results["integration_analysis"] = {"error": str(e)}
        
        # 3. Log Analysis (Skip in quick mode unless specifically requested)
        if not options.get('quick') or options.get('logs_only'):
            if not options.get('integration_only') and not options.get('processes_only'):
                print(f"\n{'='*20} PHASE 3: LOG ANALYSIS {'='*31}")
                try:
                    log_results = self.log_diagnostic.run_full_analysis(
                        service_filter=options.get('service_filter'),
                        errors_only=options.get('errors_only', False),
                        last_minutes=options.get('last_minutes')
                    )
                    comprehensive_results["log_analysis"] = log_results
                    comprehensive_results["diagnostic_metadata"]["modules_run"].append("log_analysis")
                except Exception as e:
                    print(f"‚ùå Log analysis failed: {e}")
                    comprehensive_results["log_analysis"] = {"error": str(e)}
        
        # 4. Comprehensive Analysis and Recommendations
        print(f"\n{'='*20} PHASE 4: COMPREHENSIVE ANALYSIS {'='*23}")
        comprehensive_analysis = self.generate_comprehensive_analysis(comprehensive_results)
        comprehensive_results["comprehensive_analysis"] = comprehensive_analysis
        
        # 5. Save Report (if requested)
        if options.get('save_report'):
            report_path = self.save_diagnostic_report(comprehensive_results)
            comprehensive_results["report_path"] = str(report_path)
        
        return comprehensive_results
    
    def generate_comprehensive_analysis(self, results: Dict) -> Dict:
        """Generate unified analysis across all diagnostic modules"""
        print("üß† COMPREHENSIVE SYSTEM ANALYSIS")
        print("-" * 40)
        
        analysis = {
            "overall_system_health": "unknown",
            "confidence_level": "unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": [],
            "system_scores": {},
            "health_indicators": {},
            "summary": {}
        }
        
        # Collect health indicators from each module
        process_health = self.extract_process_health(results.get("process_analysis", {}))
        integration_health = self.extract_integration_health(results.get("integration_analysis", {}))
        log_health = self.extract_log_health(results.get("log_analysis", {}))
        
        analysis["health_indicators"] = {
            "process_health": process_health,
            "integration_health": integration_health,
            "log_health": log_health
        }
        
        # Calculate weighted system scores
        scores = []
        weights = []
        
        if process_health.get("score") is not None:
            scores.append(process_health["score"])
            weights.append(0.4)  # Process health is most important
        
        if integration_health.get("score") is not None:
            scores.append(integration_health["score"])
            weights.append(0.4)  # Integration health is equally important
        
        if log_health.get("score") is not None:
            scores.append(log_health["score"])
            weights.append(0.2)  # Log health is supporting indicator
        
        # Calculate overall score
        if scores and weights:
            overall_score = sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)
            analysis["system_scores"]["overall"] = round(overall_score, 1)
        else:
            overall_score = 0
        
        # Determine overall health
        if overall_score >= 90:
            analysis["overall_system_health"] = "excellent"
            analysis["confidence_level"] = "high"
            print("üéâ OVERALL SYSTEM HEALTH: EXCELLENT")
        elif overall_score >= 75:
            analysis["overall_system_health"] = "good"
            analysis["confidence_level"] = "high"
            print("‚úÖ OVERALL SYSTEM HEALTH: GOOD")
        elif overall_score >= 60:
            analysis["overall_system_health"] = "fair"
            analysis["confidence_level"] = "medium"
            print("üü° OVERALL SYSTEM HEALTH: FAIR")
        elif overall_score >= 40:
            analysis["overall_system_health"] = "poor"
            analysis["confidence_level"] = "high"
            print("‚ö†Ô∏è OVERALL SYSTEM HEALTH: POOR")
        else:
            analysis["overall_system_health"] = "critical"
            analysis["confidence_level"] = "high"
            print("üö® OVERALL SYSTEM HEALTH: CRITICAL")
        
        print(f"System Score: {overall_score:.1f}/100")
        
        # Collect issues and recommendations from all modules
        self.collect_unified_issues_and_recommendations(results, analysis)
        
        # Generate summary
        analysis["summary"] = {
            "total_services": 9,
            "services_healthy": process_health.get("services_responding", 0),
            "workflow_functional": integration_health.get("workflow_works", False),
            "log_errors": log_health.get("total_errors", 0),
            "critical_issues_count": len(analysis["critical_issues"]),
            "recommendations_count": len(analysis["recommendations"])
        }
        
        # Display key findings
        print(f"\nKey Findings:")
        print(f"   Services Responding: {analysis['summary']['services_healthy']}/9")
        print(f"   Workflow Functional: {'Yes' if analysis['summary']['workflow_functional'] else 'No'}")
        print(f"   Log Errors: {analysis['summary']['log_errors']}")
        print(f"   Critical Issues: {analysis['summary']['critical_issues_count']}")
        
        return analysis
    
    def extract_process_health(self, process_results: Dict) -> Dict:
        """Extract health indicators from process analysis"""
        if not process_results or "error" in process_results:
            return {"score": None, "status": "unavailable"}
        
        services_total = process_results.get("services_total", 9)
        services_responding = process_results.get("services_responding", 0)
        
        # Calculate score based on service availability
        score = (services_responding / services_total) * 100 if services_total > 0 else 0
        
        return {
            "score": round(score, 1),
            "status": process_results.get("overall_status", "unknown"),
            "services_responding": services_responding,
            "services_total": services_total,
            "hybrid_manager_running": process_results.get("hybrid_manager", {}).get("status") == "running"
        }
    
    def extract_integration_health(self, integration_results: Dict) -> Dict:
        """Extract health indicators from integration analysis"""
        if not integration_results or "error" in integration_results:
            return {"score": None, "status": "unavailable"}
        
        summary = integration_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 80, "fair": 60, "poor": 30}
        
        score = health_score_map.get(summary.get("overall_health"), 0)
        workflow_result = integration_results.get("workflow", {})
        workflow_works = workflow_result.get("status") == "completed"
        
        return {
            "score": score,
            "status": summary.get("overall_health", "unknown"),
            "workflow_works": workflow_works,
            "services_functional": summary.get("services_functional", 0)
        }
    
    def extract_log_health(self, log_results: Dict) -> Dict:
        """Extract health indicators from log analysis"""
        if not log_results or "error" in log_results:
            return {"score": None, "status": "unavailable"}
        
        summary = log_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 85, "fair": 60, "poor": 20}
        
        score = health_score_map.get(summary.get("overall_log_health"), 50)
        
        return {
            "score": score,
            "status": summary.get("overall_log_health", "unknown"),
            "total_errors": summary.get("total_errors", 0),
            "total_warnings": summary.get("total_warnings", 0)
        }
    
    def collect_unified_issues_and_recommendations(self, results: Dict, analysis: Dict):
        """Collect and unify issues and recommendations from all modules"""
        # Process issues
        process_analysis = results.get("process_analysis", {})
        if process_analysis.get("issues"):
            analysis["critical_issues"].extend(process_analysis["issues"])
        if process_analysis.get("recommendations"):
            analysis["recommendations"].extend(process_analysis["recommendations"])
        
        # Integration issues
        integration_summary = results.get("integration_analysis", {}).get("summary", {})
        if integration_summary.get("critical_issues"):
            analysis["critical_issues"].extend(integration_summary["critical_issues"])
        if integration_summary.get("recommendations"):
            analysis["recommendations"].extend(integration_summary["recommendations"])
        
        # Log issues
        log_summary = results.get("log_analysis", {}).get("summary", {})
        if log_summary.get("critical_issues"):
            analysis["critical_issues"].extend(log_summary["critical_issues"])
        if log_summary.get("recommendations"):
            analysis["recommendations"].extend(log_summary["recommendations"])
        
        # Remove duplicates while preserving order
        analysis["critical_issues"] = list(dict.fromkeys(analysis["critical_issues"]))
        analysis["recommendations"] = list(dict.fromkeys(analysis["recommendations"]))
        
        # Add unified recommendations based on patterns
        if "json_serialization" in str(analysis["critical_issues"]):
            analysis["recommendations"].insert(0, "Apply JSON serialization fix to pattern_analysis.py")
        
        if "hybrid_manager_not_running" in analysis["critical_issues"]:
            analysis["recommendations"].insert(0, "Start system: python hybrid_manager.py start")
    
    def save_diagnostic_report(self, results: Dict) -> Path:
        """Save comprehensive diagnostic report to file"""
        timestamp_str = self.timestamp.strftime('%Y%m%d_%H%M%S')
        report_filename = f"trading_system_diagnostic_{timestamp_str}.json"
        report_path = self.output_dir / report_filename
        
        try:
            with open(report_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüíæ Diagnostic report saved: {report_path}")
            
            # Also create a summary text report
            summary_path = self.output_dir / f"diagnostic_summary_{timestamp_str}.txt"
            self.create_text_summary(results, summary_path)
            
            return report_path
            
        except Exception as e:
            print(f"‚ùå Error saving report: {e}")
            return None
    
    def create_text_summary(self, results: Dict, summary_path: Path):
        """Create human-readable text summary"""
        try:
            with open(summary_path, 'w') as f:
                f.write("TRADING SYSTEM DIAGNOSTIC SUMMARY\n")
                f.write("=" * 50 + "\n")
                f.write(f"Generated: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # Overall health
                comp_analysis = results.get("comprehensive_analysis", {})
                f.write(f"Overall Health: {comp_analysis.get('overall_system_health', 'unknown').upper()}\n")
                f.write(f"System Score: {comp_analysis.get('system_scores', {}).get('overall', 'N/A')}/100\n\n")
                
                # Key metrics
                summary = comp_analysis.get("summary", {})
                f.write("Key Metrics:\n")
                f.write(f"  Services Responding: {summary.get('services_healthy', 0)}/9\n")
                f.write(f"  Workflow Functional: {'Yes' if summary.get('workflow_functional') else 'No'}\n")
                f.write(f"  Log Errors: {summary.get('log_errors', 0)}\n")
                f.write(f"  Critical Issues: {summary.get('critical_issues_count', 0)}\n\n")
                
                # Critical issues
                critical_issues = comp_analysis.get("critical_issues", [])
                if critical_issues:
                    f.write("Critical Issues:\n")
                    for issue in critical_issues:
                        f.write(f"  - {issue}\n")
                    f.write("\n")
                
                # Top recommendations
                recommendations = comp_analysis.get("recommendations", [])
                if recommendations:
                    f.write("Top Recommendations:\n")
                    for i, rec in enumerate(recommendations[:5], 1):
                        f.write(f"  {i}. {rec}\n")
                
            print(f"üìÑ Text summary saved: {summary_path}")
            
        except Exception as e:
            print(f"‚ùå Error saving text summary: {e}")
    
    def run_quick_health_check(self) -> Dict:
        """Run quick health check (process + integration only)"""
        print("‚ö° QUICK HEALTH CHECK")
        print("-" * 30)
        
        return self.run_comprehensive_diagnostic({"quick": True})

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Trading System Diagnostic Toolkit',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --service scanner # Focus on scanner service
  python diagnostic_toolkit.py --report          # Save detailed report
        """
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Quick health check (skip log analysis)')
    parser.add_argument('--logs-only', action='store_true',
                       help='Run log analysis only')
    parser.add_argument('--integration-only', action='store_true',
                       help='Run integration tests only')
    parser.add_argument('--processes-only', action='store_true',
                       help='Run process/port checks only')
    parser.add_argument('--service', 
                       help='Focus on specific service')
    parser.add_argument('--errors-only', action='store_true',
                       help='Show only errors in log analysis')
    parser.add_argument('--last-minutes', type=int,
                       help='Analyze only last N minutes of logs')
    parser.add_argument('--report', action='store_true',
                       help='Save detailed diagnostic report')
    
    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_arguments()
    
    # Convert args to options dict
    options = {
        'quick': args.quick,
        'logs_only': args.logs_only,
        'integration_only': args.integration_only,
        'processes_only': args.processes_only,
        'service_filter': args.service,
        'errors_only': args.errors_only,
        'last_minutes': args.last_minutes,
        'save_report': args.report
    }
    
    # Initialize toolkit
    toolkit = TradingSystemDiagnosticToolkit()
    
    try:
        # Run diagnostic
        if args.quick:
            results = toolkit.run_quick_health_check()
        else:
            results = toolkit.run_comprehensive_diagnostic(options)
        
        # Print completion message
        print(f"\nüèÅ DIAGNOSTIC COMPLETED")
        print("=" * 30)
        
        comp_analysis = results.get("comprehensive_analysis", {})
        overall_health = comp_analysis.get("overall_system_health", "unknown")
        
        if overall_health in ["excellent", "good"]:
            print("‚úÖ Your trading system is healthy and ready for operation!")
        elif overall_health == "fair":
            print("üü° Your trading system has minor issues but is operational.")
        else:
            print("‚ùå Your trading system has significant issues requiring attention.")
        
        # Show next steps
        recommendations = comp_analysis.get("recommendations", [])
        if recommendations:
            print(f"\nüéØ Next Steps:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        return results
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Diagnostic interrupted by user")
        return None
    except Exception as e:
        print(f"\n‚ùå Diagnostic failed: {e}")
        return None

if __name__ == "__main__":
    main()

================================================================================
FILE: ./fix_coord.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: fix_coordination_service.py
Version: 1.0.0
Last Updated: 2025-06-22

PURPOSE:
Script to properly stop old coordination service and start the fixed version
"""

import os
import sys
import time
import subprocess
import psutil
from pathlib import Path

def kill_coordination_service():
    """Kill all running instances of coordination service"""
    print("üõë Stopping all coordination service instances...")
    
    killed_count = 0
    
    # Method 1: Using psutil to find and kill processes
    try:
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = proc.info.get('cmdline', [])
                if cmdline and any('coordination_service' in str(arg) for arg in cmdline):
                    print(f"  Found process: PID {proc.info['pid']}")
                    proc.terminate()
                    killed_count += 1
                    time.sleep(0.5)
                    if proc.is_running():
                        proc.kill()  # Force kill if still running
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
    except ImportError:
        print("  psutil not available, trying alternative method...")
    
    # Method 2: Using pkill command
    try:
        subprocess.run(['pkill', '-f', 'coordination_service'], capture_output=True)
        print("  Executed pkill command")
    except:
        pass
    
    # Method 3: Find processes on port 5000
    try:
        result = subprocess.run(['lsof', '-ti:5000'], capture_output=True, text=True)
        if result.stdout:
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                if pid:
                    try:
                        subprocess.run(['kill', '-9', pid], capture_output=True)
                        print(f"  Killed process on port 5000: PID {pid}")
                        killed_count += 1
                    except:
                        pass
    except:
        pass
    
    if killed_count > 0:
        print(f"‚úÖ Stopped {killed_count} coordination service instance(s)")
    else:
        print("‚ÑπÔ∏è  No running coordination service instances found")
    
    # Wait for port to be released
    time.sleep(2)
    
def check_port_availability(port=5000):
    """Check if port is available"""
    import socket
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    result = sock.connect_ex(('localhost', port))
    sock.close()
    
    if result == 0:
        print(f"‚ùå Port {port} is still in use")
        return False
    else:
        print(f"‚úÖ Port {port} is available")
        return True

def backup_old_service():
    """Backup old coordination service"""
    old_file = Path('coordination_service.py')
    if old_file.exists():
        backup_name = f'coordination_service_backup_{time.strftime("%Y%m%d_%H%M%S")}.py'
        old_file.rename(backup_name)
        print(f"üì¶ Backed up old service to: {backup_name}")

def apply_fixed_version():
    """Copy the fixed version to coordination_service.py"""
    fixed_file = Path('coordination_service_v105.py')
    target_file = Path('coordination_service.py')
    
    if not fixed_file.exists():
        print("‚ùå Fixed version (coordination_service_v105.py) not found!")
        print("   Please ensure you've created the fixed version first.")
        return False
    
    # Copy the fixed version
    target_file.write_text(fixed_file.read_text())
    print("‚úÖ Applied fixed version of coordination service")
    return True

def verify_fix():
    """Verify the fix is applied by checking the file content"""
    service_file = Path('coordination_service.py')
    
    if not service_file.exists():
        print("‚ùå coordination_service.py not found!")
        return False
    
    content = service_file.read_text()
    
    # Check for the old problematic code
    if 'service_url, service_port' in content:
        print("‚ùå Old version still in place - fix not applied!")
        return False
    
    # Check for the fixed code
    if 'host, port, status' in content:
        print("‚úÖ Fixed version verified - correct schema references found")
        return True
    
    print("‚ö†Ô∏è  Cannot verify fix - please check manually")
    return False

def start_fixed_service():
    """Start the fixed coordination service"""
    print("\nüöÄ Starting fixed coordination service...")
    
    # Start in background using subprocess
    try:
        process = subprocess.Popen(
            [sys.executable, 'coordination_service.py'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Wait a moment to see if it starts successfully
        time.sleep(3)
        
        if process.poll() is None:
            print("‚úÖ Coordination service started successfully")
            print(f"   Process PID: {process.pid}")
            
            # Check if it's responding
            time.sleep(2)
            try:
                import requests
                response = requests.get('http://localhost:5000/health', timeout=5)
                if response.status_code == 200:
                    print("‚úÖ Service is responding to health checks")
                else:
                    print("‚ö†Ô∏è  Service started but not responding properly")
            except:
                print("‚ö†Ô∏è  Cannot verify service health - check logs")
                
            return True
        else:
            print("‚ùå Service failed to start")
            stdout, stderr = process.communicate()
            if stderr:
                print(f"   Error: {stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Failed to start service: {e}")
        return False

def check_logs():
    """Display recent log entries"""
    log_file = Path('/content/logs/coordination_service.log')
    
    if log_file.exists():
        print("\nüìã Recent log entries:")
        try:
            # Get last 10 lines of log
            with open(log_file, 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-10:] if len(lines) > 10 else lines
                for line in recent_lines:
                    print(f"   {line.strip()}")
        except:
            print("   Could not read log file")
    else:
        print("\nüìã No log file found yet")

def main():
    """Main execution flow"""
    print("=" * 60)
    print("COORDINATION SERVICE FIX SCRIPT")
    print("=" * 60)
    
    # Step 1: Kill existing services
    kill_coordination_service()
    
    # Step 2: Check port availability
    if not check_port_availability():
        print("\n‚ö†Ô∏è  Port 5000 is still in use. Waiting 5 seconds...")
        time.sleep(5)
        kill_coordination_service()  # Try again
        
    # Step 3: Backup old service
    backup_old_service()
    
    # Step 4: Apply fixed version
    if not apply_fixed_version():
        print("\n‚ùå Failed to apply fix. Exiting.")
        sys.exit(1)
    
    # Step 5: Verify fix
    if not verify_fix():
        print("\n‚ö†Ô∏è  Warning: Could not verify fix was applied correctly")
        response = input("Continue anyway? (y/n): ")
        if response.lower() != 'y':
            sys.exit(1)
    
    # Step 6: Start fixed service
    if start_fixed_service():
        print("\n‚úÖ SUCCESS: Fixed coordination service is running!")
        
        # Step 7: Check logs
        check_logs()
        
        print("\nüìå Next steps:")
        print("   1. Monitor logs: tail -f /content/logs/coordination_service.log")
        print("   2. Check health: curl http://localhost:5000/health")
        print("   3. Verify services: curl http://localhost:5000/services")
    else:
        print("\n‚ùå FAILED: Could not start fixed service")
        print("   Check the logs for more information")
        check_logs()

if __name__ == "__main__":
    # Install psutil if not available
    try:
        import psutil
    except ImportError:
        print("Installing psutil for better process management...")
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'psutil', '-q'], capture_output=True)
        
    main()

================================================================================
FILE: ./google_drive_service.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: google_drive_service.py
Version: 1.0.1
Last Updated: 2025-01-11

REVISION HISTORY:
- v1.0.1 (2025-01-11) - Refactored to remove fallback, conform to project methodology
- v1.0.0 (2024-12-28) - Initial version with fallback support

PURPOSE:
Provides unified Google Drive access for Trading System services.
Used by TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py and enhanced_startup_manager.py.
"""

import json
import os
import logging
from pathlib import Path
from typing import Dict, Optional, Any, List
from datetime import datetime
from google.colab import userdata
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2.service_account import Credentials
import io
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaIoBaseUpload

class GoogleDriveService:
    """
    Unified Google Drive service for Trading System.
    Provides API access to Google Drive without fallback mechanisms.
    """
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """Initialize Google Drive service"""
        self.service = None
        self.project_folder_id = None
        self.logger = logger or self._setup_default_logger()
        
        # Project configuration
        self.project_name = "TradingSystem_Phase1"
        self.subfolders = [
            'data', 'models', 'logs', 'config', 
            'backups', 'reports', 'coordination', 
            'project_documentation', 'updates'
        ]
        
        # Initialize service
        self._authenticate()
        self._setup_project_structure()
    
    def _setup_default_logger(self) -> logging.Logger:
        """Setup default logger if none provided"""
        logger = logging.getLogger('GoogleDriveService')
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger
    
    def _authenticate(self):
        """Authenticate using service account credentials from Colab secrets"""
        try:
            # Build service account info from Colab secrets
            service_account_info = {
                "type": "service_account",
                "project_id": userdata.get('GOOGLE_PROJECT_ID'),
                "private_key_id": userdata.get('GOOGLE_PRIVATE_KEY_ID'),
                "private_key": userdata.get('GOOGLE_PRIVATE_KEY').replace('\\n', '\n'),
                "client_email": userdata.get('GOOGLE_CLIENT_EMAIL'),
                "client_id": userdata.get('GOOGLE_CLIENT_ID'),
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
                "client_x509_cert_url": f"https://www.googleapis.com/robot/v1/metadata/x509/{userdata.get('GOOGLE_CLIENT_EMAIL')}"
            }
            
            # Validate required fields
            required_fields = ['project_id', 'private_key', 'client_email', 'client_id']
            for field in required_fields:
                if not service_account_info.get(field):
                    raise ValueError(f"Missing required field: GOOGLE_{field.upper()}")
            
            # Create credentials
            credentials = Credentials.from_service_account_info(
                service_account_info,
                scopes=['https://www.googleapis.com/auth/drive']
            )
            
            # Build the service
            self.service = build('drive', 'v3', credentials=credentials)
            
            # Test authentication
            self.service.about().get(fields="user").execute()
            
            self.logger.info("‚úÖ Google Drive API authenticated successfully")
            
        except Exception as e:
            self.logger.error(f"‚ùå Google Drive authentication failed: {e}")
            raise RuntimeError(f"Failed to authenticate with Google Drive: {e}")
    
    def _setup_project_structure(self):
        """Setup project folder structure in Google Drive"""
        try:
            # Find or create main project folder
            self.project_folder_id = self._find_or_create_folder(
                self.project_name, 
                parent_id=None
            )
            self.logger.info(f"‚úÖ Project folder ready: {self.project_name}")
            
            # Create subfolders
            for subfolder in self.subfolders:
                self._find_or_create_folder(subfolder, self.project_folder_id)
                
        except Exception as e:
            self.logger.error(f"‚ùå Error setting up project structure: {e}")
            raise
    
    def _find_or_create_folder(self, folder_name: str, parent_id: Optional[str] = None) -> str:
        """Find existing folder or create new one"""
        try:
            # Build query
            query = f"name='{folder_name}' and mimeType='application/vnd.google-apps.folder'"
            if parent_id:
                query += f" and parents in '{parent_id}'"
            query += " and trashed=false"
            
            # Search for existing folder
            results = self.service.files().list(
                q=query,
                fields="files(id, name)",
                pageSize=1
            ).execute()
            
            if results.get('files'):
                folder_id = results['files'][0]['id']
                self.logger.debug(f"   üìÅ Found existing folder: {folder_name}")
                return folder_id
            
            # Create new folder
            folder_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder'
            }
            if parent_id:
                folder_metadata['parents'] = [parent_id]
                
            folder = self.service.files().create(
                body=folder_metadata,
                fields='id'
            ).execute()
            
            self.logger.info(f"   üìÅ Created folder: {folder_name}")
            return folder['id']
            
        except Exception as e:
            self.logger.error(f"Error with folder {folder_name}: {e}")
            raise
    
    def get_subfolder_id(self, subfolder: str) -> str:
        """Get the ID of a subfolder"""
        if not self.project_folder_id:
            raise RuntimeError("Project folder not initialized")
            
        return self._find_or_create_folder(subfolder, self.project_folder_id)
    
    def list_files(self, folder_name: Optional[str] = None, mime_type: Optional[str] = None) -> List[Dict]:
        """List files in a folder"""
        try:
            # Determine parent folder
            if folder_name:
                parent_id = self.get_subfolder_id(folder_name)
            else:
                parent_id = self.project_folder_id
            
            # Build query
            query = f"parents in '{parent_id}' and trashed=false"
            if mime_type:
                query += f" and mimeType='{mime_type}'"
            
            # Get files
            results = self.service.files().list(
                q=query,
                fields="files(id, name, mimeType, modifiedTime, size)",
                orderBy="modifiedTime desc"
            ).execute()
            
            return results.get('files', [])
            
        except Exception as e:
            self.logger.error(f"Error listing files: {e}")
            return []
    
    def read_file(self, filename: str, subfolder: Optional[str] = None) -> bytes:
        """Read file content from Google Drive"""
        try:
            # Find the file
            file_id = self._find_file(filename, subfolder)
            if not file_id:
                raise FileNotFoundError(f"File not found: {filename}")
            
            # Download file
            request = self.service.files().get_media(fileId=file_id)
            file_content = io.BytesIO()
            downloader = MediaIoBaseDownload(file_content, request)
            
            done = False
            while not done:
                status, done = downloader.next_chunk()
                
            file_content.seek(0)
            return file_content.read()
            
        except Exception as e:
            self.logger.error(f"Error reading file {filename}: {e}")
            raise
    
    def write_file(self, filename: str, content: bytes, subfolder: Optional[str] = None, 
                   mime_type: str = 'application/octet-stream') -> str:
        """Write file to Google Drive"""
        try:
            # Get parent folder
            if subfolder:
                parent_id = self.get_subfolder_id(subfolder)
            else:
                parent_id = self.project_folder_id
            
            # Check if file exists
            existing_file_id = self._find_file(filename, subfolder)
            
            # Prepare content
            media = MediaIoBaseUpload(
                io.BytesIO(content),
                mimetype=mime_type,
                resumable=True
            )
            
            if existing_file_id:
                # Update existing file
                file_metadata = {'name': filename}
                updated_file = self.service.files().update(
                    fileId=existing_file_id,
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                self.logger.info(f"Updated file: {filename}")
                return updated_file['id']
            else:
                # Create new file
                file_metadata = {
                    'name': filename,
                    'parents': [parent_id]
                }
                created_file = self.service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id'
                ).execute()
                self.logger.info(f"Created file: {filename}")
                return created_file['id']
                
        except Exception as e:
            self.logger.error(f"Error writing file {filename}: {e}")
            raise
    
    def _find_file(self, filename: str, subfolder: Optional[str] = None) -> Optional[str]:
        """Find file ID by name"""
        try:
            # Get parent folder
            if subfolder:
                parent_id = self.get_subfolder_id(subfolder)
            else:
                parent_id = self.project_folder_id
            
            # Search for file
            query = f"name='{filename}' and parents in '{parent_id}' and trashed=false"
            results = self.service.files().list(
                q=query,
                fields="files(id)",
                pageSize=1
            ).execute()
            
            files = results.get('files', [])
            return files[0]['id'] if files else None
            
        except Exception as e:
            self.logger.error(f"Error finding file {filename}: {e}")
            return None
    
    def delete_file(self, filename: str, subfolder: Optional[str] = None) -> bool:
        """Delete a file from Google Drive"""
        try:
            file_id = self._find_file(filename, subfolder)
            if not file_id:
                self.logger.warning(f"File not found for deletion: {filename}")
                return False
            
            self.service.files().delete(fileId=file_id).execute()
            self.logger.info(f"Deleted file: {filename}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error deleting file {filename}: {e}")
            return False
    
    def create_backup(self, source_folder: str, backup_name: str) -> str:
        """Create a backup of a folder"""
        try:
            # Create backup folder
            backup_folder_id = self._find_or_create_folder(
                backup_name, 
                self.get_subfolder_id('backups')
            )
            
            # Get source folder contents
            source_files = self.list_files(source_folder)
            
            # Copy files to backup
            for file_info in source_files:
                if file_info['mimeType'] != 'application/vnd.google-apps.folder':
                    # Copy file
                    copy_metadata = {
                        'name': file_info['name'],
                        'parents': [backup_folder_id]
                    }
                    self.service.files().copy(
                        fileId=file_info['id'],
                        body=copy_metadata
                    ).execute()
                    
            self.logger.info(f"Created backup: {backup_name}")
            return backup_folder_id
            
        except Exception as e:
            self.logger.error(f"Error creating backup: {e}")
            raise
    
    # Convenience methods for JSON operations
    def save_json(self, filename: str, data: Dict, subfolder: Optional[str] = None) -> bool:
        """Save JSON data to Google Drive"""
        try:
            json_content = json.dumps(data, indent=2, default=str)
            self.write_file(
                filename, 
                json_content.encode('utf-8'), 
                subfolder,
                mime_type='application/json'
            )
            return True
        except Exception as e:
            self.logger.error(f"Error saving JSON {filename}: {e}")
            return False
    
    def load_json(self, filename: str, subfolder: Optional[str] = None) -> Optional[Dict]:
        """Load JSON data from Google Drive"""
        try:
            content = self.read_file(filename, subfolder)
            return json.loads(content.decode('utf-8'))
        except FileNotFoundError:
            return None
        except Exception as e:
            self.logger.error(f"Error loading JSON {filename}: {e}")
            return None
    
    def get_database_info(self) -> Dict[str, Any]:
        """Get information about the database file"""
        try:
            db_info = {
                'exists': False,
                'size': 0,
                'last_modified': None,
                'file_id': None
            }
            
            file_id = self._find_file('trading.db', 'data')
            if file_id:
                # Get file metadata
                file_meta = self.service.files().get(
                    fileId=file_id,
                    fields='size,modifiedTime'
                ).execute()
                
                db_info.update({
                    'exists': True,
                    'size': int(file_meta.get('size', 0)),
                    'last_modified': file_meta.get('modifiedTime'),
                    'file_id': file_id
                })
                
            return db_info
            
        except Exception as e:
            self.logger.error(f"Error getting database info: {e}")
            return {'exists': False, 'error': str(e)}


# Global instance management
_drive_service_instance = None

def get_drive_service(logger: Optional[logging.Logger] = None) -> GoogleDriveService:
    """Get or create global drive service instance"""
    global _drive_service_instance
    if _drive_service_instance is None:
        _drive_service_instance = GoogleDriveService(logger)
    return _drive_service_instance

def init_drive_service(logger: Optional[logging.Logger] = None) -> GoogleDriveService:
    """Initialize drive service - call this in each notebook/script"""
    return get_drive_service(logger)

# Legacy compatibility functions
def get_project_path(subfolder: Optional[str] = None) -> str:
    """Legacy: Get virtual project path for compatibility"""
    base = f"/drive_api/{get_drive_service().project_name}"
    return f"{base}/{subfolder}" if subfolder else base

def get_database_path() -> str:
    """Legacy: Get virtual database path for compatibility"""
    return f"/drive_api/{get_drive_service().project_name}/data/trading.db"

# Direct export for convenience
save_json = lambda filename, data, subfolder=None: get_drive_service().save_json(filename, data, subfolder)
load_json = lambda filename, subfolder=None: get_drive_service().load_json(filename, subfolder)

if __name__ == "__main__":
    # Test the service
    print("üöÄ Google Drive Service v1.0.1")
    print("================================")
    
    try:
        service = init_drive_service()
        print("‚úÖ Service initialized successfully")
        
        # Test listing files
        print("\nüìÅ Project structure:")
        for folder in service.subfolders:
            files = service.list_files(folder)
            print(f"   {folder}/: {len(files)} files")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


================================================================================
FILE: ./hybrid_manager.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1 - ENHANCED VERSION (CLI ONLY)
Service: Hybrid Service Manager (CLI Service Control)
Version: 3.0.2-CLI
Last Updated: 2025-06-20

REVISION HISTORY:
- v3.0.2-CLI (2025-06-20) - Removed GUI for headless operation in Google Colab
- v3.0.2 (2025-06-20) - Added database table verification, startup delays, health check verification
- v3.0.1 (2025-06-19) - Enhanced with architecture implementation requirements
- v3.0.0 (2025-06-18) - Major refactor with GUI, enhanced monitoring and orchestration

This CLI version provides:
1. Service lifecycle management
2. Real-time health monitoring
3. System metrics and performance tracking
4. Database table verification before startup
5. Command-line interface for Google Colab
"""

import subprocess
import psutil
import requests
import json
import time
import threading
import os
import sys
import sqlite3
from datetime import datetime
from pathlib import Path
import logging
import signal

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('hybrid_manager')

class ServiceManager:
    def __init__(self):
        # Updated service list to match v3.0.2
        self.services = [
            {"name": "coordination_service", "port": 5000, "process": None, "status": "Stopped", "critical": True, "startup_delay": 5},
            {"name": "security_scanner", "port": 5001, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "pattern_analysis", "port": 5002, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "technical_analysis", "port": 5003, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "paper_trading", "port": 5005, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "pattern_recognition_service", "port": 5006, "process": None, "status": "Stopped", "critical": True, "startup_delay": 3},
            {"name": "news_service", "port": 5008, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3},
            {"name": "reporting_service", "port": 5009, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3},
            {"name": "web_dashboard", "port": 8080, "process": None, "status": "Stopped", "critical": False, "startup_delay": 3}
        ]
        self.monitoring_active = False
        self.db_path = "./trading_system.db"
        self.running = True
        
        # Set up signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals"""
        print("\nüõë Shutdown signal received. Stopping services...")
        self.running = False
        self.stop_all_services()
        sys.exit(0)
        
    def verify_database_tables(self):
        """Verify all required database tables exist"""
        try:
            if not Path(self.db_path).exists():
                logger.error(f"Database {self.db_path} not found!")
                return False
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # List of required tables from v3.0.2
            required_tables = [
                'service_registry',
                'trading_schedule',
                'news_articles',
                'technical_indicators',
                'patterns',
                'signals',
                'trades',
                'risk_metrics',
                'ml_predictions',
                'system_events'
            ]
            
            # Check each table
            missing_tables = []
            for table in required_tables:
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,))
                if not cursor.fetchone():
                    missing_tables.append(table)
            
            conn.close()
            
            if missing_tables:
                logger.warning(f"Missing database tables: {missing_tables}")
                logger.info("Running database migration...")
                return self.run_database_migration()
            
            logger.info("All required database tables verified ‚úì")
            return True
            
        except Exception as e:
            logger.error(f"Database verification failed: {str(e)}")
            return False
    
    def run_database_migration(self):
        """Run database migration if needed"""
        try:
            migration_path = Path("./database_migration.py")
            if migration_path.exists():
                result = subprocess.run([sys.executable, str(migration_path)], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    logger.info("Database migration completed successfully")
                    return True
                else:
                    logger.error(f"Database migration failed: {result.stderr}")
                    return False
            else:
                logger.error("database_migration.py not found")
                return False
        except Exception as e:
            logger.error(f"Migration error: {str(e)}")
            return False
    
    def start_service(self, service):
        """Start an individual service with health check verification"""
        try:
            if service["process"] and service["process"].poll() is None:
                logger.info(f"{service['name']} is already running")
                return True
                
            logger.info(f"Starting {service['name']}...")
            
            # Build command based on service name
            if service["name"] == "web_dashboard":
                cmd = [sys.executable, f"./web_dashboard_service.py"]
            else:
                cmd = [sys.executable, f"./{service['name']}.py"]
                
            # Start the process
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
            
            service["process"] = process
            service["status"] = "Starting"
            service["start_time"] = datetime.now()
            
            # Wait for startup delay
            logger.info(f"Waiting {service['startup_delay']}s for {service['name']} to initialize...")
            time.sleep(service['startup_delay'])
            
            # Verify service is healthy
            if self.check_service_health(service):
                service["status"] = "Running"
                logger.info(f"‚úì {service['name']} started successfully (PID: {process.pid})")
                return True
            else:
                service["status"] = "Failed"
                if process.poll() is None:
                    process.terminate()
                    process.wait()
                logger.error(f"‚úó {service['name']} failed health check")
                return False
                
        except Exception as e:
            logger.error(f"Failed to start {service['name']}: {str(e)}")
            service["status"] = "Error"
            return False
    
    def check_service_health(self, service, max_retries=10):
        """Check if a service is healthy via HTTP endpoint"""
        url = f"http://localhost:{service['port']}/health"
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    return True
            except:
                pass
            
            if attempt < max_retries - 1:
                time.sleep(1)
        
        return False
    
    def stop_service(self, service):
        """Stop an individual service"""
        try:
            if service["process"] and service["process"].poll() is None:
                logger.info(f"Stopping {service['name']}...")
                service["process"].terminate()
                service["process"].wait(timeout=5)
                service["status"] = "Stopped"
                service["process"] = None
                logger.info(f"‚úì {service['name']} stopped")
            else:
                service["status"] = "Stopped"
                service["process"] = None
        except subprocess.TimeoutExpired:
            logger.warning(f"Force killing {service['name']}")
            service["process"].kill()
            service["process"].wait()
            service["status"] = "Stopped"
            service["process"] = None
        except Exception as e:
            logger.error(f"Error stopping {service['name']}: {str(e)}")
    
    def start_all_services(self):
        """Start all services in the correct order"""
        print("\nüöÄ STARTING TRADING SYSTEM SERVICES")
        print("=" * 60)
        
        # Verify database first
        if not self.verify_database_tables():
            print("‚ùå Database verification failed. Cannot start services.")
            return False
        
        # Start coordination service first (it's the master)
        coord_service = next(s for s in self.services if s["name"] == "coordination_service")
        if not self.start_service(coord_service):
            print("‚ùå Failed to start coordination service. Aborting.")
            return False
        
        # Start other services
        for service in self.services:
            if service["name"] != "coordination_service":
                self.start_service(service)
        
        # Print status
        self.print_status()
        
        # Start monitoring
        if self.monitoring_active:
            self.start_monitoring()
        
        return True
    
    def stop_all_services(self):
        """Stop all services"""
        print("\nüõë STOPPING ALL SERVICES")
        print("=" * 60)
        
        for service in reversed(self.services):  # Stop in reverse order
            self.stop_service(service)
        
        print("‚úì All services stopped")
    
    def print_status(self):
        """Print current service status"""
        print("\nüìä SERVICE STATUS")
        print("=" * 60)
        print(f"{'Service':<25} {'Status':<12} {'Port':<6} {'PID':<8} {'Uptime'}")
        print("-" * 60)
        
        for service in self.services:
            if service["process"] and service["process"].poll() is None:
                pid = service["process"].pid
                uptime = str(datetime.now() - service.get("start_time", datetime.now())).split('.')[0]
            else:
                pid = "-"
                uptime = "-"
            
            status_icon = {
                "Running": "‚úì",
                "Stopped": "‚úó",
                "Starting": "‚è≥",
                "Failed": "‚ùå",
                "Error": "‚ö†Ô∏è"
            }.get(service["status"], "?")
            
            print(f"{service['name']:<25} {status_icon} {service['status']:<10} {service['port']:<6} {pid:<8} {uptime}")
        
        print("=" * 60)
        
        # System metrics
        print("\nüìà SYSTEM METRICS")
        print("=" * 60)
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            print(f"CPU Usage: {cpu_percent}%")
            print(f"Memory: {memory.percent}% ({memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB)")
            print(f"Disk: {disk.percent}% ({disk.used / (1024**3):.1f}GB / {disk.total / (1024**3):.1f}GB)")
        except Exception as e:
            print(f"Error getting system metrics: {e}")
        print("=" * 60)
    
    def monitor_services(self):
        """Monitor services and restart if needed"""
        monitor_count = 0
        while self.running and self.monitoring_active:
            monitor_count += 1
            
            for service in self.services:
                if service["process"] and service["process"].poll() is not None:
                    # Service crashed
                    logger.warning(f"{service['name']} crashed! Restarting...")
                    service["status"] = "Crashed"
                    self.start_service(service)
            
            # Print status every 10 checks (5 minutes)
            if monitor_count % 10 == 0:
                print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Monitor check #{monitor_count}")
                self.print_status()
            
            time.sleep(30)  # Check every 30 seconds
    
    def start_monitoring(self):
        """Start the monitoring thread"""
        self.monitoring_active = True
        monitor_thread = threading.Thread(target=self.monitor_services, daemon=True)
        monitor_thread.start()
        logger.info("Service monitoring started")


def main():
    """Main entry point"""
    manager = ServiceManager()
    
    # Parse command line arguments
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "start":
            manager.monitoring_active = True
            if manager.start_all_services():
                print("\n‚úÖ Trading System Started Successfully!")
                print("Press Ctrl+C to stop the system\n")
                
                # Keep running
                try:
                    while manager.running:
                        time.sleep(1)
                except KeyboardInterrupt:
                    pass
            else:
                print("\n‚ùå Failed to start trading system")
                sys.exit(1)
                
        elif command == "stop":
            manager.stop_all_services()
            
        elif command == "status":
            manager.print_status()
            
        elif command == "restart":
            manager.stop_all_services()
            time.sleep(2)
            manager.monitoring_active = True
            manager.start_all_services()
            
        elif command == "help":
            print("Trading System Hybrid Manager v3.0.2-CLI")
            print("\nUsage: python hybrid_manager.py [command]")
            print("\nCommands:")
            print("  start    - Start all services with monitoring")
            print("  stop     - Stop all services")
            print("  status   - Show current service status")
            print("  restart  - Restart all services")
            print("  help     - Show this help message")
        else:
            print(f"Unknown command: {command}")
            print("Use 'python hybrid_manager.py help' for usage")
    else:
        # Default: start with monitoring
        print("Starting Trading System (use 'python hybrid_manager.py help' for options)")
        manager.monitoring_active = True
        if manager.start_all_services():
            print("\n‚úÖ Trading System Started Successfully!")
            print("Press Ctrl+C to stop the system\n")
            
            try:
                while manager.running:
                    time.sleep(1)
            except KeyboardInterrupt:
                pass
        else:
            print("\n‚ùå Failed to start trading system")
            sys.exit(1)


if __name__ == "__main__":
    main()


================================================================================
FILE: ./install-requirements.py
================================================================================

#!/usr/bin/env python3
"""
Robust requirement installer for Trading Application
Handles common pip issues in Codespaces
"""

import subprocess
import sys

def run_command(cmd):
    """Run a command and return success status"""
    print(f"Running: {cmd}")
    result = subprocess.run(cmd, shell=True)
    return result.returncode == 0

def main():
    print("üîß Installing Trading Application Requirements\n")
    
    # Step 1: Upgrade pip and core tools
    print("Step 1: Upgrading pip and setuptools...")
    if not run_command(f"{sys.executable} -m pip install --upgrade pip setuptools wheel"):
        print("‚ùå Failed to upgrade pip/setuptools")
        return 1
    
    # Step 2: Install requirements one by one to identify issues
    requirements = [
        "flask==3.0.0",
        "requests==2.31.0",
        "pandas==2.1.4",
        "numpy==1.26.2",
        "scikit-learn==1.3.2",
        "yfinance==0.2.33",
        "psutil==5.9.6",
        "python-dateutil==2.8.2",
        "pytz==2023.3",
        "beautifulsoup4==4.12.2",
        "lxml==4.9.3"
    ]
    
    print("\nStep 2: Installing packages...")
    failed = []
    
    for req in requirements:
        print(f"\nInstalling {req}...")
        if not run_command(f"{sys.executable} -m pip install {req}"):
            failed.append(req)
            print(f"‚ö†Ô∏è  Failed to install {req}")
        else:
            print(f"‚úÖ {req} installed")
    
    # Step 3: Try to install Alpaca separately (often causes issues)
    print("\nStep 3: Attempting Alpaca installation...")
    if not run_command(f"{sys.executable} -m pip install alpaca-py==0.21.1"):
        print("‚ö†Ô∏è  Alpaca installation failed - you can run without it")
        print("   Paper trading will use mock trades instead")
    else:
        print("‚úÖ Alpaca installed successfully")
    
    # Summary
    print("\n" + "="*50)
    print("üìä Installation Summary:")
    print(f"‚úÖ Successful: {len(requirements) - len(failed)} packages")
    
    if failed:
        print(f"‚ùå Failed: {len(failed)} packages")
        for pkg in failed:
            print(f"   - {pkg}")
        print("\nYour trading system can still run, but some features may be limited.")
    else:
        print("\nüéâ All packages installed successfully!")
    
    print("\nüöÄ Next step: python setup_codespace.py")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

================================================================================
FILE: ./make_web.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: make_web.py
Version: 1.0.1
Last Updated: 2025-06-22

REVISION HISTORY:
- v1.0.1 (2025-06-22) - Fixed CSS syntax error
- v1.0.0 (2025-06-22) - Initial version

PURPOSE:
Creates web interface for Trading Application following project standards.
"""

import os
import sys
from pathlib import Path
from datetime import datetime

def create_web_interface():
    """Create the web interface with proper CSS embedding"""
    
    # HTML template with CSS properly embedded in string
    html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>Trading Application</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .header {
            background: #333;
            color: white;
            padding: 20px;
            margin: -20px -20px 20px -20px;
            border-radius: 10px 10px 0 0;
        }
        
        .nav {
            background: #444;
            padding: 10px;
            margin: 0 -20px 20px -20px;
        }
        
        .nav a {
            color: white;
            text-decoration: none;
            padding: 10px 20px;
            display: inline-block;
        }
        
        .nav a:hover {
            background: #555;
        }
        
        .content {
            padding: 20px 0;
        }
        
        .status-card {
            background: #f8f9fa;
            padding: 20px;
            margin: 10px 0;
            border-radius: 8px;
            border: 1px solid #dee2e6;
        }
        
        .status-healthy {
            border-left: 4px solid #28a745;
        }
        
        .status-error {
            border-left: 4px solid #dc3545;
        }
        
        .metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .metric {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #dee2e6;
            text-align: center;
        }
        
        .metric h3 {
            margin: 0 0 10px 0;
            color: #495057;
        }
        
        .metric .value {
            font-size: 2em;
            font-weight: bold;
            color: #007bff;
        }
        
        .button {
            background: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin: 5px;
        }
        
        .button:hover {
            background: #0056b3;
        }
        
        .button.success {
            background: #28a745;
        }
        
        .button.success:hover {
            background: #218838;
        }
        
        .button.danger {
            background: #dc3545;
        }
        
        .button.danger:hover {
            background: #c82333;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #dee2e6;
        }
        
        th {
            background: #f8f9fa;
            font-weight: bold;
        }
        
        .footer {
            margin-top: 40px;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            border-top: 1px solid #dee2e6;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Trading Application Dashboard</h1>
            <p>Real-time trading system management</p>
        </div>
        
        <div class="nav">
            <a href="#overview">Overview</a>
            <a href="#services">Services</a>
            <a href="#trading">Trading</a>
            <a href="#performance">Performance</a>
            <a href="#logs">Logs</a>
        </div>
        
        <div class="content">
            <section id="overview">
                <h2>System Overview</h2>
                
                <div class="metrics">
                    <div class="metric">
                        <h3>System Status</h3>
                        <div class="value">Active</div>
                    </div>
                    <div class="metric">
                        <h3>Active Services</h3>
                        <div class="value">8/8</div>
                    </div>
                    <div class="metric">
                        <h3>Trading Mode</h3>
                        <div class="value">Paper</div>
                    </div>
                    <div class="metric">
                        <h3>Database Status</h3>
                        <div class="value">Connected</div>
                    </div>
                </div>
                
                <div class="status-card status-healthy">
                    <h3>Quick Actions</h3>
                    <button class="button success">Start Trading</button>
                    <button class="button">View Positions</button>
                    <button class="button">Check Performance</button>
                    <button class="button danger">Stop All Services</button>
                </div>
            </section>
            
            <section id="services">
                <h2>Service Status</h2>
                
                <table>
                    <thead>
                        <tr>
                            <th>Service</th>
                            <th>Port</th>
                            <th>Status</th>
                            <th>Uptime</th>
                            <th>Actions</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Trading Engine</td>
                            <td>5000</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Market Data</td>
                            <td>5001</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Pattern Analysis</td>
                            <td>5002</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Risk Management</td>
                            <td>5003</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>News Service</td>
                            <td>5004</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Portfolio Manager</td>
                            <td>5005</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>Scheduler</td>
                            <td>5007</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                        <tr>
                            <td>ML Service</td>
                            <td>5009</td>
                            <td>Running</td>
                            <td>2h 15m</td>
                            <td><button class="button">Restart</button></td>
                        </tr>
                    </tbody>
                </table>
            </section>
        </div>
        
        <div class="footer">
            <p>Trading Application v1.0.1 | Last Updated: """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</p>
        </div>
    </div>
</body>
</html>
"""
    
    # Write the HTML file
    output_path = Path("trading_dashboard.html")
    
    try:
        with open(output_path, 'w') as f:
            f.write(html_content)
        
        print(f"‚úÖ Successfully created web interface: {output_path}")
        print(f"üìÅ File location: {output_path.absolute()}")
        
        # If running in Colab, provide additional instructions
        if 'google.colab' in sys.modules:
            print("\nüåê To view in Google Colab:")
            print("1. Run: from IPython.display import HTML")
            print("2. Run: HTML(filename='trading_dashboard.html')")
            
    except Exception as e:
        print(f"‚ùå Error creating web interface: {e}")
        return False
    
    return True

def main():
    """Main entry point"""
    print("Creating Trading Application Web Interface...")
    print("-" * 50)
    
    success = create_web_interface()
    
    if success:
        print("\n‚úÖ Web interface created successfully!")
    else:
        print("\n‚ùå Failed to create web interface")
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================================================
FILE: ./news_service.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM NEWS SERVICE
Version: 1.0.5
Last Updated: 2025-06-21
REVISION HISTORY:
v1.0.5 (2025-06-21) - Fixed NOT NULL constraint by ensuring article_date is always populated
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Enhanced sentiment analysis
v1.0.2 (2025-06-15) - Initial version
v1.0.1 (2025-06-15) - Original implementation

News Service - Provides news sentiment analysis for securities
Uses multiple NLP models for comprehensive sentiment analysis
Fixed to handle yfinance websockets dependency issues gracefully
Fixed NOT NULL constraint error for article_date field
"""

import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

# Import TextBlob for sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
    print("‚úÖ TextBlob imported successfully")
except ImportError:
    TEXTBLOB_AVAILABLE = False
    print("‚ö†Ô∏è TextBlob not available - using keyword-based sentiment only")

# Import database utilities if available
try:
    from database_utils import DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found, using direct SQLite connections")

class NewsService(DatabaseServiceMixin if USE_DB_UTILS else object):
    def __init__(self, db_path='./trading_system.db'):
        if USE_DB_UTILS:
            super().__init__(db_path)
        else:
            self.db_path = db_path
        
        self.app = Flask(__name__)
        self.setup_routes()
        self.setup_logging()
        
        # Sentiment keywords for basic analysis
        self.positive_keywords = [
            'upgrade', 'buy', 'strong', 'outperform', 'positive', 'growth',
            'beat', 'exceed', 'surge', 'rally', 'gain', 'profit', 'revenue',
            'bullish', 'optimistic', 'successful', 'breakthrough', 'innovation'
        ]
        
        self.negative_keywords = [
            'downgrade', 'sell', 'weak', 'underperform', 'negative', 'loss',
            'miss', 'decline', 'fall', 'drop', 'cut', 'layoff', 'lawsuit',
            'bearish', 'pessimistic', 'failure', 'concern', 'risk', 'warning'
        ]
        
        self.logger.info("News Service initialized with database utilities" if USE_DB_UTILS else "News Service initialized")
        
    def setup_logging(self):
        """Setup logging configuration"""
        self.logger = logging.getLogger('news_service')
        self.logger.setLevel(logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        self.logger.addHandler(handler)
        
    def setup_routes(self):
        """Setup Flask routes"""
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({"status": "healthy", "service": "news_service"})
            
        @self.app.route('/analyze_sentiment', methods=['POST'])
        def analyze_sentiment():
            data = request.json
            symbol = data.get('symbol')
            
            if not symbol:
                return jsonify({"error": "Symbol required"}), 400
                
            analysis = self._analyze_sentiment(symbol)
            return jsonify(analysis)
            
    def _get_news_data(self, symbol: str) -> List[Dict]:
        """Fetch news data for a symbol"""
        news_items = []
        
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                news = ticker.news
                
                for item in news[:10]:  # Get latest 10 news items
                    news_items.append({
                        'title': item.get('title', ''),
                        'publisher': item.get('publisher', ''),
                        'link': item.get('link', ''),
                        'timestamp': item.get('providerPublishTime', 0)
                    })
                    
            except Exception as e:
                self.logger.error(f"Error fetching news for {symbol}: {e}")
                
        else:
            # Simulated news data when yfinance not available
            news_items = [
                {
                    'title': f"{symbol} Shows Strong Performance in Q4",
                    'publisher': 'Financial Times',
                    'link': 'https://example.com',
                    'timestamp': int(datetime.now().timestamp())
                },
                {
                    'title': f"Analysts Upgrade {symbol} to Buy Rating",
                    'publisher': 'Reuters',
                    'link': 'https://example.com',
                    'timestamp': int(datetime.now().timestamp())
                }
            ]
            
        return news_items
        
    def _calculate_sentiment(self, text: str) -> tuple:
        """Calculate sentiment score and label"""
        if TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                
                if polarity > 0.1:
                    label = 'positive'
                elif polarity < -0.1:
                    label = 'negative'
                else:
                    label = 'neutral'
                    
                return polarity, label
                
            except Exception as e:
                self.logger.error(f"TextBlob error: {e}")
                
        # Fallback to keyword-based sentiment
        text_lower = text.lower()
        positive_count = sum(1 for word in self.positive_keywords if word in text_lower)
        negative_count = sum(1 for word in self.negative_keywords if word in text_lower)
        
        if positive_count > negative_count:
            score = 0.5
            label = 'positive'
        elif negative_count > positive_count:
            score = -0.5
            label = 'negative'
        else:
            score = 0.0
            label = 'neutral'
            
        return score, label
        
    def _analyze_sentiment(self, symbol: str) -> Dict:
        """Analyze sentiment for a symbol"""
        news_items = self._get_news_data(symbol)
        
        if not news_items:
            return {
                'symbol': symbol,
                'sentiment_score': 0.0,
                'sentiment_label': 'neutral',
                'news_count': 0,
                'analysis_time': datetime.now().isoformat()
            }
            
        sentiments = []
        positive_count = 0
        negative_count = 0
        
        for item in news_items:
            score, label = self._calculate_sentiment(item['title'])
            sentiments.append(score)
            
            if label == 'positive':
                positive_count += 1
            elif label == 'negative':
                negative_count += 1
                
        # Calculate average sentiment
        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0.0
        
        # Determine overall label
        if avg_sentiment > 0.1:
            overall_label = 'positive'
        elif avg_sentiment < -0.1:
            overall_label = 'negative'
        else:
            overall_label = 'neutral'
            
        analysis_result = {
            'symbol': symbol,
            'sentiment_score': round(avg_sentiment, 3),
            'sentiment_label': overall_label,
            'news_count': len(news_items),
            'positive_articles': positive_count,
            'negative_articles': negative_count,
            'neutral_articles': len(news_items) - positive_count - negative_count,
            'latest_news': news_items[:5],  # Return top 5 news items
            'analysis_time': datetime.now().isoformat()
        }
        
        # Save to database
        self._save_sentiment_analysis(analysis_result)
        
        return analysis_result
        
    def _save_sentiment_analysis(self, sentiment_data: Dict):
        """Save sentiment analysis to database"""
        try:
            # Ensure article_date is always populated
            analysis_time = sentiment_data.get('analysis_time', datetime.now().isoformat())
            
            # Convert ISO format to datetime object then back to ensure consistency
            if isinstance(analysis_time, str):
                article_date = datetime.fromisoformat(analysis_time.replace('Z', '+00:00')).isoformat()
            else:
                article_date = datetime.now().isoformat()
            
            if USE_DB_UTILS:
                # Use database utilities with retry logic
                with self.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    cursor.execute('''
                        INSERT INTO news_sentiment 
                        (symbol, article_date, sentiment_score, sentiment_label, 
                         news_count, positive_articles, negative_articles, metadata, analysis_timestamp)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        sentiment_data['symbol'],
                        article_date,  # FIXED: Always populated
                        sentiment_data['sentiment_score'],
                        sentiment_data['sentiment_label'],
                        sentiment_data['news_count'],
                        sentiment_data.get('positive_articles', 0),
                        sentiment_data.get('negative_articles', 0),
                        json.dumps(sentiment_data),
                        datetime.now().isoformat()
                    ))
                    
                    conn.commit()
                    
            else:
                # Direct SQLite connection
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO news_sentiment 
                    (symbol, article_date, sentiment_score, sentiment_label, 
                     news_count, positive_articles, negative_articles, metadata, analysis_timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    sentiment_data['symbol'],
                    article_date,  # FIXED: Always populated
                    sentiment_data['sentiment_score'],
                    sentiment_data['sentiment_label'],
                    sentiment_data['news_count'],
                    sentiment_data.get('positive_articles', 0),
                    sentiment_data.get('negative_articles', 0),
                    json.dumps(sentiment_data),
                    datetime.now().isoformat()
                ))
                
                conn.commit()
                conn.close()
                
            self.logger.info(f"Saved sentiment analysis for {sentiment_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving sentiment analysis: {e}")
            # Log the specific data that caused the error for debugging
            self.logger.error(f"Failed data: {sentiment_data}")
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        textblob_status = "with TextBlob" if TEXTBLOB_AVAILABLE else "keyword-based only"
        db_mode = "with retry logic" if USE_DB_UTILS else "direct connection"
        
        self.logger.info(f"Starting News Service on port 5008 {mode}, {textblob_status}, {db_mode}")
        self.app.run(host='0.0.0.0', port=5008, debug=False)

if __name__ == "__main__":
    service = NewsService()
    service.run()


================================================================================
FILE: ./paper_trading.py
================================================================================

"""
Name of Service: TRADING SYSTEM PHASE 1 - PAPER TRADING
Version: 1.0.2
Last Updated: 2025-06-15
REVISION HISTORY:
v1.0.2 (2025-06-15) - Updated to use ALPACA_PAPER_API_KEY and ALPACA_PAPER_API_SECRET environment variables
v1.0.1 (2025-06-15) - Updated header format and moved API credentials to environment variables
v1.0.0 (2025-06-15) - Initial release with Alpaca paper trading integration

Paper Trading Service - Executes trades using Alpaca paper trading API
Receives trading signals and executes them via Alpaca
"""

import os
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Note: You'll need to install alpaca-trade-api
# !pip install alpaca-trade-api
try:
    import alpaca_trade_api as tradeapi
    ALPACA_AVAILABLE = True
except ImportError:
    ALPACA_AVAILABLE = False

class PaperTradingService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        # Alpaca configuration from environment variables - UPDATED TO USE PAPER TRADING VARS
        self.api_key = os.environ.get('ALPACA_PAPER_API_KEY', '')
        self.api_secret = os.environ.get('ALPACA_PAPER_API_SECRET', '')
        self.base_url = os.environ.get('ALPACA_BASE_URL', 'https://paper-api.alpaca.markets')
        
        # Log configuration status (without exposing secrets)
        if self.api_key and self.api_secret:
            self.logger.info("Alpaca Paper Trading API credentials loaded from environment")
        else:
            self.logger.warning("Alpaca Paper Trading API credentials not found in environment variables")
            self.logger.info("Set ALPACA_PAPER_API_KEY and ALPACA_PAPER_API_SECRET environment variables")
        
        self.alpaca_api = None
        self._setup_alpaca_api()
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PaperTradingService')
        
        handler = logging.FileHandler('./logs/paper_trading_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_alpaca_api(self):
        """Setup Alpaca API connection"""
        if not ALPACA_AVAILABLE:
            self.logger.warning("Alpaca API not available - install alpaca-trade-api")
            return
        
        if not self.api_key or not self.api_secret:
            self.logger.warning("Alpaca Paper Trading API credentials not configured - running in simulation mode")
            return
        
        try:
            self.alpaca_api = tradeapi.REST(
                self.api_key,
                self.api_secret,
                self.base_url,
                api_version='v2'
            )
            
            # Test connection
            account = self.alpaca_api.get_account()
            self.logger.info(f"Connected to Alpaca Paper Trading API. Account status: {account.status}")
            self.logger.info(f"Paper Trading Account - Buying Power: ${float(account.buying_power):,.2f}")
            
        except Exception as e:
            self.logger.error(f"Error setting up Alpaca Paper Trading API: {e}")
            self.alpaca_api = None
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "paper_trading",
                "alpaca_connected": self.alpaca_api is not None,
                "trading_mode": "paper" if self.alpaca_api else "simulation"
            })
        
        @self.app.route('/execute_trades', methods=['POST'])
        def execute_trades_endpoint():
            signals = request.json.get('signals', [])
            trades = self._execute_trades(signals)
            return jsonify(trades)
        
        @self.app.route('/account', methods=['GET'])
        def get_account():
            account_info = self._get_account_info()
            return jsonify(account_info)
        
        @self.app.route('/positions', methods=['GET'])
        def get_positions():
            positions = self._get_positions()
            return jsonify(positions)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "paper_trading",
                "port": 5005
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _execute_trades(self, trading_signals: List[Dict]) -> List[Dict]:
        """Execute trades based on trading signals"""
        self.logger.info(f"Executing trades for {len(trading_signals)} signals")
        
        executed_trades = []
        
        for signal in trading_signals:
            try:
                if signal.get('signal') not in ['BUY', 'SELL']:
                    continue
                
                trade_result = self._execute_single_trade(signal)
                if trade_result:
                    executed_trades.append(trade_result)
                
            except Exception as e:
                self.logger.error(f"Error executing trade for {signal.get('symbol', 'unknown')}: {e}")
        
        self.logger.info(f"Trade execution completed: {len(executed_trades)} trades executed")
        return executed_trades
    
    def _execute_single_trade(self, signal: Dict) -> Optional[Dict]:
        """Execute a single trade"""
        symbol = signal['symbol']
        signal_type = signal['signal']
        quantity = signal.get('quantity', 100)
        
        try:
            if self.alpaca_api is None:
                # Simulate trade execution for demo
                return self._simulate_trade_execution(signal)
            
            # Check account
            account = self.alpaca_api.get_account()
            if signal_type == 'BUY' and float(account.buying_power) < 1000:
                self.logger.warning(f"Insufficient buying power for {symbol}")
                return None
            
            # Get current price
            try:
                latest_trade = self.alpaca_api.get_latest_trade(symbol)
                current_price = latest_trade.price
            except:
                # Fallback to last close price
                current_price = signal.get('current_price', 100)
            
            # Submit order
            side = 'buy' if signal_type == 'BUY' else 'sell'
            
            order = self.alpaca_api.submit_order(
                symbol=symbol,
                qty=quantity,
                side=side,
                type='market',
                time_in_force='day'
            )
            
            # Create trade record
            trade_record = {
                'symbol': symbol,
                'signal': signal_type,
                'quantity': quantity,
                'entry_price': current_price,
                'confidence': signal.get('confidence', 0.0),
                'reason': signal.get('reason', 'Technical analysis signal'),
                'alpaca_order_id': order.id,
                'status': 'executed',
                'timestamp': datetime.now().isoformat()
            }
            
            # Save to database
            self._save_trade_record(trade_record)
            
            self.logger.info(f"Executed {signal_type} trade for {symbol}: {quantity} shares at ${current_price:.2f}")
            return trade_record
            
        except Exception as e:
            self.logger.error(f"Error executing trade for {symbol}: {e}")
            return None
    
    def _simulate_trade_execution(self, signal: Dict) -> Dict:
        """Simulate trade execution when Alpaca API is not available"""
        symbol = signal['symbol']
        signal_type = signal['signal']
        quantity = signal.get('quantity', 100)
        current_price = signal.get('current_price', 100)
        
        trade_record = {
            'symbol': symbol,
            'signal': signal_type,
            'quantity': quantity,
            'entry_price': current_price,
            'confidence': signal.get('confidence', 0.0),
            'reason': signal.get('reason', 'Simulated trade'),
            'alpaca_order_id': f"SIM_{datetime.now().strftime('%Y%m%d%H%M%S')}",
            'status': 'simulated',
            'timestamp': datetime.now().isoformat()
        }
        
        # Save to database
        self._save_trade_record(trade_record)
        
        self.logger.info(f"Simulated {signal_type} trade for {symbol}: {quantity} shares at ${current_price:.2f}")
        return trade_record
    
    def _save_trade_record(self, trade_data: Dict):
        """Save trade record to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trades 
                (symbol, signal_type, quantity, entry_price, confidence, 
                 trade_reason, alpaca_order_id, status, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                trade_data['symbol'],
                trade_data['signal'],
                trade_data['quantity'],
                trade_data['entry_price'],
                trade_data['confidence'],
                trade_data['reason'],
                trade_data['alpaca_order_id'],
                trade_data['status'],
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved trade record for {trade_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving trade record: {e}")
    
    def _get_account_info(self) -> Dict:
        """Get account information"""
        try:
            if self.alpaca_api is None:
                return {
                    "status": "simulated", 
                    "buying_power": 100000, 
                    "portfolio_value": 100000,
                    "mode": "simulation"
                }
            
            account = self.alpaca_api.get_account()
            
            return {
                "account_status": account.status,
                "buying_power": float(account.buying_power),
                "portfolio_value": float(account.portfolio_value),
                "cash": float(account.cash),
                "equity": float(account.equity),
                "mode": "paper_trading",
                "pattern_day_trader": account.pattern_day_trader,
                "trading_blocked": account.trading_blocked,
                "transfers_blocked": account.transfers_blocked
            }
            
        except Exception as e:
            self.logger.error(f"Error getting account info: {e}")
            return {"error": str(e)}
    
    def _get_positions(self) -> List[Dict]:
        """Get current positions"""
        try:
            if self.alpaca_api is None:
                return []
            
            positions = self.alpaca_api.list_positions()
            
            position_list = []
            for position in positions:
                position_list.append({
                    "symbol": position.symbol,
                    "quantity": int(position.qty),
                    "side": "long" if int(position.qty) > 0 else "short",
                    "market_value": float(position.market_value),
                    "unrealized_pl": float(position.unrealized_pl),
                    "unrealized_plpc": float(position.unrealized_plpc),
                    "avg_entry_price": float(position.avg_entry_price),
                    "current_price": float(position.current_price) if hasattr(position, 'current_price') else None
                })
            
            return position_list
            
        except Exception as e:
            self.logger.error(f"Error getting positions: {e}")
            return []
    
    def run(self):
        self.logger.info("Starting Paper Trading Service on port 5005")
        if self.alpaca_api:
            self.logger.info("Alpaca Paper Trading API connected and ready")
        else:
            self.logger.info("Running in simulation mode (no Alpaca connection)")
        self.app.run(host='0.0.0.0', port=5005, debug=False)

if __name__ == "__main__":
    service = PaperTradingService()
    service.run()


================================================================================
FILE: ./pattern_analysis.py
================================================================================

# ================================================================
# 3. pattern_analysis.py (Port 5002) - FIXED VERSION
# ================================================================
"""
Name of Service: TRADING SYSTEM PATTERN ANALYSIS - FIXED VERSION
Version: 1.0.5
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.5 (2025-06-17) - CRITICAL FIX: JSON serialization error with boolean values
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Removed TA-Lib dependency, using only manual pattern detection
v1.0.2 (2025-06-15) - Fixed version with TA-Lib fallback
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Pattern Analysis Service - Analyzes technical patterns using manual calculation methods
CRITICAL BUG FIX: Fixed JSON serialization error when saving patterns with boolean values
"""

import numpy as np
import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

class PatternAnalysisService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        self.pattern_recognition_url = "http://localhost:5006"
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PatternAnalysisService')
        
        handler = logging.FileHandler('./logs/pattern_analysis_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "pattern_analysis", 
                "implementation": "manual_algorithms",
                "yfinance_available": YFINANCE_AVAILABLE,
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated"
            })
        
        @self.app.route('/analyze_patterns/<symbol>', methods=['GET'])
        def analyze_patterns_endpoint(symbol):
            analysis = self._analyze_patterns(symbol)
            return jsonify(analysis)
        
        @self.app.route('/supported_patterns', methods=['GET'])
        def get_supported_patterns():
            return jsonify(self._get_supported_patterns())
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "pattern_analysis", 
                "port": 5002
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _analyze_patterns(self, symbol: str) -> Dict:
        """Main pattern analysis logic"""
        self.logger.info(f"Starting pattern analysis for {symbol}")
        
        try:
            # Get market data
            hist_data = self._get_historical_data(symbol)
            if hist_data is None:
                return {'symbol': symbol, 'patterns': [], 'error': 'No data available'}
            
            # Manual pattern analysis
            basic_patterns = self._detect_basic_patterns(symbol, hist_data)
            
            # Get enhanced patterns from pattern recognition service
            enhanced_patterns = self._get_enhanced_patterns(symbol)
            
            # Combine results
            all_patterns = basic_patterns + enhanced_patterns.get('candlestick_patterns', [])
            
            # Calculate overall confidence
            confidence_score = self._calculate_confidence(all_patterns)
            
            combined_analysis = {
                'symbol': symbol,
                'basic_patterns': basic_patterns,
                'enhanced_patterns': enhanced_patterns,
                'patterns': all_patterns,
                'confidence_score': confidence_score,
                'pattern_count': len(all_patterns),
                'analysis_timestamp': datetime.now().isoformat(),
                'implementation': 'manual_algorithms',
                'data_source': 'yfinance' if YFINANCE_AVAILABLE else 'simulated'
            }
            
            # Save to database - FIXED VERSION with proper JSON handling
            self._save_pattern_analysis(symbol, combined_analysis)
            
            self.logger.info(f"Pattern analysis completed for {symbol}: {len(all_patterns)} patterns found")
            return combined_analysis
            
        except Exception as e:
            self.logger.error(f"Error in pattern analysis for {symbol}: {e}")
            return {'symbol': symbol, 'patterns': [], 'error': str(e)}
    
    def _get_historical_data(self, symbol: str, period: str = "30d") -> Optional[pd.DataFrame]:
        """Get historical data for pattern analysis"""
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period=period)
                
                if len(hist) < 20:
                    return self._generate_simulated_data(symbol)
                
                return hist
                
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using simulated data: {e}")
                return self._generate_simulated_data(symbol)
        else:
            return self._generate_simulated_data(symbol)
    
    def _generate_simulated_data(self, symbol: str) -> pd.DataFrame:
        """Generate simulated OHLCV data for pattern analysis"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data
        random.seed(hash(symbol) + int(time.time() / 86400))
        
        # Generate 30 days of simulated data
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        
        # Start with a base price
        base_price = random.uniform(10, 200)
        prices = []
        volumes = []
        
        current_price = base_price
        for i in range(30):
            # Random daily change
            change_pct = random.uniform(-0.05, 0.05)  # -5% to +5%
            current_price *= (1 + change_pct)
            
            # Generate OHLC
            open_price = current_price * random.uniform(0.98, 1.02)
            close_price = current_price
            high_price = max(open_price, close_price) * random.uniform(1.0, 1.05)
            low_price = min(open_price, close_price) * random.uniform(0.95, 1.0)
            volume = random.randint(100000, 1000000)
            
            prices.append([open_price, high_price, low_price, close_price])
            volumes.append(volume)
        
        # Create DataFrame
        data = pd.DataFrame(prices, columns=['Open', 'High', 'Low', 'Close'], index=dates)
        data['Volume'] = volumes
        
        return data
    
    def _detect_basic_patterns(self, symbol: str, data: pd.DataFrame) -> List[Dict]:
        """Detect basic candlestick patterns using manual calculations"""
        patterns = []
        
        try:
            close_prices = data['Close'].values
            open_prices = data['Open'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            
            # Check last 5 days for patterns
            for i in range(-5, 0):
                if abs(i) > len(close_prices):
                    continue
                    
                current_open = open_prices[i]
                current_close = close_prices[i]
                current_high = high_prices[i]
                current_low = low_prices[i]
                
                # Calculate body and shadows
                body = abs(current_close - current_open)
                lower_shadow = min(current_open, current_close) - current_low
                upper_shadow = current_high - max(current_open, current_close)
                total_range = current_high - current_low
                
                # Avoid division by zero
                if total_range == 0:
                    continue
                
                # Doji pattern (open ‚âà close) - FIXED: Convert booleans properly
                if body < (total_range * 0.1):
                    patterns.append({
                        'pattern_type': 'doji',
                        'signal_strength': 100,
                        'confidence_score': 0.7,
                        'bullish': None,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Small body indicates indecision'
                    })
                
                # Hammer pattern (small body at top, long lower shadow)
                if lower_shadow > body * 2 and upper_shadow < body * 0.5 and body > 0:
                    patterns.append({
                        'pattern_type': 'hammer',
                        'signal_strength': 100,
                        'confidence_score': 0.6,
                        'bullish': True,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Long lower shadow suggests buying pressure'
                    })
                
                # Shooting star (small body at bottom, long upper shadow)
                if upper_shadow > body * 2 and lower_shadow < body * 0.5 and body > 0:
                    patterns.append({
                        'pattern_type': 'shooting_star',
                        'signal_strength': -100,
                        'confidence_score': 0.6,
                        'bullish': False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': 'Long upper shadow suggests selling pressure'
                    })
                
                # Engulfing patterns (requires previous candle)
                if i > -len(close_prices) and abs(i) > 1:
                    prev_open = open_prices[i-1]
                    prev_close = close_prices[i-1]
                    prev_body = abs(prev_close - prev_open)
                    
                    # Bullish engulfing
                    if (current_close > current_open and  # Current candle is bullish
                        prev_close < prev_open and        # Previous candle is bearish
                        current_open < prev_close and     # Current opens below previous close
                        current_close > prev_open and     # Current closes above previous open
                        body > prev_body * 1.1):          # Current body is larger
                        
                        patterns.append({
                            'pattern_type': 'bullish_engulfing',
                            'signal_strength': 100,
                            'confidence_score': 0.8,
                            'bullish': True,  # Will be converted to string
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_calculation',
                            'description': 'Bullish candle engulfs previous bearish candle'
                        })
                    
                    # Bearish engulfing
                    elif (current_close < current_open and  # Current candle is bearish
                          prev_close > prev_open and        # Previous candle is bullish
                          current_open > prev_close and     # Current opens above previous close
                          current_close < prev_open and     # Current closes below previous open
                          body > prev_body * 1.1):          # Current body is larger
                        
                        patterns.append({
                            'pattern_type': 'bearish_engulfing',
                            'signal_strength': -100,
                            'confidence_score': 0.8,
                            'bullish': False,  # Will be converted to string
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_calculation',
                            'description': 'Bearish candle engulfs previous bullish candle'
                        })
            
            # Trend detection using linear regression
            if len(close_prices) >= 10:
                recent_trend = np.polyfit(range(10), close_prices[-10:], 1)[0]
                price_std = np.std(close_prices[-20:]) if len(close_prices) >= 20 else np.std(close_prices)
                
                if abs(recent_trend) > price_std * 0.01:  # Significant trend
                    patterns.append({
                        'pattern_type': 'trend_detected',
                        'signal_strength': 100 if recent_trend > 0 else -100,
                        'confidence_score': min(abs(recent_trend) * 100, 0.9),
                        'bullish': True if recent_trend > 0 else False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f"{'Upward' if recent_trend > 0 else 'Downward'} trend detected"
                    })
            
            # Support and resistance levels
            if len(close_prices) >= 20:
                recent_high = np.max(high_prices[-20:])
                recent_low = np.min(low_prices[-20:])
                current_price = close_prices[-1]
                
                # Near resistance
                if current_price >= recent_high * 0.98:
                    patterns.append({
                        'pattern_type': 'near_resistance',
                        'signal_strength': -50,
                        'confidence_score': 0.7,
                        'bullish': False,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f'Price near resistance level: ${recent_high:.2f}'
                    })
                
                # Near support
                if current_price <= recent_low * 1.02:
                    patterns.append({
                        'pattern_type': 'near_support',
                        'signal_strength': 50,
                        'confidence_score': 0.7,
                        'bullish': True,  # Will be converted to string
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_calculation',
                        'description': f'Price near support level: ${recent_low:.2f}'
                    })
            
        except Exception as e:
            self.logger.error(f"Error in manual pattern detection: {e}")
        
        return patterns
    
    def _get_enhanced_patterns(self, symbol: str) -> Dict:
        """Get enhanced patterns from pattern recognition service"""
        try:
            response = requests.get(f"{self.pattern_recognition_url}/detect_advanced_patterns/{symbol}", 
                                  timeout=30)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.warning(f"Could not get enhanced patterns for {symbol}")
                return {}
        except Exception as e:
            self.logger.warning(f"Error getting enhanced patterns for {symbol}: {e}")
            return {}
    
    def _calculate_confidence(self, patterns: List[Dict]) -> float:
        """Calculate overall pattern confidence"""
        if not patterns:
            return 0.0
        
        total_confidence = sum([p.get('confidence_score', 0) for p in patterns])
        return min(total_confidence / len(patterns), 1.0)
    
    def _save_pattern_analysis(self, symbol: str, analysis_data: Dict):
        """Save pattern analysis to database - FIXED VERSION with proper JSON serialization"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # CRITICAL FIX: Convert all values to JSON-serializable format
            json_safe_data = self._make_json_serializable(analysis_data)
            
            cursor.execute('''
                INSERT INTO pattern_analysis 
                (symbol, analysis_date, pattern_type, pattern_name, confidence_score, additional_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                symbol,
                datetime.now().date().isoformat(),
                'combined_analysis',
                f"{len(analysis_data.get('patterns', []))} patterns detected",
                analysis_data.get('confidence_score', 0.0),
                json.dumps(json_safe_data),  # Now safe to serialize
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved pattern analysis for {symbol}")
            
        except Exception as e:
            self.logger.error(f"Error saving pattern analysis for {symbol}: {e}")
    
    def _make_json_serializable(self, obj):
        """CRITICAL FIX: Convert object to JSON-serializable format"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, bool):
            return str(obj).lower()  # Convert True/False to "true"/"false"
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)  # Convert numpy types to standard Python types
        elif obj is None:
            return "null"  # Convert None to string
        else:
            return obj  # Return as-is for strings, numbers, etc.
    
    def _get_supported_patterns(self) -> List[str]:
        """Get list of supported pattern types"""
        return [
            'doji', 'hammer', 'shooting_star', 
            'bullish_engulfing', 'bearish_engulfing',
            'trend_detected', 'near_resistance', 'near_support',
            'advanced_chart_patterns', 'advanced_volume_patterns'
        ]
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        self.logger.info(f"Starting Pattern Analysis Service on port 5002 {mode}")
        self.app.run(host='0.0.0.0', port=5002, debug=False)

if __name__ == "__main__":
    service = PatternAnalysisService()
    service.run()

================================================================================
FILE: ./pattern_recognition_service.py
================================================================================

# ================================================================
# 6. pattern_recognition_service.py (Port 5006) - CORRECTED VERSION
# ================================================================
"""
Name of Service: TRADING SYSTEM ADVANCED PATTERN RECOGNITION - CORRECTED VERSION
Version: 1.0.5
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.5 (2025-06-19) - Integrated database utilities to fix database locking issues
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Removed TA-Lib dependency, using only manual pattern detection algorithms
v1.0.2 (2025-06-15) - Enhanced pattern detection
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Advanced Pattern Recognition Service - Enhanced pattern detection using manual ML algorithms
Complements the basic pattern analysis with advanced mathematical algorithms
Now uses database utilities with retry logic to prevent locking issues
"""

import numpy as np
import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Import database utilities
try:
    from database_utils import get_database_manager, DatabaseServiceMixin
    USE_DB_UTILS = True
except ImportError:
    USE_DB_UTILS = False
    print("Warning: database_utils not found, using direct SQLite connections")

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

class PatternRecognitionService(DatabaseServiceMixin if USE_DB_UTILS else object):
    def __init__(self, db_path='./trading_system.db'):
        # Initialize database utilities if available
        if USE_DB_UTILS:
            super().__init__(db_path)
        else:
            self.db_path = db_path
            
        self.app = Flask(__name__)
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('PatternRecognitionService')
        
        handler = logging.FileHandler('./logs/pattern_recognition_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "pattern_recognition", 
                "implementation": "manual_algorithms",
                "yfinance_available": YFINANCE_AVAILABLE,
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated",
                "database_mode": "with_retry" if USE_DB_UTILS else "direct"
            })
        
        @self.app.route('/detect_advanced_patterns/<symbol>', methods=['GET'])
        def detect_advanced_patterns(symbol):
            patterns = self._detect_advanced_patterns(symbol)
            return jsonify(patterns)
        
        @self.app.route('/candlestick_patterns/<symbol>', methods=['GET'])
        def candlestick_patterns(symbol):
            patterns = self._detect_candlestick_patterns(symbol)
            return jsonify(patterns)
        
        @self.app.route('/chart_patterns/<symbol>', methods=['GET'])
        def chart_patterns(symbol):
            patterns = self._detect_chart_patterns(symbol)
            return jsonify(patterns)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "pattern_recognition",
                "port": 5006
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _detect_advanced_patterns(self, symbol: str) -> Dict:
        """Detect advanced patterns using multiple manual techniques"""
        try:
            data = self._get_market_data(symbol)
            if data.empty:
                return {"error": "No data available"}
            
            # Different pattern types using manual algorithms
            candlestick_patterns = self._detect_candlestick_patterns(symbol, data)
            chart_patterns = self._detect_chart_patterns(symbol, data)
            volume_patterns = self._detect_volume_patterns(symbol, data)
            
            # Calculate overall pattern score
            pattern_score = self._calculate_pattern_score(candlestick_patterns, chart_patterns, volume_patterns)
            
            result = {
                'symbol': symbol,
                'candlestick_patterns': candlestick_patterns,
                'chart_patterns': chart_patterns,
                'volume_patterns': volume_patterns,
                'overall_pattern_score': pattern_score,
                'analysis_time': datetime.now().isoformat(),
                'implementation': 'manual_algorithms',
                'data_source': 'yfinance' if YFINANCE_AVAILABLE else 'simulated'
            }
            
            # Save to database with retry logic
            self._save_pattern_analysis(result)
            
            self.logger.info(f"Advanced pattern analysis completed for {symbol}: score {pattern_score}")
            return result
            
        except Exception as e:
            self.logger.error(f"Error in advanced pattern detection for {symbol}: {e}")
            return {"error": str(e)}
    
    def _get_market_data(self, symbol: str, period: str = "30d") -> pd.DataFrame:
        """Get market data for pattern analysis"""
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period=period)
                
                if len(hist) < 20:
                    return self._generate_simulated_data(symbol)
                
                return hist
                
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using simulated data: {e}")
                return self._generate_simulated_data(symbol)
        else:
            return self._generate_simulated_data(symbol)
    
    def _generate_simulated_data(self, symbol: str) -> pd.DataFrame:
        """Generate simulated OHLCV data for pattern analysis"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data
        random.seed(hash(symbol) + int(time.time() / 86400))
        
        # Generate 30 days of simulated data
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        
        # Start with a base price
        base_price = random.uniform(10, 200)
        prices = []
        volumes = []
        
        current_price = base_price
        for i in range(30):
            # Random daily change
            change_pct = random.uniform(-0.05, 0.05)  # -5% to +5%
            current_price *= (1 + change_pct)
            
            # Generate OHLC
            open_price = current_price * random.uniform(0.98, 1.02)
            close_price = current_price
            high_price = max(open_price, close_price) * random.uniform(1.0, 1.05)
            low_price = min(open_price, close_price) * random.uniform(0.95, 1.0)
            volume = random.randint(100000, 1000000)
            
            prices.append([open_price, high_price, low_price, close_price])
            volumes.append(volume)
        
        # Create DataFrame
        data = pd.DataFrame(prices, columns=['Open', 'High', 'Low', 'Close'], index=dates)
        data['Volume'] = volumes
        
        return data
    
    def _detect_candlestick_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect candlestick patterns using manual mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            open_prices = data['Open'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            close_prices = data['Close'].values
            
            # Analyze last 10 days for patterns
            for i in range(max(-10, -len(close_prices)), 0):
                if abs(i) > len(close_prices) or abs(i-1) > len(close_prices):
                    continue
                
                # Current candle
                curr_open = open_prices[i]
                curr_high = high_prices[i]
                curr_low = low_prices[i]
                curr_close = close_prices[i]
                
                # Calculate current candle properties
                curr_body = abs(curr_close - curr_open)
                curr_range = curr_high - curr_low
                curr_upper_shadow = curr_high - max(curr_open, curr_close)
                curr_lower_shadow = min(curr_open, curr_close) - curr_low
                
                if curr_range == 0:
                    continue
                
                # Previous candle (for multi-candle patterns)
                if abs(i-1) < len(close_prices):
                    prev_open = open_prices[i-1]
                    prev_high = high_prices[i-1]
                    prev_low = low_prices[i-1] 
                    prev_close = close_prices[i-1]
                    prev_body = abs(prev_close - prev_open)
                
                # DOJI Pattern
                if curr_body < curr_range * 0.1:
                    patterns.append({
                        'pattern_name': 'doji',
                        'signal_strength': 0,  # Neutral
                        'confidence_score': 0.7,
                        'bullish': None,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Indecision pattern - open equals close'
                    })
                
                # HAMMER Pattern
                if (curr_lower_shadow > curr_body * 2 and 
                    curr_upper_shadow < curr_body * 0.3 and 
                    curr_body > 0):
                    patterns.append({
                        'pattern_name': 'hammer',
                        'signal_strength': 80,
                        'confidence_score': 0.75,
                        'bullish': True,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Bullish reversal - long lower shadow'
                    })
                
                # HANGING MAN Pattern (like hammer but at top of uptrend)
                if (curr_lower_shadow > curr_body * 2 and 
                    curr_upper_shadow < curr_body * 0.3 and
                    curr_body > 0 and
                    i > -5):  # Check if in potential uptrend
                    recent_trend = np.mean(close_prices[i-3:i]) if abs(i-3) < len(close_prices) else curr_close
                    if curr_close > recent_trend * 1.02:  # In uptrend
                        patterns.append({
                            'pattern_name': 'hanging_man',
                            'signal_strength': -60,
                            'confidence_score': 0.65,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Bearish reversal at top of uptrend'
                        })
                
                # SHOOTING STAR Pattern
                if (curr_upper_shadow > curr_body * 2 and 
                    curr_lower_shadow < curr_body * 0.3 and 
                    curr_body > 0):
                    patterns.append({
                        'pattern_name': 'shooting_star',
                        'signal_strength': -80,
                        'confidence_score': 0.75,
                        'bullish': False,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Bearish reversal - long upper shadow'
                    })
                
                # ENGULFING Patterns (requires previous candle)
                if abs(i-1) < len(close_prices):
                    # Bullish Engulfing
                    if (curr_close > curr_open and  # Current is bullish
                        prev_close < prev_open and  # Previous is bearish
                        curr_open < prev_close and  # Opens below prev close
                        curr_close > prev_open and  # Closes above prev open
                        curr_body > prev_body * 1.1):  # Larger body
                        
                        patterns.append({
                            'pattern_name': 'bullish_engulfing',
                            'signal_strength': 90,
                            'confidence_score': 0.85,
                            'bullish': True,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bullish reversal pattern'
                        })
                    
                    # Bearish Engulfing
                    elif (curr_close < curr_open and  # Current is bearish
                          prev_close > prev_open and  # Previous is bullish
                          curr_open > prev_close and  # Opens above prev close
                          curr_close < prev_open and  # Closes below prev open
                          curr_body > prev_body * 1.1):  # Larger body
                        
                        patterns.append({
                            'pattern_name': 'bearish_engulfing',
                            'signal_strength': -90,
                            'confidence_score': 0.85,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bearish reversal pattern'
                        })
                
                # SPINNING TOP Pattern
                if (curr_body < curr_range * 0.3 and
                    curr_upper_shadow > curr_body * 0.5 and
                    curr_lower_shadow > curr_body * 0.5):
                    patterns.append({
                        'pattern_name': 'spinning_top',
                        'signal_strength': 0,
                        'confidence_score': 0.6,
                        'bullish': None,
                        'detected_at': datetime.now().isoformat(),
                        'source': 'manual_algorithm',
                        'description': 'Indecision with long shadows both ways'
                    })
            
            # THREE WHITE SOLDIERS Pattern (3 consecutive bullish candles)
            if len(close_prices) >= 3:
                for i in range(-3, 0):
                    if abs(i-2) >= len(close_prices):
                        continue
                    
                    candle1_bull = close_prices[i-2] > open_prices[i-2]
                    candle2_bull = close_prices[i-1] > open_prices[i-1] 
                    candle3_bull = close_prices[i] > open_prices[i]
                    
                    if (candle1_bull and candle2_bull and candle3_bull and
                        close_prices[i-1] > close_prices[i-2] and
                        close_prices[i] > close_prices[i-1]):
                        
                        patterns.append({
                            'pattern_name': 'three_white_soldiers',
                            'signal_strength': 85,
                            'confidence_score': 0.8,
                            'bullish': True,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bullish continuation pattern'
                        })
                        break  # Only detect once
            
            # THREE BLACK CROWS Pattern (3 consecutive bearish candles)
            if len(close_prices) >= 3:
                for i in range(-3, 0):
                    if abs(i-2) >= len(close_prices):
                        continue
                    
                    candle1_bear = close_prices[i-2] < open_prices[i-2]
                    candle2_bear = close_prices[i-1] < open_prices[i-1]
                    candle3_bear = close_prices[i] < open_prices[i]
                    
                    if (candle1_bear and candle2_bear and candle3_bear and
                        close_prices[i-1] < close_prices[i-2] and
                        close_prices[i] < close_prices[i-1]):
                        
                        patterns.append({
                            'pattern_name': 'three_black_crows',
                            'signal_strength': -85,
                            'confidence_score': 0.8,
                            'bullish': False,
                            'detected_at': datetime.now().isoformat(),
                            'source': 'manual_algorithm',
                            'description': 'Strong bearish continuation pattern'
                        })
                        break  # Only detect once
                        
        except Exception as e:
            self.logger.error(f"Error in candlestick pattern detection: {e}")
        
        return patterns
    
    def _detect_chart_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect chart patterns like support/resistance, trends using mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            close_prices = data['Close'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            
            # Trend detection using linear regression
            if len(close_prices) >= 20:
                # Short-term trend (10 days)
                short_x = np.arange(10)
                short_trend = np.polyfit(short_x, close_prices[-10:], 1)[0]
                
                # Long-term trend (20 days) 
                long_x = np.arange(20)
                long_trend = np.polyfit(long_x, close_prices[-20:], 1)[0]
                
                # Calculate trend strength
                price_std = np.std(close_prices[-20:])
                short_strength = abs(short_trend) / price_std if price_std > 0 else 0
                long_strength = abs(long_trend) / price_std if price_std > 0 else 0
                
                # Strong uptrend
                if short_trend > 0 and long_trend > 0 and short_strength > 0.02:
                    patterns.append({
                        'pattern_name': 'strong_uptrend',
                        'strength': min(short_strength * 10, 1.0),
                        'timeframe': 'short_and_long_term',
                        'confidence_score': 0.8,
                        'trend_slope': short_trend,
                        'description': 'Sustained upward price movement'
                    })
                
                # Strong downtrend
                elif short_trend < 0 and long_trend < 0 and short_strength > 0.02:
                    patterns.append({
                        'pattern_name': 'strong_downtrend',
                        'strength': min(short_strength * 10, 1.0),
                        'timeframe': 'short_and_long_term', 
                        'confidence_score': 0.8,
                        'trend_slope': short_trend,
                        'description': 'Sustained downward price movement'
                    })
                
                # Trend reversal
                elif (short_trend > 0 and long_trend < 0) or (short_trend < 0 and long_trend > 0):
                    patterns.append({
                        'pattern_name': 'trend_reversal',
                        'strength': min(abs(short_trend - long_trend) / price_std, 1.0),
                        'timeframe': 'reversal_detected',
                        'confidence_score': 0.7,
                        'description': 'Recent trend change detected'
                    })
            
            # Support and Resistance Levels
            current_price = close_prices[-1]
            
            # Calculate recent highs and lows
            if len(close_prices) >= 20:
                highs_20 = np.max(high_prices[-20:])
                lows_20 = np.min(low_prices[-20:])
                
                # Resistance level analysis
                if current_price >= highs_20 * 0.98:  # Within 2% of resistance
                    resistance_touches = np.sum(high_prices[-20:] >= highs_20 * 0.99)
                    patterns.append({
                        'pattern_name': 'near_resistance',
                        'resistance_level': highs_20,
                        'current_price': current_price,
                        'distance_pct': ((current_price - highs_20) / highs_20) * 100,
                        'touches': int(resistance_touches),
                        'confidence_score': min(0.9, 0.5 + resistance_touches * 0.1),
                        'description': f'Price near resistance at ${highs_20:.2f}'
                    })
                
                # Support level analysis
                if current_price <= lows_20 * 1.02:  # Within 2% of support
                    support_touches = np.sum(low_prices[-20:] <= lows_20 * 1.01)
                    patterns.append({
                        'pattern_name': 'near_support',
                        'support_level': lows_20,
                        'current_price': current_price,
                        'distance_pct': ((current_price - lows_20) / lows_20) * 100,
                        'touches': int(support_touches),
                        'confidence_score': min(0.9, 0.5 + support_touches * 0.1),
                        'description': f'Price near support at ${lows_20:.2f}'
                    })
            
            # Consolidation/Triangle patterns
            if len(close_prices) >= 15:
                recent_highs = high_prices[-15:]
                recent_lows = low_prices[-15:]
                
                # Linear regression on highs and lows
                x_vals = np.arange(len(recent_highs))
                high_trend = np.polyfit(x_vals, recent_highs, 1)[0]
                low_trend = np.polyfit(x_vals, recent_lows, 1)[0]
                
                # Consolidation (horizontal movement)
                if abs(high_trend) < np.std(recent_highs) * 0.1 and abs(low_trend) < np.std(recent_lows) * 0.1:
                    patterns.append({
                        'pattern_name': 'consolidation',
                        'high_trend': high_trend,
                        'low_trend': low_trend,
                        'range_size': (np.max(recent_highs) - np.min(recent_lows)) / current_price * 100,
                        'confidence_score': 0.75,
                        'description': 'Sideways price movement - consolidation phase'
                    })
                
                # Ascending Triangle
                elif abs(high_trend) < np.std(recent_highs) * 0.1 and low_trend > 0:
                    patterns.append({
                        'pattern_name': 'ascending_triangle',
                        'pattern_type': 'continuation',
                        'bias': 'bullish',
                        'confidence_score': 0.7,
                        'description': 'Rising lows with horizontal resistance'
                    })
                
                # Descending Triangle  
                elif abs(low_trend) < np.std(recent_lows) * 0.1 and high_trend < 0:
                    patterns.append({
                        'pattern_name': 'descending_triangle',
                        'pattern_type': 'continuation',
                        'bias': 'bearish',
                        'confidence_score': 0.7,
                        'description': 'Falling highs with horizontal support'
                    })
                
        except Exception as e:
            self.logger.error(f"Error in chart pattern detection: {e}")
        
        return patterns
    
    def _detect_volume_patterns(self, symbol: str, data: pd.DataFrame = None) -> List[Dict]:
        """Detect volume-based patterns using mathematical analysis"""
        if data is None:
            data = self._get_market_data(symbol)
        
        if data.empty:
            return []
        
        patterns = []
        
        try:
            volume = data['Volume'].values
            close_prices = data['Close'].values
            
            if len(volume) >= 20:
                avg_volume_20 = np.mean(volume[-20:])
                current_volume = volume[-1]
                
                # High volume analysis
                if current_volume > avg_volume_20 * 2:
                    price_change = (close_prices[-1] - close_prices[-2]) / close_prices[-2] * 100
                    
                    patterns.append({
                        'pattern_name': 'high_volume_breakout',
                        'volume_ratio': current_volume / avg_volume_20,
                        'price_change_pct': price_change,
                        'bullish': price_change > 0,
                        'confidence_score': min(0.9, 0.5 + (current_volume / avg_volume_20) * 0.1),
                        'description': f'Volume spike ({current_volume/avg_volume_20:.1f}x average)'
                    })
                
                # Volume trend analysis
                if len(volume) >= 10:
                    volume_trend = np.polyfit(range(10), volume[-10:], 1)[0]
                    volume_strength = abs(volume_trend) / avg_volume_20
                    
                    if volume_strength > 0.1:
                        patterns.append({
                            'pattern_name': 'volume_trend',
                            'trend_direction': 'increasing' if volume_trend > 0 else 'decreasing',
                            'trend_strength': volume_strength,
                            'confidence_score': min(0.8, 0.4 + volume_strength),
                            'description': f'{"Increasing" if volume_trend > 0 else "Decreasing"} volume trend'
                        })
                
                # On-Balance Volume (OBV) analysis
                obv = self._calculate_obv(close_prices, volume)
                if len(obv) >= 10:
                    obv_trend = np.polyfit(range(min(10, len(obv))), obv[-10:], 1)[0]
                    obv_strength = abs(obv_trend) / np.std(obv[-20:]) if len(obv) >= 20 else 0
                    
                    if obv_strength > 0.1:
                        patterns.append({
                            'pattern_name': 'obv_divergence',
                            'obv_trend': 'bullish' if obv_trend > 0 else 'bearish',
                            'strength': obv_strength,
                            'confidence_score': min(0.8, 0.5 + obv_strength),
                            'description': f'OBV showing {"accumulation" if obv_trend > 0 else "distribution"}'
                        })
                
                # Volume Price Trend (VPT) analysis
                vpt = self._calculate_vpt(close_prices, volume)
                if len(vpt) >= 10:
                    vpt_trend = np.polyfit(range(min(10, len(vpt))), vpt[-10:], 1)[0]
                    
                    if abs(vpt_trend) > np.std(vpt[-20:]) * 0.1 if len(vpt) >= 20 else 0:
                        patterns.append({
                            'pattern_name': 'volume_price_trend',
                            'vpt_direction': 'positive' if vpt_trend > 0 else 'negative',
                            'strength': abs(vpt_trend),
                            'confidence_score': 0.7,
                            'description': f'Volume-price trend is {"positive" if vpt_trend > 0 else "negative"}'
                        })
                
                # Low volume warning
                if current_volume < avg_volume_20 * 0.5:
                    patterns.append({
                        'pattern_name': 'low_volume_warning',
                        'volume_ratio': current_volume / avg_volume_20,
                        'confidence_score': 0.6,
                        'description': 'Unusually low volume - reduced reliability'
                    })
                    
        except Exception as e:
            self.logger.error(f"Error in volume pattern detection: {e}")
        
        return patterns
    
    def _calculate_obv(self, prices: np.array, volumes: np.array) -> np.array:
        """Calculate On-Balance Volume manually"""
        obv = np.zeros(len(prices))
        if len(prices) == 0:
            return obv
            
        obv[0] = volumes[0]
        
        for i in range(1, len(prices)):
            if prices[i] > prices[i-1]:
                obv[i] = obv[i-1] + volumes[i]
            elif prices[i] < prices[i-1]:
                obv[i] = obv[i-1] - volumes[i]
            else:
                obv[i] = obv[i-1]
        
        return obv
    
    def _calculate_vpt(self, prices: np.array, volumes: np.array) -> np.array:
        """Calculate Volume Price Trend manually"""
        vpt = np.zeros(len(prices))
        if len(prices) <= 1:
            return vpt
            
        vpt[0] = volumes[0]
        
        for i in range(1, len(prices)):
            price_change_pct = (prices[i] - prices[i-1]) / prices[i-1] if prices[i-1] != 0 else 0
            vpt[i] = vpt[i-1] + (volumes[i] * price_change_pct)
        
        return vpt
    
    def _calculate_pattern_score(self, candlestick: List, chart: List, volume: List) -> float:
        """Calculate overall pattern strength score"""
        score = 0.0
        
        # Candlestick patterns contribute 40%
        if candlestick:
            candlestick_score = sum([p.get('confidence_score', 0) for p in candlestick]) / len(candlestick)
            score += candlestick_score * 0.4
        
        # Chart patterns contribute 35%
        if chart:
            chart_score = sum([p.get('confidence_score', 0) for p in chart]) / len(chart)
            score += chart_score * 0.35
        
        # Volume patterns contribute 25%
        if volume:
            volume_score = sum([p.get('confidence_score', 0) for p in volume]) / len(volume)
            score += volume_score * 0.25
        
        return min(score, 1.0)
    
    def _save_pattern_analysis(self, analysis_data: Dict):
        """Save pattern analysis to database with retry logic"""
        try:
            data = {
                'symbol': analysis_data['symbol'],
                'detection_date': datetime.now().isoformat(),
                'pattern_category': 'advanced_combined',
                'pattern_score': analysis_data['overall_pattern_score'],
                'candlestick_patterns_count': len(analysis_data.get('candlestick_patterns', [])),
                'chart_patterns_count': len(analysis_data.get('chart_patterns', [])),
                'volume_patterns_count': len(analysis_data.get('volume_patterns', [])),
                'pattern_metadata': json.dumps(analysis_data),
                'created_at': datetime.now().isoformat()
            }
            
            if USE_DB_UTILS:
                # Use database utilities with retry logic
                success = self.save_to_database('advanced_patterns', data)
                if success:
                    self.logger.info(f"Pattern analysis saved with retry logic for {analysis_data['symbol']}")
                else:
                    self.logger.error(f"Failed to save pattern analysis for {analysis_data['symbol']}")
            else:
                # Fallback to direct connection
                conn = sqlite3.connect(self.db_path, timeout=30)
                cursor = conn.cursor()
                
                cursor.execute('''
                    INSERT INTO advanced_patterns 
                    (symbol, detection_date, pattern_category, pattern_score, 
                     candlestick_patterns_count, chart_patterns_count, volume_patterns_count,
                     pattern_metadata, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data['symbol'],
                    data['detection_date'],
                    data['pattern_category'],
                    data['pattern_score'],
                    data['candlestick_patterns_count'],
                    data['chart_patterns_count'],
                    data['volume_patterns_count'],
                    data['pattern_metadata'],
                    data['created_at']
                ))
                
                conn.commit()
                conn.close()
                self.logger.info(f"Pattern analysis saved for {analysis_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving pattern analysis: {e}")
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        db_mode = "with retry logic" if USE_DB_UTILS else "direct connection"
        self.logger.info(f"Starting Pattern Recognition Service on port 5006 {mode}, database {db_mode}")
        self.app.run(host='0.0.0.0', port=5006, debug=False)

if __name__ == "__main__":
    service = PatternRecognitionService()
    service.run()


================================================================================
FILE: ./reporting_service.py
================================================================================

# ================================================================
# 8. reporting_service.py (Port 5009)
# ================================================================
"""
Reporting Service - Generates comprehensive trading reports and analytics
Provides insights into trading performance and pattern effectiveness
"""

import sqlite3
import pandas as pd
import logging
import json
import requests
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

class ReportingService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('ReportingService')
        
        handler = logging.FileHandler('./logs/reporting_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({"status": "healthy", "service": "reporting"})
        
        @self.app.route('/daily_summary', methods=['GET'])
        def daily_summary():
            report = self._generate_daily_summary()
            return jsonify(report)
        
        @self.app.route('/pattern_effectiveness', methods=['GET'])
        def pattern_effectiveness():
            report = self._analyze_pattern_effectiveness()
            return jsonify(report)
        
        @self.app.route('/trading_performance', methods=['GET'])
        def trading_performance():
            days = request.args.get('days', 30, type=int)
            report = self._generate_performance_report(days)
            return jsonify(report)
        
        @self.app.route('/system_health', methods=['GET'])
        def system_health():
            report = self._generate_system_health_report()
            return jsonify(report)
        
        @self.app.route('/cycle_analysis', methods=['GET'])
        def cycle_analysis():
            report = self._analyze_trading_cycles()
            return jsonify(report)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "reporting",
                "port": 5009
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _generate_daily_summary(self) -> Dict:
        """Generate daily trading summary"""
        try:
            today = datetime.now().date()
            
            conn = sqlite3.connect(self.db_path)
            
            # Trading statistics
            query = '''
                SELECT 
                    COUNT(*) as total_trades,
                    SUM(CASE WHEN profit_loss > 0 THEN 1 ELSE 0 END) as winning_trades,
                    SUM(profit_loss) as total_pnl,
                    AVG(profit_loss) as avg_pnl,
                    MAX(profit_loss) as best_trade,
                    MIN(profit_loss) as worst_trade
                FROM trades 
                WHERE DATE(created_at) = ?
            '''
            
            cursor = conn.cursor()
            cursor.execute(query, (today,))
            trade_result = cursor.fetchone()
            
            # Securities scanned today
            securities_query = '''
                SELECT COUNT(*) as securities_scanned
                FROM selected_securities
                WHERE DATE(created_at) = ?
            '''
            cursor.execute(securities_query, (today,))
            securities_result = cursor.fetchone()
            
            # Patterns analyzed today
            patterns_query = '''
                SELECT COUNT(*) as patterns_analyzed,
                       AVG(confidence_score) as avg_confidence
                FROM pattern_analysis
                WHERE DATE(created_at) = ?
            '''
            cursor.execute(patterns_query, (today,))
            patterns_result = cursor.fetchone()
            
            # Trading signals generated today
            signals_query = '''
                SELECT COUNT(*) as signals_generated,
                       SUM(CASE WHEN signal_type = 'BUY' THEN 1 ELSE 0 END) as buy_signals,
                       SUM(CASE WHEN signal_type = 'SELL' THEN 1 ELSE 0 END) as sell_signals,
                       AVG(confidence) as avg_signal_confidence
                FROM trading_signals
                WHERE DATE(created_at) = ?
            '''
            cursor.execute(signals_query, (today,))
            signals_result = cursor.fetchone()
            
            conn.close()
            
            total_trades = trade_result[0] or 0
            winning_trades = trade_result[1] or 0
            
            summary = {
                'date': str(today),
                'trading_activity': {
                    'securities_scanned': securities_result[0] or 0,
                    'patterns_analyzed': patterns_result[0] or 0,
                    'average_pattern_confidence': round(patterns_result[1] or 0, 3),
                    'signals_generated': signals_result[0] or 0,
                    'buy_signals': signals_result[1] or 0,
                    'sell_signals': signals_result[2] or 0,
                    'average_signal_confidence': round(signals_result[3] or 0, 3)
                },
                'trading_performance': {
                    'total_trades': total_trades,
                    'winning_trades': winning_trades,
                    'losing_trades': total_trades - winning_trades,
                    'win_rate': (winning_trades / total_trades * 100) if total_trades > 0 else 0,
                    'total_pnl': round(trade_result[2] or 0, 2),
                    'average_pnl': round(trade_result[3] or 0, 2),
                    'best_trade': round(trade_result[4] or 0, 2),
                    'worst_trade': round(trade_result[5] or 0, 2)
                },
                'generated_at': datetime.now().isoformat()
            }
            
            self.logger.info(f"Generated daily summary: {total_trades} trades, {summary['trading_performance']['win_rate']:.1f}% win rate")
            return summary
            
        except Exception as e:
            self.logger.error(f"Error generating daily summary: {e}")
            return {"error": str(e)}
    
    def _analyze_pattern_effectiveness(self) -> Dict:
        """Analyze which patterns are most effective for trading"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Get pattern effectiveness from trades
            query = '''
                SELECT 
                    COALESCE(pattern_used, 'unknown') as pattern_name,
                    COUNT(*) as trade_count,
                    SUM(CASE WHEN profit_loss > 0 THEN 1 ELSE 0 END) as winning_trades,
                    AVG(profit_loss) as avg_pnl,
                    SUM(profit_loss) as total_pnl,
                    AVG(confidence) as avg_confidence
                FROM trades 
                WHERE status IN ('executed', 'simulated', 'closed')
                GROUP BY COALESCE(pattern_used, 'unknown')
                HAVING COUNT(*) >= 1
                ORDER BY avg_pnl DESC
            '''
            
            cursor = conn.cursor()
            cursor.execute(query)
            results = cursor.fetchall()
            
            pattern_analysis = []
            for row in results:
                pattern_name = row[0]
                trade_count = row[1]
                winning_trades = row[2]
                avg_pnl = row[3]
                total_pnl = row[4]
                avg_confidence = row[5]
                
                pattern_analysis.append({
                    'pattern_name': pattern_name,
                    'trade_count': trade_count,
                    'winning_trades': winning_trades,
                    'losing_trades': trade_count - winning_trades,
                    'win_rate': round((winning_trades / trade_count * 100) if trade_count > 0 else 0, 1),
                    'average_pnl': round(avg_pnl, 2),
                    'total_pnl': round(total_pnl, 2),
                    'average_confidence': round(avg_confidence or 0, 3),
                    'effectiveness_score': round((avg_pnl * (winning_trades / trade_count)) if trade_count > 0 else 0, 3)
                })
            
            # Get pattern detection statistics
            pattern_stats_query = '''
                SELECT 
                    COUNT(*) as total_analyses,
                    AVG(patterns_detected) as avg_patterns_per_symbol,
                    AVG(confidence_score) as avg_analysis_confidence
                FROM pattern_analysis
                WHERE created_at >= date('now', '-30 days')
            '''
            
            cursor.execute(pattern_stats_query)
            stats_result = cursor.fetchone()
            
            conn.close()
            
            # Sort by effectiveness score
            pattern_analysis.sort(key=lambda x: x['effectiveness_score'], reverse=True)
            
            report = {
                'pattern_effectiveness': pattern_analysis,
                'best_pattern': pattern_analysis[0] if pattern_analysis else None,
                'worst_pattern': pattern_analysis[-1] if pattern_analysis else None,
                'total_patterns_analyzed': len(pattern_analysis),
                'pattern_detection_stats': {
                    'total_analyses_30d': stats_result[0] or 0,
                    'avg_patterns_per_symbol': round(stats_result[1] or 0, 1),
                    'avg_analysis_confidence': round(stats_result[2] or 0, 3)
                },
                'generated_at': datetime.now().isoformat()
            }
            
            self.logger.info(f"Pattern effectiveness analysis: {len(pattern_analysis)} patterns analyzed")
            return report
            
        except Exception as e:
            self.logger.error(f"Error analyzing pattern effectiveness: {e}")
            return {"error": str(e)}
    
    def _generate_performance_report(self, days: int = 30) -> Dict:
        """Generate trading performance report for specified period"""
        try:
            start_date = datetime.now() - timedelta(days=days)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Overall performance
            overall_query = '''
                SELECT 
                    COUNT(*) as total_trades,
                    SUM(CASE WHEN profit_loss > 0 THEN 1 ELSE 0 END) as total_winners,
                    SUM(profit_loss) as total_pnl,
                    AVG(profit_loss) as avg_pnl,
                    MAX(profit_loss) as best_trade,
                    MIN(profit_loss) as worst_trade,
                    AVG(confidence) as avg_confidence
                FROM trades 
                WHERE created_at >= ?
            '''
            
            cursor.execute(overall_query, (start_date,))
            overall_result = cursor.fetchone()
            
            # Daily performance
            daily_query = '''
                SELECT 
                    DATE(created_at) as trade_date,
                    COUNT(*) as daily_trades,
                    SUM(CASE WHEN profit_loss > 0 THEN 1 ELSE 0 END) as daily_winners,
                    SUM(profit_loss) as daily_pnl
                FROM trades 
                WHERE created_at >= ?
                GROUP BY DATE(created_at)
                ORDER BY trade_date DESC
                LIMIT 30
            '''
            
            cursor.execute(daily_query, (start_date,))
            daily_results = cursor.fetchall()
            
            # Symbol performance
            symbol_query = '''
                SELECT 
                    symbol,
                    COUNT(*) as trade_count,
                    SUM(CASE WHEN profit_loss > 0 THEN 1 ELSE 0 END) as winning_trades,
                    SUM(profit_loss) as total_pnl,
                    AVG(profit_loss) as avg_pnl
                FROM trades 
                WHERE created_at >= ?
                GROUP BY symbol
                HAVING COUNT(*) >= 2
                ORDER BY total_pnl DESC
                LIMIT 10
            '''
            
            cursor.execute(symbol_query, (start_date,))
            symbol_results = cursor.fetchall()
            
            conn.close()
            
            # Process results
            total_trades = overall_result[0] or 0
            total_winners = overall_result[1] or 0
            
            # Daily performance
            daily_performance = []
            for row in daily_results:
                daily_trades = row[1]
                daily_winners = row[2]
                
                daily_performance.append({
                    'date': row[0],
                    'trades': daily_trades,
                    'winners': daily_winners,
                    'losers': daily_trades - daily_winners,
                    'win_rate': round((daily_winners / daily_trades * 100) if daily_trades > 0 else 0, 1),
                    'pnl': round(row[3], 2)
                })
            
            # Symbol performance
            symbol_performance = []
            for row in symbol_results:
                symbol_trades = row[1]
                symbol_winners = row[2]
                
                symbol_performance.append({
                    'symbol': row[0],
                    'trade_count': symbol_trades,
                    'winning_trades': symbol_winners,
                    'win_rate': round((symbol_winners / symbol_trades * 100) if symbol_trades > 0 else 0, 1),
                    'total_pnl': round(row[3], 2),
                    'average_pnl': round(row[4], 2)
                })
            
            report = {
                'period_days': days,
                'start_date': start_date.date().isoformat(),
                'end_date': datetime.now().date().isoformat(),
                'overall_performance': {
                    'total_trades': total_trades,
                    'winning_trades': total_winners,
                    'losing_trades': total_trades - total_winners,
                    'win_rate': round((total_winners / total_trades * 100) if total_trades > 0 else 0, 1),
                    'total_pnl': round(overall_result[2] or 0, 2),
                    'average_pnl': round(overall_result[3] or 0, 2),
                    'best_trade': round(overall_result[4] or 0, 2),
                    'worst_trade': round(overall_result[5] or 0, 2),
                    'average_confidence': round(overall_result[6] or 0, 3)
                },
                'daily_performance': daily_performance,
                'top_symbols': symbol_performance,
                'generated_at': datetime.now().isoformat()
            }
            
            self.logger.info(f"Generated {days}-day performance report: {total_trades} trades")
            return report
            
        except Exception as e:
            self.logger.error(f"Error generating performance report: {e}")
            return {"error": str(e)}
    
    def _analyze_trading_cycles(self) -> Dict:
        """Analyze trading cycle performance"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            query = '''
                SELECT 
                    cycle_id,
                    status,
                    started_at,
                    completed_at,
                    securities_scanned,
                    patterns_analyzed,
                    signals_generated,
                    trades_executed
                FROM trading_cycles
                ORDER BY started_at DESC
                LIMIT 20
            '''
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            cycles = []
            successful_cycles = 0
            total_securities = 0
            total_trades = 0
            
            for row in results:
                cycle = {
                    'cycle_id': row[0],
                    'status': row[1],
                    'started_at': row[2],
                    'completed_at': row[3],
                    'securities_scanned': row[4] or 0,
                    'patterns_analyzed': row[5] or 0,
                    'signals_generated': row[6] or 0,
                    'trades_executed': row[7] or 0,
                    'conversion_rate': round((row[7] / row[4] * 100) if row[4] and row[4] > 0 else 0, 1)
                }
                cycles.append(cycle)
                
                if row[1] == 'completed':
                    successful_cycles += 1
                    total_securities += row[4] or 0
                    total_trades += row[7] or 0
            
            conn.close()
            
            report = {
                'recent_cycles': cycles,
                'summary': {
                    'total_cycles': len(cycles),
                    'successful_cycles': successful_cycles,
                    'success_rate': round((successful_cycles / len(cycles) * 100) if cycles else 0, 1),
                    'avg_securities_per_cycle': round(total_securities / successful_cycles if successful_cycles > 0 else 0, 1),
                    'avg_trades_per_cycle': round(total_trades / successful_cycles if successful_cycles > 0 else 0, 1),
                    'overall_conversion_rate': round((total_trades / total_securities * 100) if total_securities > 0 else 0, 1)
                },
                'generated_at': datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"Error analyzing trading cycles: {e}")
            return {"error": str(e)}
    
    def _generate_system_health_report(self) -> Dict:
        """Generate system health and service status report"""
        try:
            # Check database connectivity
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Count records in key tables
            table_counts = {}
            tables = ['selected_securities', 'pattern_analysis', 'trading_signals', 'trades', 'news_sentiment']
            
            for table in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    table_counts[table] = cursor.fetchone()[0]
                except:
                    table_counts[table] = 0
            
            # Get recent activity
            cursor.execute('''
                SELECT COUNT(*) FROM selected_securities 
                WHERE created_at >= datetime('now', '-24 hours')
            ''')
            recent_securities = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT COUNT(*) FROM trades 
                WHERE created_at >= datetime('now', '-24 hours')
            ''')
            recent_trades = cursor.fetchone()[0]
            
            conn.close()
            
            # Try to get service status from coordination service
            service_status = {}
            try:
                response = requests.get(f"{self.coordination_service_url}/service_status", timeout=5)
                if response.status_code == 200:
                    service_status = response.json()
            except:
                service_status = {"error": "Could not reach coordination service"}
            
            report = {
                'database_health': {
                    'status': 'connected',
                    'table_record_counts': table_counts,
                    'recent_activity_24h': {
                        'securities_scanned': recent_securities,
                        'trades_executed': recent_trades
                    }
                },
                'service_registry': service_status,
                'system_status': 'operational',
                'last_check': datetime.now().isoformat()
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"Error generating system health report: {e}")
            return {"error": str(e)}
    
    def run(self):
        self.logger.info("Starting Reporting Service on port 5009")
        self.app.run(host='0.0.0.0', port=5009, debug=False)

if __name__ == "__main__":
    service = ReportingService()
    service.run()

================================================================================
FILE: ./requirements.txt
================================================================================

# Trading System Requirements
# Version: 2.0.0
# Date: 2025-06-22
# Description: Comprehensive requirements for all trading system services running in GitHub Codespace
 
# Core dependencies
flask==3.0.0
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
scikit-learn==1.3.2
yfinance==0.2.33
psutil==5.9.6
python-dateutil==2.8.2
pytz==2023.3
beautifulsoup4==4.12.2
lxml==4.9.3

# Alpaca Trading (install separately if needed)
# alpaca-py==0.21.1

# Development tools
setuptools>=69.0.0
wheel>=0.42.0

================================================================================
FILE: ./security_scanner.py
================================================================================

# ================================================================
# 2. security_scanner.py (Port 5001) - FIXED VERSION
# ================================================================

"""
Name of Service: TRADING SYSTEM SECURITY SCANNER - FIXED VERSION
Version: 1.0.4
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance fallback
v1.0.3 (2025-06-17) - Added error handling for yfinance import issues
v1.0.2 (2025-06-15) - Enhanced integration with news service
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Security Scanner Service - Scans market for securities meeting criteria
Integrates with news service for sentiment analysis
Fixed to handle yfinance websockets dependency issues gracefully
"""

import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

class SecurityScannerService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        self.news_service_url = "http://localhost:5008"
        
        # Trading criteria
        self.criteria = {
            "min_price": 2.0,
            "max_price": 20.0,
            "min_volume_ratio": 5.0,
            "min_price_change_pct": 10.0
        }
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('SecurityScannerService')
        
        handler = logging.FileHandler('./logs/security_scanner.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "security_scanner",
                "yfinance_available": YFINANCE_AVAILABLE,
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated"
            })
        
        @self.app.route('/scan_securities', methods=['GET'])
        def scan_securities_endpoint():
            securities = self._scan_securities()
            return jsonify(securities)
        
        @self.app.route('/criteria', methods=['GET', 'POST'])
        def manage_criteria():
            if request.method == 'GET':
                return jsonify(self.criteria)
            else:
                self.criteria.update(request.json)
                self.logger.info(f"Updated criteria: {self.criteria}")
                return jsonify({"message": "Criteria updated"})
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "security_scanner",
                "port": 5001
            }
            response = requests.post(f"{self.coordination_service_url}/register_service", 
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _scan_securities(self) -> List[Dict]:
        """Main security scanning logic"""
        self.logger.info("Starting securities scan")
        
        selected_securities = []
        watchlist = self._get_watchlist()
        
        for symbol in watchlist:
            try:
                if YFINANCE_AVAILABLE:
                    security_data = self._analyze_security_yfinance(symbol)
                else:
                    security_data = self._analyze_security_simulated(symbol)
                
                if security_data and self._meets_criteria(security_data):
                    # Get news sentiment
                    news_data = self._get_news_sentiment(symbol)
                    security_data.update(news_data)
                    
                    # Save to database
                    self._save_selected_security(security_data)
                    
                    selected_securities.append(security_data)
                    self.logger.info(f"Selected {symbol}: {security_data['selection_reason']}")
                
            except Exception as e:
                self.logger.error(f"Error analyzing {symbol}: {e}")
        
        self.logger.info(f"Scan completed: {len(selected_securities)} securities selected")
        return selected_securities
    
    def _get_watchlist(self) -> List[str]:
        """Get list of symbols to analyze"""
        return ['AAPL', 'TSLA', 'AMD', 'NVDA', 'MSFT', 'GOOGL', 'META', 'AMZN', 
                'SPY', 'QQQ', 'PLTR', 'GME', 'AMC', 'BB', 'NOK', 'SOXL', 'TQQQ']
    
    def _analyze_security_yfinance(self, symbol: str) -> Optional[Dict]:
        """Analyze individual security using yfinance"""
        try:
            ticker = yf.Ticker(symbol)
            
            # Use simpler method that doesn't require websockets
            try:
                hist = ticker.history(period="5d")
                info = ticker.info
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using fallback: {e}")
                return self._analyze_security_simulated(symbol)
            
            if len(hist) < 2:
                return None
            
            current_price = hist['Close'].iloc[-1]
            prev_price = hist['Close'].iloc[-2]
            current_volume = hist['Volume'].iloc[-1]
            avg_volume = hist['Volume'].mean()
            
            price_change_pct = ((current_price - prev_price) / prev_price) * 100
            volume_ratio = current_volume / avg_volume if avg_volume > 0 else 0
            
            return {
                'symbol': symbol,
                'price': float(current_price),
                'volume': int(current_volume),
                'volume_ratio': float(volume_ratio),
                'price_change_pct': float(price_change_pct),
                'market_cap': info.get('marketCap', 0),
                'selection_reason': f"Price change: {price_change_pct:.2f}%, Volume ratio: {volume_ratio:.2f}x",
                'data_source': 'yfinance'
            }
            
        except Exception as e:
            self.logger.error(f"Error analyzing {symbol} with yfinance: {e}")
            return self._analyze_security_simulated(symbol)
    
    def _analyze_security_simulated(self, symbol: str) -> Optional[Dict]:
        """Analyze security using simulated data when yfinance is unavailable"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data per symbol
        random.seed(hash(symbol) + int(time.time() / 86400))  # Changes daily
        
        # Generate realistic simulated data
        base_price = random.uniform(5, 500)
        price_change_pct = random.uniform(-20, 20)
        volume_ratio = random.uniform(0.5, 8.0)
        
        current_price = base_price * (1 + price_change_pct / 100)
        
        return {
            'symbol': symbol,
            'price': round(current_price, 2),
            'volume': random.randint(100000, 5000000),
            'volume_ratio': round(volume_ratio, 2),
            'price_change_pct': round(price_change_pct, 2),
            'market_cap': random.randint(1000000000, 100000000000),
            'selection_reason': f"Simulated: Price change: {price_change_pct:.2f}%, Volume ratio: {volume_ratio:.2f}x",
            'data_source': 'simulated'
        }
    
    def _meets_criteria(self, security: Dict) -> bool:
        """Check if security meets trading criteria"""
        criteria_met = (
            self.criteria["min_price"] <= security["price"] <= self.criteria["max_price"] and
            security["volume_ratio"] >= self.criteria["min_volume_ratio"] and
            abs(security["price_change_pct"]) >= self.criteria["min_price_change_pct"]
        )
        
        return criteria_met
    
    def _get_news_sentiment(self, symbol: str) -> Dict:
        """Get news sentiment from news service"""
        try:
            response = requests.get(f"{self.news_service_url}/news_sentiment/{symbol}", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.warning(f"Could not get news sentiment for {symbol}")
                return {"sentiment_score": 0.0, "sentiment_label": "neutral"}
        except Exception as e:
            self.logger.warning(f"Error getting news sentiment for {symbol}: {e}")
            return {"sentiment_score": 0.0, "sentiment_label": "neutral"}
    
    def _save_selected_security(self, security_data: Dict):
        """Save selected security to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO selected_securities 
                (symbol, selection_date, selection_criteria, market_cap, average_volume, 
                 sector, industry, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                security_data['symbol'],
                datetime.now().date().isoformat(),
                security_data['selection_reason'],
                security_data.get('market_cap', 0),
                security_data.get('volume', 0),
                'Technology',  # Default sector
                'Software',    # Default industry
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved selected security: {security_data['symbol']}")
            
        except Exception as e:
            self.logger.error(f"Error saving security {security_data['symbol']}: {e}")
    
    def run(self):
        mode = "with yfinance" if YFINANCE_AVAILABLE else "in simulation mode"
        self.logger.info(f"Starting Security Scanner Service on port 5001 {mode}")
        self.app.run(host='0.0.0.0', port=5001, debug=False)

if __name__ == "__main__":
    service = SecurityScannerService()
    service.run()

================================================================================
FILE: ./setup_codespace.py
================================================================================

#!/usr/bin/env python3
"""
Setup script for GitHub Codespace
Configures the Trading Application environment
"""
import os
import sys
import subprocess
import sqlite3
from pathlib import Path

def create_directories():
    """Create necessary directories"""
    directories = [
        'logs',
        'backups',
        'project_documentation',
        'updates',
        '.update_state'
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"‚úì Created {directory}/")

def update_service_paths():
    """Update paths in service files from Colab to Codespace format"""
    services = [
        'coordination_service.py',
        'security_scanner.py',
        'pattern_analysis.py',
        'technical_analysis.py',
        'paper_trading.py',
        'pattern_recognition_service.py',
        'news_service.py',
        'reporting_service.py',
        'web_dashboard.py',
        'hybrid_manager.py',
        'database_migration.py',
        'diagnostic_toolkit.py'
    ]
    
    replacements = [
        ('./trading_system.db', './trading_system.db'),
        ('./logs/', './logs/'),
        ('./backups/', './backups/'),
        ('/conten./', './'),
        ('./', './')
    ]
    
    for service in services:
        if Path(service).exists():
            with open(service, 'r') as f:
                content = f.read()
            
            original_content = content
            for old_path, new_path in replacements:
                content = content.replace(old_path, new_path)
            
            # Remove Colab-specific imports
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if 'from google.colab import' not in line and 'import google.colab' not in line:
                    filtered_lines.append(line)
                else:
                    filtered_lines.append(f"# {line}  # Removed for Codespace")
            
            content = '\n'.join(filtered_lines)
            
            if content != original_content:
                with open(service, 'w') as f:
                    f.write(content)
                print(f"‚úì Updated paths in {service}")

def initialize_database():
    """Initialize the database"""
    if Path('database_migration.py').exists():
        print("\nüîß Initializing database...")
        result = subprocess.run([sys.executable, 'database_migration.py'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úì Database initialized successfully")
        else:
            print(f"‚ö†Ô∏è  Database initialization warning: {result.stderr}")
    else:
        print("‚ö†Ô∏è  database_migration.py not found")

def create_requirements_file():
    """Create requirements.txt if it doesn't exist"""
    if not Path('requirements.txt').exists():
        requirements = """flask==3.0.0
requests==2.31.0
pandas==2.1.4
numpy==1.26.2
scikit-learn==1.3.2
yfinance==0.2.33
alpaca-py==0.21.1
psutil==5.9.6
python-dateutil==2.8.2
pytz==2023.3
beautifulsoup4==4.12.2
"""
        with open('requirements.txt', 'w') as f:
            f.write(requirements)
        print("‚úì Created requirements.txt")

def create_startup_script():
    """Create a quick startup script"""
    startup_content = """#!/bin/bash
# Quick startup script for Trading Application

echo "üöÄ Starting Trading Application Services..."
python hybrid_manager.py
"""
    
    with open('start_trading.sh', 'w') as f:
        f.write(startup_content)
    
    os.chmod('start_trading.sh', 0o755)
    print("‚úì Created start_trading.sh")

def main():
    print("üîß Setting up Trading Application for GitHub Codespaces\n")
    
    # Check if we're in the right directory
    if not any(Path(f).exists() for f in ['coordination_service.py', 'hybrid_manager.py']):
        print("‚ö†Ô∏è  Warning: Trading application files not found in current directory")
        print("   Make sure you're in the root of your Trading_Application repository")
        response = input("\nContinue anyway? (y/n): ")
        if response.lower() != 'y':
            return
    
    create_directories()
    create_requirements_file()
    update_service_paths()
    initialize_database()
    create_startup_script()
    
    print("\n‚úÖ Codespace setup complete!")
    print("\nüìã Next steps:")
    print("1. Install requirements: pip install -r requirements.txt")
    print("2. Run diagnostic check: python diagnostic_toolkit.py --report")
    print("3. Start services: python hybrid_manager.py")
    print("   OR use: ./start_trading.sh")
    print("\nüí° Tip: The web dashboard will be available at port 8080")

if __name__ == "__main__":
    main()

================================================================================
FILE: ./technical_analysis.py
================================================================================

# ================================================================
# 4. technical_analysis.py (Port 5003) - CORRECTED VERSION
# ================================================================
"""
Name of Service: TRADING SYSTEM TECHNICAL ANALYSIS - CORRECTED VERSION
Version: 1.0.4
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.4 (2025-06-17) - Fixed websockets dependency issue with yfinance graceful import
v1.0.3 (2025-06-15) - Removed TA-Lib dependency, using only manual technical indicator calculations
v1.0.2 (2025-06-15) - Fixed version with TA-Lib fallback  
v1.0.1 (2025-06-15) - Initial version
v1.0.0 (2025-06-15) - Original implementation

Technical Analysis Service - Generates trading signals using manual technical indicator calculations
"""

import numpy as np
import pandas as pd
import requests
import logging
import sqlite3
import json
from datetime import datetime
from flask import Flask, request, jsonify
from typing import Dict, List, Optional

# Handle yfinance import with fallback for websockets issues
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
    print("‚úÖ yfinance imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è yfinance import failed: {e}")
    YFINANCE_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è yfinance import error: {e}")
    YFINANCE_AVAILABLE = False

# Try to import ML libraries
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
    from sklearn.model_selection import train_test_split
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logging.warning("Scikit-learn not available, using rule-based signals only")

class TechnicalAnalysisService:
    def __init__(self, db_path='./trading_system.db'):
        self.app = Flask(__name__)
        self.db_path = db_path
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        # Initialize ML models if available
        self.ml_model = None
        self.confidence_model = None
        if ML_AVAILABLE:
            self._init_ml_models()
        
        self._setup_routes()
        self._register_with_coordination()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('TechnicalAnalysisService')
        
        handler = logging.FileHandler('./logs/technical_analysis_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _init_ml_models(self):
        """Initialize ML models for signal generation"""
        try:
            # Primary model: Random Forest for signal classification
            self.ml_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                random_state=42,
                class_weight='balanced'
            )
            
            # Secondary model: Gradient Boosting for confidence scoring
            self.confidence_model = GradientBoostingRegressor(
                n_estimators=50,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
            
            self.logger.info("ML models initialized")
            
        except Exception as e:
            self.logger.error(f"Error initializing ML models: {e}")
            self.ml_model = None
            self.confidence_model = None
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                "status": "healthy", 
                "service": "technical_analysis",
                "ml_available": ML_AVAILABLE,
                "yfinance_available": YFINANCE_AVAILABLE,
                "implementation": "manual_indicators",
                "data_source": "yfinance" if YFINANCE_AVAILABLE else "simulated"
            })
        
        @self.app.route('/generate_signals', methods=['POST'])
        def generate_signals_endpoint():
            securities = request.json.get('securities', [])
            signals = self._generate_signals(securities)
            return jsonify(signals)
        
        @self.app.route('/analyze/<symbol>', methods=['GET'])
        def analyze_single_symbol(symbol):
            # Single symbol analysis
            analysis = self._analyze_single_security({'symbol': symbol})
            return jsonify(analysis)
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "technical_analysis",
                "port": 5003
            }
            response = requests.post(f"{self.coordination_service_url}/register_service",
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _generate_signals(self, securities_with_patterns: List[Dict]) -> List[Dict]:
        """Generate trading signals for securities with pattern analysis"""
        self.logger.info(f"Generating trading signals for {len(securities_with_patterns)} securities")
        
        trading_signals = []
        
        for security in securities_with_patterns:
            try:
                signal_data = self._analyze_single_security(security)
                
                if signal_data and signal_data.get('signal') in ['BUY', 'SELL']:
                    # Save to database
                    self._save_trading_signal(security['symbol'], signal_data)
                    trading_signals.append(signal_data)
                    
                    self.logger.info(f"Generated {signal_data['signal']} signal for {security['symbol']}")
                
            except Exception as e:
                self.logger.error(f"Error generating signal for {security.get('symbol', 'unknown')}: {e}")
        
        self.logger.info(f"Signal generation completed: {len(trading_signals)} signals generated")
        return trading_signals
    
    def _analyze_single_security(self, security: Dict) -> Optional[Dict]:
        """Analyze single security and generate signal"""
        symbol = security['symbol']
        
        try:
            # Get market data
            market_data = self._get_market_data(symbol)
            if market_data is None:
                return None
            
            # Calculate technical indicators using manual methods
            indicators = self._calculate_indicators_manual(market_data)
            
            # Get patterns from security data
            patterns = security.get('patterns', [])
            
            # Generate signal
            signal_data = self._generate_rule_based_signal(symbol, indicators, patterns)
            
            return signal_data
            
        except Exception as e:
            self.logger.error(f"Error analyzing {symbol}: {e}")
            return None
    
    def _get_market_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """Get market data for analysis"""
        if YFINANCE_AVAILABLE:
            try:
                ticker = yf.Ticker(symbol)
                hist = ticker.history(period="30d")
                
                if len(hist) < 20:
                    return self._generate_simulated_data(symbol)
                
                return hist
                
            except Exception as e:
                self.logger.warning(f"yfinance error for {symbol}, using simulated data: {e}")
                return self._generate_simulated_data(symbol)
        else:
            return self._generate_simulated_data(symbol)
    
    def _generate_simulated_data(self, symbol: str) -> pd.DataFrame:
        """Generate simulated OHLCV data for technical analysis"""
        import random
        import time
        
        # Use symbol hash for consistent "random" data
        random.seed(hash(symbol) + int(time.time() / 86400))
        
        # Generate 30 days of simulated data
        dates = pd.date_range(end=datetime.now(), periods=30, freq='D')
        
        # Start with a base price
        base_price = random.uniform(10, 200)
        prices = []
        volumes = []
        
        current_price = base_price
        for i in range(30):
            # Random daily change with some trend
            trend_factor = 0.001 if i > 15 else -0.001  # Slight trend
            change_pct = random.uniform(-0.05, 0.05) + trend_factor
            current_price *= (1 + change_pct)
            
            # Generate OHLC
            open_price = current_price * random.uniform(0.98, 1.02)
            close_price = current_price
            high_price = max(open_price, close_price) * random.uniform(1.0, 1.05)
            low_price = min(open_price, close_price) * random.uniform(0.95, 1.0)
            volume = random.randint(100000, 1000000)
            
            prices.append([open_price, high_price, low_price, close_price])
            volumes.append(volume)
        
        # Create DataFrame
        data = pd.DataFrame(prices, columns=['Open', 'High', 'Low', 'Close'], index=dates)
        data['Volume'] = volumes
        
        return data
    
    def _calculate_indicators_manual(self, data: pd.DataFrame) -> Dict:
        """Calculate technical indicators manually using mathematical formulas"""
        indicators = {}
        
        try:
            close_prices = data['Close'].values
            high_prices = data['High'].values
            low_prices = data['Low'].values
            volume = data['Volume'].values
            
            # Current price and volume
            indicators['current_price'] = float(close_prices[-1])
            indicators['current_volume'] = float(volume[-1])
            
            # RSI Calculation (14-period)
            def calculate_rsi(prices, period=14):
                if len(prices) < period + 1:
                    return 50.0  # Neutral RSI if insufficient data
                
                deltas = np.diff(prices)
                gains = np.where(deltas > 0, deltas, 0)
                losses = np.where(deltas < 0, -deltas, 0)
                
                # Initial average gain and loss
                avg_gain = np.mean(gains[:period])
                avg_loss = np.mean(losses[:period])
                
                # Calculate RS and RSI for each subsequent period
                for i in range(period, len(deltas)):
                    avg_gain = (avg_gain * (period - 1) + gains[i]) / period
                    avg_loss = (avg_loss * (period - 1) + losses[i]) / period
                
                if avg_loss == 0:
                    return 100.0
                
                rs = avg_gain / avg_loss
                rsi = 100 - (100 / (1 + rs))
                return rsi
            
            indicators['rsi'] = calculate_rsi(close_prices)
            
            # Simple Moving Averages
            indicators['sma_20'] = float(np.mean(close_prices[-20:]))
            indicators['sma_50'] = float(np.mean(close_prices[-50:])) if len(close_prices) >= 50 else indicators['sma_20']
            
            # Exponential Moving Average (12-period)
            def calculate_ema(prices, period):
                if len(prices) < period:
                    return np.mean(prices)
                
                multiplier = 2 / (period + 1)
                ema = np.mean(prices[:period])  # Start with SMA
                
                for price in prices[period:]:
                    ema = (price * multiplier) + (ema * (1 - multiplier))
                
                return ema
            
            indicators['ema_12'] = calculate_ema(close_prices, 12)
            indicators['ema_26'] = calculate_ema(close_prices, 26)
            
            # MACD Calculation
            macd_line = indicators['ema_12'] - indicators['ema_26']
            signal_line = calculate_ema([macd_line], 9)  # 9-period EMA of MACD
            macd_histogram = macd_line - signal_line
            
            indicators['macd'] = {
                'macd': macd_line,
                'signal': signal_line,
                'histogram': macd_histogram
            }
            
            # Bollinger Bands (20-period, 2 standard deviations)
            sma_20 = indicators['sma_20']
            std_20 = np.std(close_prices[-20:])
            bb_upper = sma_20 + (2 * std_20)
            bb_lower = sma_20 - (2 * std_20)
            bb_position = (close_prices[-1] - bb_lower) / (bb_upper - bb_lower) if bb_upper != bb_lower else 0.5
            
            indicators['bollinger'] = {
                'upper': bb_upper,
                'middle': sma_20,
                'lower': bb_lower,
                'position': bb_position  # 0 = at lower band, 1 = at upper band
            }
            
            # Stochastic Oscillator (14-period)
            if len(close_prices) >= 14:
                low_14 = np.min(low_prices[-14:])
                high_14 = np.max(high_prices[-14:])
                k_percent = ((close_prices[-1] - low_14) / (high_14 - low_14) * 100) if high_14 != low_14 else 50
                
                # Calculate %D (3-period SMA of %K) - simplified to current %K
                indicators['stochastic'] = {
                    'k': k_percent,
                    'd': k_percent  # Simplified - normally this would be a moving average of k
                }
            else:
                indicators['stochastic'] = {'k': 50, 'd': 50}
            
            # Volume analysis
            indicators['volume_sma'] = float(np.mean(volume[-20:]))
            volume_ratio = indicators['current_volume'] / indicators['volume_sma'] if indicators['volume_sma'] > 0 else 1.0
            indicators['volume_ratio'] = volume_ratio
            
            # Price momentum (5-day rate of change)
            if len(close_prices) >= 5:
                momentum = ((close_prices[-1] - close_prices[-5]) / close_prices[-5]) * 100
                indicators['momentum_5d'] = momentum
            else:
                indicators['momentum_5d'] = 0.0
            
            # Volatility (20-day standard deviation)
            volatility = np.std(close_prices[-20:]) / np.mean(close_prices[-20:]) * 100
            indicators['volatility'] = volatility
            
        except Exception as e:
            self.logger.error(f"Error calculating manual indicators: {e}")
        
        return indicators
    
    def _generate_rule_based_signal(self, symbol: str, indicators: Dict, patterns: List[Dict]) -> Dict:
        """Generate trading signal using rule-based logic"""
        signal_score = 0
        reasons = []
        
        # RSI signals (30/70 oversold/overbought levels)
        rsi = indicators.get('rsi', 50)
        if rsi < 30:
            signal_score += 2
            reasons.append(f"RSI oversold ({rsi:.1f})")
        elif rsi > 70:
            signal_score -= 2
            reasons.append(f"RSI overbought ({rsi:.1f})")
        elif rsi < 40:
            signal_score += 1
            reasons.append(f"RSI bullish ({rsi:.1f})")
        elif rsi > 60:
            signal_score -= 1
            reasons.append(f"RSI bearish ({rsi:.1f})")
        
        # MACD signals
        macd_hist = indicators.get('macd', {}).get('histogram', 0)
        if macd_hist > 0:
            signal_score += 1
            reasons.append("MACD bullish crossover")
        elif macd_hist < 0:
            signal_score -= 1
            reasons.append("MACD bearish crossover")
        
        # Moving average signals
        current_price = indicators.get('current_price', 0)
        sma_20 = indicators.get('sma_20', current_price)
        sma_50 = indicators.get('sma_50', current_price)
        ema_12 = indicators.get('ema_12', current_price)
        
        # Price vs SMA signals
        if current_price > sma_20:
            signal_score += 1
            reasons.append("Price above SMA20")
        else:
            signal_score -= 1
            reasons.append("Price below SMA20")
        
        # EMA vs SMA crossover
        if ema_12 > sma_20:
            signal_score += 1
            reasons.append("EMA12 above SMA20")
        
        # SMA crossover
        if sma_20 > sma_50:
            signal_score += 1
            reasons.append("SMA20 above SMA50")
        elif sma_20 < sma_50:
            signal_score -= 1
            reasons.append("SMA20 below SMA50")
        
        # Bollinger Bands signals
        bb_position = indicators.get('bollinger', {}).get('position', 0.5)
        if bb_position < 0.2:
            signal_score += 1
            reasons.append("Near lower Bollinger Band")
        elif bb_position > 0.8:
            signal_score -= 1
            reasons.append("Near upper Bollinger Band")
        
        # Stochastic signals
        stoch_k = indicators.get('stochastic', {}).get('k', 50)
        if stoch_k < 20:
            signal_score += 1
            reasons.append(f"Stochastic oversold ({stoch_k:.1f})")
        elif stoch_k > 80:
            signal_score -= 1
            reasons.append(f"Stochastic overbought ({stoch_k:.1f})")
        
        # Momentum signals
        momentum = indicators.get('momentum_5d', 0)
        if momentum > 5:
            signal_score += 1
            reasons.append(f"Strong positive momentum ({momentum:.1f}%)")
        elif momentum < -5:
            signal_score -= 1
            reasons.append(f"Strong negative momentum ({momentum:.1f}%)")
        
        # Pattern signals
        bullish_patterns = len([p for p in patterns if p.get('bullish', False)])
        bearish_patterns = len([p for p in patterns if p.get('bullish') == False])
        
        signal_score += bullish_patterns
        signal_score -= bearish_patterns
        
        if bullish_patterns > 0:
            reasons.append(f"{bullish_patterns} bullish pattern(s)")
        if bearish_patterns > 0:
            reasons.append(f"{bearish_patterns} bearish pattern(s)")
        
        # Volume confirmation
        volume_ratio = indicators.get('volume_ratio', 1.0)
        if volume_ratio > 1.5:
            reasons.append(f"High volume confirmation ({volume_ratio:.1f}x)")
            signal_score = int(signal_score * 1.2)  # Amplify signal on high volume
        elif volume_ratio < 0.5:
            reasons.append(f"Low volume warning ({volume_ratio:.1f}x)")
            signal_score = int(signal_score * 0.8)  # Reduce signal on low volume
        
        # Volatility adjustment
        volatility = indicators.get('volatility', 0)
        if volatility > 5:  # High volatility
            signal_score = int(signal_score * 0.9)  # Slightly reduce confidence
            reasons.append(f"High volatility ({volatility:.1f}%)")
        
        # Determine final signal
        if signal_score >= 4:
            signal = 'BUY'
            confidence = min(0.9, 0.5 + (signal_score * 0.08))
        elif signal_score <= -4:
            signal = 'SELL'
            confidence = min(0.9, 0.5 + (abs(signal_score) * 0.08))
        else:
            signal = 'HOLD'
            confidence = 0.3 + (abs(signal_score) * 0.05)
        
        # Calculate position size based on confidence and volatility
        base_quantity = 100
        volatility_factor = max(0.5, 1 - (volatility / 20))  # Reduce size for high volatility
        quantity = int(base_quantity * confidence * volatility_factor)
        
        return {
            'symbol': symbol,
            'signal': signal,
            'confidence': round(confidence, 3),
            'current_price': current_price,
            'quantity': quantity,
            'reason': '; '.join(reasons) if reasons else 'No clear signal',
            'signal_score': signal_score,
            'indicators_used': len([k for k in indicators.keys() if not k.startswith('current')]),
            'patterns_analyzed': len(patterns),
            'rsi': rsi,
            'macd_histogram': macd_hist,
            'bb_position': bb_position,
            'volume_ratio': volume_ratio,
            'volatility': volatility,
            'timestamp': datetime.now().isoformat(),
            'implementation': 'manual_calculations',
            'data_source': 'yfinance' if YFINANCE_AVAILABLE else 'simulated'
        }
    
    def _save_trading_signal(self, symbol: str, signal_data: Dict):
        """Save trading signal to database"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO trading_signals 
                (symbol, signal_type, signal_strength, ml_confidence, entry_price, 
                 technical_indicators, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                symbol,
                signal_data['signal'],
                signal_data['confidence'],
                signal_data['confidence'],  # Using same confidence for both
                signal_data.get('current_price', 0),
                json.dumps(signal_data),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Saved trading signal for {symbol}: {signal_data['signal']}")
            
        except Exception as e:
            self.logger.error(f"Error saving trading signal for {symbol}: {e}")
    
    def run(self):
        mode = f"ML: {ML_AVAILABLE}, yfinance: {YFINANCE_AVAILABLE}"
        self.logger.info(f"Starting Technical Analysis Service on port 5003 ({mode})")
        self.app.run(host='0.0.0.0', port=5003, debug=False)

if __name__ == "__main__":
    service = TechnicalAnalysisService()
    service.run()

================================================================================
FILE: ./trading_scheduler.py
================================================================================

"""
Name of Service: TRADING SYSTEM PHASE 1 - TRADING SCHEDULER
Version: 1.0.0
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.0 (2025-06-19) - Initial release with automated trading schedule management

Trading Scheduler Service - Manages automated trading cycles based on configured schedule
"""

import os
import requests
import logging
import threading
import time
from datetime import datetime, timedelta
from flask import Flask, jsonify
import pytz

class TradingSchedulerService:
    def __init__(self, port=5011):
        self.app = Flask(__name__)
        self.port = port
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        # Service state
        self.is_running = True
        self.scheduler_thread = None
        
        # Setup routes and start scheduler
        self._setup_routes()
        self._register_with_coordination()
        self._start_scheduler()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('TradingSchedulerService')
        
        handler = logging.FileHandler('./logs/trading_scheduler_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "scheduler",
                "port": self.port,
                "endpoints": ["/health", "/status"]
            }
            response = requests.post(f"{self.coordination_service_url}/register_service", 
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _start_scheduler(self):
        """Start the scheduler thread"""
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()
        self.logger.info("Trading scheduler started")
    
    def _scheduler_loop(self):
        """Main scheduler loop that coordinates with coordination service"""
        while self.is_running:
            try:
                # Get schedule configuration from coordination service
                response = requests.get(f"{self.coordination_service_url}/schedule/status", timeout=5)
                
                if response.status_code == 200:
                    schedule_status = response.json()
                    
                    # The coordination service handles the actual scheduling logic
                    # This service just monitors and logs
                    if schedule_status.get('enabled'):
                        next_run = schedule_status.get('next_run')
                        if next_run:
                            self.logger.info(f"Trading schedule active. Next run: {next_run}")
                    else:
                        self.logger.debug("Trading schedule is disabled")
                
                # Check every minute
                time.sleep(60)
                
            except Exception as e:
                self.logger.error(f"Error in scheduler loop: {e}")
                time.sleep(60)
    
    def _setup_routes(self):
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({"status": "healthy", "service": "scheduler"})
        
        @self.app.route('/status', methods=['GET'])
        def status():
            """Get scheduler service status"""
            try:
                # Get schedule info from coordination service
                response = requests.get(f"{self.coordination_service_url}/schedule/status", timeout=5)
                
                if response.status_code == 200:
                    schedule_info = response.json()
                    return jsonify({
                        "service": "Trading Scheduler",
                        "running": self.is_running,
                        "schedule_enabled": schedule_info.get('enabled', False),
                        "next_run": schedule_info.get('next_run'),
                        "last_run": schedule_info.get('last_run'),
                        "message": "Scheduler service is monitoring trading schedule"
                    })
                
            except Exception as e:
                self.logger.error(f"Error getting status: {e}")
            
            return jsonify({
                "service": "Trading Scheduler",
                "running": self.is_running,
                "message": "Unable to get schedule information"
            })
    
    def run(self):
        """Start the scheduler service"""
        self.logger.info(f"Starting Trading Scheduler Service on port {self.port}")
        
        # Run in production mode
        from waitress import serve
        serve(self.app, host='0.0.0.0', port=self.port, threads=2)

if __name__ == "__main__":
    scheduler = TradingSchedulerService()
    scheduler.run()


================================================================================
FILE: ./trading_system_automated_update_v201.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
Version: 2.0.1
Last Updated: 2025-01-11

REVISION HISTORY:
- v2.0.1 (2025-01-11) - Integrated with google_drive_service_v101.py
- v2.0.0 (2025-06-19) - Full compliance with Project Methodology v3.0.2
  - Added implementation plan reading functionality
  - Implemented proper Change Diary naming with Implementation ID
  - Added What's Next Task List after each phase
  - Implemented all required command line options
  - Added Implementation ID tracking throughout lifecycle
- v1.0.0 (2025-06-19) - Initial automated update process

PURPOSE:
Automated update process for Trading System with full Google Drive integration.
Executes implementation plans following the 6-phase methodology.
"""

import os
import sys
import json
import shutil
import logging
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

# Import the Google Drive service
from google_drive_service_v101 import get_drive_service

@dataclass
class ImplementationPlan:
    """Represents an approved implementation plan"""
    implementation_id: str
    plan_name: str
    date_created: str
    phases: List[Dict]
    files_to_update: List[Dict]
    risk_level: str = "MEDIUM"
    rollback_strategy: Dict = field(default_factory=dict)
    raw_content: str = ""
    
    @classmethod
    def from_drive_content(cls, plan_name: str, content: str) -> 'ImplementationPlan':
        """Load implementation plan from Google Drive content"""
        # Extract Implementation ID from filename
        # Format: "Implementation Plan - [ID] - [DATE].md"
        parts = plan_name.replace("Implementation Plan - ", "").replace(".md", "").split(" - ")
        implementation_id = parts[0] if parts else "UNKNOWN"
        
        # Parse content to extract files to update
        files_to_update = []
        lines = content.split('\n')
        in_files_section = False
        
        for line in lines:
            if 'Files to Update' in line or 'Files to Deliver' in line:
                in_files_section = True
                continue
            if in_files_section and line.strip().startswith('-'):
                # Extract filename from various formats
                # - news_service_v105.py
                # - `news_service_v105.py` ‚Üí `news_service.py`
                if '‚Üí' in line or '->' in line:
                    parts = line.replace('‚Üí', '->').split('->')
                    source = parts[0].strip().strip('-').strip().strip('`')
                    target = parts[1].strip().strip('`')
                else:
                    source = line.strip().strip('-').strip().strip('`')
                    target = source.replace('_v', '_').split('_')[0] + '.py'
                
                if source.endswith('.py'):
                    files_to_update.append({
                        'filename': source,
                        'target': target,
                        'action': 'update'
                    })
        
        # Extract risk level if present
        risk_level = "MEDIUM"
        for line in lines:
            if 'Risk Level:' in line:
                if 'HIGH' in line.upper():
                    risk_level = "HIGH"
                elif 'LOW' in line.upper():
                    risk_level = "LOW"
                break
        
        return cls(
            implementation_id=implementation_id,
            plan_name=plan_name,
            date_created=datetime.now().strftime("%Y-%m-%d"),
            phases=[
                {"name": "Discovery", "description": "Scan for updates"},
                {"name": "Documentation", "description": "Create change records"},
                {"name": "Preparation", "description": "Backup system"},
                {"name": "Implementation", "description": "Apply updates"},
                {"name": "Testing", "description": "Verify functionality"},
                {"name": "Completion", "description": "Finalize update"}
            ],
            files_to_update=files_to_update,
            risk_level=risk_level,
            raw_content=content
        )


class WhatsNextTaskList:
    """Generate What's Next Task List based on phase and status"""
    
    @staticmethod
    def generate(phase_name: str, phase_status: str, context: Dict) -> str:
        """Generate task list for the current situation"""
        task_list = [
            "=== WHAT'S NEXT TASK LIST ===",
            f"Phase: {phase_name} - {phase_status} {'‚úì' if phase_status == 'COMPLETED' else '‚úó'}",
            ""
        ]
        
        # Determine if in Jupyter/Colab
        python_cmd = "!python" if 'ipykernel' in sys.modules else "python"
        
        if phase_status == "COMPLETED":
            # Success path
            if "Discovery" in phase_name:
                update_count = context.get('update_count', 0)
                task_list.extend([
                    f"‚úÖ Found {update_count} updates to process",
                    "",
                    "Actions Required:",
                    "1. Review discovered updates in Change Diary",
                    f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                    "3. Monitor progress in Change Diary"
                ])
                
            elif "Documentation" in phase_name:
                task_list.extend([
                    "‚úÖ Change documentation created",
                    "",
                    "Actions Required:",
                    "1. Review impact analysis in Change Diary",
                    f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                    "3. Ensure system is ready for backup"
                ])
                
            elif "Preparation" in phase_name:
                backup_path = context.get('backup_path', './backups/backup_[timestamp]')
                task_list.extend([
                    "‚úÖ System prepared and backed up",
                    "",
                    "Actions Required:",
                    "1. Review Preparation results in Change Diary",
                    f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                    f"3. Verify backup completion in: {backup_path}",
                    "4. Confirm all services have been stopped"
                ])
                
            elif "Implementation" in phase_name:
                applied_count = context.get('applied_count', 0)
                task_list.extend([
                    f"‚úÖ Applied {applied_count} updates successfully",
                    "",
                    "Actions Required:",
                    "1. Review implementation results in Change Diary",
                    f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                    "3. Prepare for system testing"
                ])
                
            elif "Testing" in phase_name:
                task_list.extend([
                    "‚úÖ All tests passed",
                    "",
                    "Actions Required:",
                    "1. Review test results in Change Diary",
                    f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                    "3. Prepare for final completion"
                ])
                
            elif "Completion" in phase_name:
                task_list.extend([
                    "‚úÖ Update process completed successfully!",
                    "",
                    "Post-Implementation Tasks:",
                    "1. Review final Change Diary",
                    "2. Verify all services are operational",
                    "3. Archive implementation plan",
                    "4. Monitor system for 24 hours"
                ])
                
        elif phase_status == "FAILED":
            # Failure path
            task_list.extend([
                "Phase FAILED - Intervention Required",
                "",
                "Diagnostic Steps:",
                f"1. Run: {python_cmd} diagnostic_toolkit.py --report",
                "2. Review error details in Change Diary",
                "3. Check Google Drive connectivity",
                "",
                "Recovery Options:",
                f"- Fix issue and retry: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --retry",
                f"- Rollback changes: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --rollback",
                "- Get help: Review Implementation Plan for troubleshooting steps"
            ])
            
            if context.get('error'):
                task_list.extend([
                    "",
                    f"Error Details: {context['error']}"
                ])
                
        return "\n".join(task_list)


class ChangeManagementDiary:
    """Enhanced Change Management Diary with Google Drive integration"""
    
    def __init__(self, implementation_id: str, plan_name: str, drive_service):
        self.implementation_id = implementation_id
        self.plan_name = plan_name
        self.drive_service = drive_service
        self.start_time = datetime.now()
        
        # Create diary filename with proper naming convention
        date_str = self.start_time.strftime("%Y-%m-%d")
        self.diary_filename = f"Change Diary - {implementation_id} - {date_str}.md"
        
        self.phases = {
            "Phase 1": {"name": "Discovery and Verification", "status": "PENDING"},
            "Phase 2": {"name": "Change Management Documentation", "status": "PENDING"},
            "Phase 3": {"name": "Pre-Update Preparation", "status": "PENDING"},
            "Phase 4": {"name": "Implementation", "status": "PENDING"},
            "Phase 5": {"name": "Testing and Validation", "status": "PENDING"},
            "Phase 6": {"name": "Completion or Rollback", "status": "PENDING"}
        }
        self.current_phase = None
        self.detailed_progress = []
        self.phase_metadata = {}
        self.last_task_list = ""
        self._initialize_diary()
        
    def _initialize_diary(self):
        """Create initial diary with proper header format"""
        header = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Add phase checkboxes
        for phase_id, phase_info in self.phases.items():
            header += f"- [ ] {phase_id}: {phase_info['name']}\n"
            
        header += "\n## Current Status\n**Active Phase**: Starting\n"
        header += f"**Status**: INITIALIZING\n"
        header += f"**Last Updated**: {datetime.now().isoformat()}\n\n"
        header += "## What's Next Task List\nInitializing...\n\n"
        header += "## Detailed Progress\n\n"
        
        self._write_diary(header)
        
    def _write_diary(self, content: str):
        """Write content to diary file in Google Drive"""
        self.drive_service.write_file(
            self.diary_filename,
            content.encode('utf-8'),
            'project_documentation',
            mime_type='text/markdown'
        )
        
    def _update_diary(self):
        """Update the diary with current state"""
        content = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Update phase checkboxes
        for phase_id, phase_info in self.phases.items():
            status = phase_info.get('status', 'PENDING')
            checkbox = "[x]" if status in ['COMPLETE', 'COMPLETED'] else "[ ]"
            content += f"- {checkbox} {phase_id}: {phase_info['name']}"
            if status not in ['PENDING', 'COMPLETE', 'COMPLETED']:
                content += f" ({status})"
            content += "\n"
            
        # Current status
        content += f"\n## Current Status\n"
        content += f"**Active Phase**: {self.current_phase or 'None'}\n"
        content += f"**Status**: {self._get_overall_status()}\n"
        content += f"**Last Updated**: {datetime.now().isoformat()}\n"
        content += f"**Duration**: {(datetime.now() - self.start_time).total_seconds():.1f} seconds\n\n"
        
        # Add What's Next Task List
        content += "## What's Next Task List\n"
        content += self.last_task_list + "\n\n"
        
        # Detailed progress
        content += "## Detailed Progress\n\n"
        content += "\n".join(self.detailed_progress)
        
        self._write_diary(content)
    
    def start_phase(self, phase_name: str, metadata: Optional[Dict] = None):
        """Start a new phase"""
        self.current_phase = phase_name
        
        # Update phase status
        for phase_key, phase_info in self.phases.items():
            if phase_info['name'] == phase_name or phase_name in phase_key:
                self.phases[phase_key]['status'] = 'IN_PROGRESS'
                self.phases[phase_key]['start_time'] = datetime.now().isoformat()
                break
                
        # Store metadata
        if metadata:
            self.phase_metadata[phase_name] = metadata
            
        # Add to detailed progress
        entry = f"### {phase_name}\n"
        entry += f"**Started**: {datetime.now().isoformat()}\n"
        entry += f"**Status**: IN PROGRESS\n"
        
        if metadata:
            for key, value in metadata.items():
                entry += f"**{key.replace('_', ' ').title()}**: {value}\n"
                
        self.detailed_progress.append(entry)
        self._update_diary()
        
    def complete_phase(self, phase_name: str, status: str, message: str, 
                      context: Optional[Dict] = None):
        """Complete a phase with status"""
        # Update phase status
        for phase_key, phase_info in self.phases.items():
            if phase_info['name'] == phase_name or phase_name in phase_key:
                self.phases[phase_key]['status'] = status
                self.phases[phase_key]['end_time'] = datetime.now().isoformat()
                self.phases[phase_key]['message'] = message
                
        # Find the last entry for this phase and update it
        for i in range(len(self.detailed_progress) - 1, -1, -1):
            if phase_name in self.detailed_progress[i]:
                self.detailed_progress[i] += f"\n**Completed**: {datetime.now().isoformat()}\n"
                self.detailed_progress[i] += f"**Final Status**: {status}\n"
                self.detailed_progress[i] += f"**Message**: {message}\n"
                break
                
        # Generate What's Next Task List
        task_context = context or {}
        task_context['error'] = message if status == 'FAILED' else None
        self.last_task_list = WhatsNextTaskList.generate(phase_name, status, task_context)
        
        self._update_diary()
        
    def _get_overall_status(self) -> str:
        """Determine overall implementation status"""
        if all(p.get('status') in ['COMPLETE', 'COMPLETED'] for p in self.phases.values()):
            return 'COMPLETE'
        elif any(p.get('status') == 'FAILED' for p in self.phases.values()):
            return 'FAILED'
        elif any(p.get('status') == 'ROLLED_BACK' for p in self.phases.values()):
            return 'ROLLED_BACK'
        else:
            return 'IN_PROGRESS'


class TradingSystemUpdater:
    """Enhanced automated update process with Google Drive integration"""
    
    def __init__(self, implementation_plan: ImplementationPlan):
        self.plan = implementation_plan
        self.drive_service = get_drive_service()
        self.logger = logging.getLogger('TradingSystemUpdater')
        
        # Setup logging
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
        
        # Service mapping for correct target names
        self.service_mapping = {
            'coordination': 'coordination_service.py',
            'security_scanner': 'security_scanner.py',
            'pattern_analysis': 'pattern_analysis.py',
            'technical_analysis': 'technical_analysis.py',
            'paper_trading': 'paper_trading.py',
            'pattern_recognition': 'pattern_recognition_service.py',
            'news_service': 'news_service.py',
            'reporting': 'reporting_service.py',
            'web_dashboard': 'web_dashboard.py',
            'database_migration': 'database_migration.py',
            'diagnostic_toolkit': 'diagnostic_toolkit.py',
            'hybrid_manager': 'hybrid_manager.py'
        }
        
        # Initialize Change Diary
        self.diary = ChangeManagementDiary(
            implementation_plan.implementation_id,
            implementation_plan.plan_name,
            self.drive_service
        )
        
        self.logger.info(f"‚úÖ Initialized updater for Implementation ID: {self.plan.implementation_id}")
        
    def load_state(self) -> Optional[Dict]:
        """Load saved state from Google Drive"""
        return self.drive_service.load_json('.update_state.json', 'coordination')
        
    def save_state(self, phase: str, status: str):
        """Save current state to Google Drive"""
        state = {
            'implementation_id': self.plan.implementation_id,
            'plan_name': self.plan.plan_name,
            'current_phase': phase,
            'status': status,
            'timestamp': datetime.now().isoformat()
        }
        self.drive_service.save_json('.update_state.json', state, 'coordination')
        
    def execute_update(self, continue_from: Optional[str] = None) -> bool:
        """Execute the complete update process"""
        try:
            self.logger.info(f"üöÄ Starting update process for: {self.plan.implementation_id}")
            
            # Define phases
            phases = [
                ("Discovery", self._discover_updates),
                ("Documentation", self._document_changes),
                ("Preparation", self._prepare_system),
                ("Implementation", self._implement_updates),
                ("Testing", self._test_system),
                ("Completion", self._complete_update)
            ]
            
            # Determine starting point
            start_index = 0
            if continue_from:
                for i, (phase_name, _) in enumerate(phases):
                    if phase_name == continue_from:
                        start_index = i
                        break
                        
            # Execute phases
            context = {}
            for phase_name, phase_func in phases[start_index:]:
                self.save_state(phase_name, "IN_PROGRESS")
                
                try:
                    result = phase_func(context)
                    if not result.get('success', False):
                        # Phase failed
                        self.diary.complete_phase(phase_name, "FAILED", 
                                                result.get('error', 'Unknown error'),
                                                context)
                        self.save_state(phase_name, "FAILED")
                        return False
                        
                    # Phase succeeded
                    self.diary.complete_phase(phase_name, "COMPLETED", 
                                            result.get('message', 'Success'),
                                            context)
                    self.save_state(phase_name, "COMPLETED")
                    
                    # Update context for next phase
                    context.update(result.get('context', {}))
                    
                except Exception as e:
                    self.logger.error(f"Phase {phase_name} failed: {str(e)}")
                    self.diary.complete_phase(phase_name, "FAILED", str(e), context)
                    self.save_state(phase_name, "FAILED")
                    return False
                    
            # All phases completed
            self.diary.complete_phase("Update Process", "COMPLETED", 
                                    "All phases completed successfully")
            self.drive_service.delete_file('.update_state.json', 'coordination')
            return True
            
        except Exception as e:
            self.logger.error(f"Update process failed: {str(e)}")
            self.diary.complete_phase("Error", "FAILED", str(e))
            return False
            
    def _discover_updates(self, context: Dict) -> Dict:
        """Phase 1: Discover available updates"""
        self.diary.start_phase("Phase 1: Discovery and Verification")
        
        updates = []
        
        # Check updates folder in Google Drive
        update_files = self.drive_service.list_files('updates')
        
        # Also check for files specified in implementation plan
        if self.plan.files_to_update:
            # Look for these specific files
            for file_info in self.plan.files_to_update:
                filename = file_info['filename']
                
                # Check in updates folder
                found = False
                for update_file in update_files:
                    if update_file['name'] == filename:
                        updates.append({
                            'filename': filename,
                            'target': file_info['target'],
                            'file_id': update_file['id'],
                            'size': update_file.get('size', 0),
                            'modified': update_file.get('modifiedTime')
                        })
                        found = True
                        break
                
                if not found:
                    self.logger.warning(f"Update file not found: {filename}")
        else:
            # Auto-discover Python files in updates folder
            for update_file in update_files:
                if update_file['name'].endswith('.py'):
                    target = self._determine_target_name(update_file['name'])
                    updates.append({
                        'filename': update_file['name'],
                        'target': target,
                        'file_id': update_file['id'],
                        'size': update_file.get('size', 0),
                        'modified': update_file.get('modifiedTime')
                    })
        
        if not updates:
            return {
                'success': False,
                'error': 'No updates found in Google Drive updates folder'
            }
        
        self.diary.start_phase("Phase 1: Discovery and Verification", {
            'update_count': len(updates),
            'update_files': [u['filename'] for u in updates]
        })
        
        self.logger.info(f"‚úÖ Discovered {len(updates)} updates")
        
        return {
            'success': True,
            'message': f"Found {len(updates)} updates",
            'context': {
                'updates': updates,
                'update_count': len(updates)
            }
        }
        
    def _determine_target_name(self, source_filename: str) -> str:
        """Remove version suffix from filename"""
        import re
        base_name = re.sub(r'_v\d+', '', source_filename.replace('.py', ''))
        
        # Map to correct service name
        for service_key, target_name in self.service_mapping.items():
            if base_name in service_key:
                return target_name
                
        # Default: just remove version
        return base_name + '.py'
        
    def _document_changes(self, context: Dict) -> Dict:
        """Phase 2: Create change management documentation"""
        updates = context.get('updates', [])
        
        self.diary.start_phase("Phase 2: Change Management Documentation", {
            'files_to_update': len(updates),
            'update_list': [u['filename'] for u in updates]
        })
        
        # Analyze impact based on implementation plan
        impact_analysis = {
            'risk_level': self.plan.risk_level,
            'expected_downtime': f"{len(updates) * 30} seconds",
            'services_affected': len(updates),
            'database_changes': any('database' in u['filename'] for u in updates),
            'api_changes': any('api' in u['filename'] or 'service' in u['filename'] for u in updates)
        }
        
        self.logger.info(f"Impact Analysis: {impact_analysis}")
        
        return {
            'success': True,
            'message': f"Documented {len(updates)} changes, Risk Level: {impact_analysis['risk_level']}",
            'context': {'impact_analysis': impact_analysis}
        }
        
    def _prepare_system(self, context: Dict) -> Dict:
        """Phase 3: Prepare system for update"""
        self.diary.start_phase("Phase 3: Pre-Update Preparation")
        
        # Create backup
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{self.plan.implementation_id}_{timestamp}"
        
        try:
            # Get list of all service files
            service_files = []
            main_files = self.drive_service.list_files()
            
            for file in main_files:
                if file['name'].endswith('.py') and any(
                    svc in file['name'] for svc in ['service', 'manager', 'toolkit', 'migration']
                ):
                    service_files.append(file)
            
            # Create backup folder and copy files
            backup_folder_id = self.drive_service._find_or_create_folder(
                backup_name,
                self.drive_service.get_subfolder_id('backups')
            )
            
            # Copy each service file to backup
            for file_info in service_files:
                try:
                    copy_metadata = {
                        'name': file_info['name'],
                        'parents': [backup_folder_id]
                    }
                    self.drive_service.service.files().copy(
                        fileId=file_info['id'],
                        body=copy_metadata
                    ).execute()
                except Exception as e:
                    self.logger.warning(f"Failed to backup {file_info['name']}: {e}")
            
            self.logger.info(f"‚úÖ Created backup: {backup_name}")
            
            # Stop running services (simulate)
            self.logger.info("Stopping services...")
            
            return {
                'success': True,
                'message': f"System prepared, backup created: {backup_name}",
                'context': {
                    'backup_name': backup_name,
                    'backup_path': f"backups/{backup_name}",
                    'files_backed_up': len(service_files)
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"Preparation failed: {str(e)}"
            }
            
    def _implement_updates(self, context: Dict) -> Dict:
        """Phase 4: Apply updates to system"""
        self.diary.start_phase("Phase 4: Implementation")
        
        updates = context.get('updates', [])
        applied_updates = []
        
        try:
            for update in updates:
                # Read update file from updates folder
                update_content = self.drive_service.read_file(
                    update['filename'], 
                    'updates'
                )
                
                # Write to main directory (replacing existing)
                self.drive_service.write_file(
                    update['target'],
                    update_content,
                    None,  # Main directory
                    mime_type='text/x-python'
                )
                
                self.logger.info(f"‚úÖ Applied: {update['filename']} ‚Üí {update['target']}")
                
                applied_updates.append({
                    'source': update['filename'],
                    'target': update['target'],
                    'timestamp': datetime.now().isoformat()
                })
                
            return {
                'success': True,
                'message': f"Applied {len(applied_updates)} updates",
                'context': {
                    'applied_updates': applied_updates,
                    'applied_count': len(applied_updates)
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"Implementation failed: {str(e)}"
            }
            
    def _test_system(self, context: Dict) -> Dict:
        """Phase 5: Test the updated system"""
        self.diary.start_phase("Phase 5: Testing and Validation")
        
        test_results = []
        all_passed = True
        
        # Run basic validation tests
        tests = [
            ('File Integrity', self._test_file_integrity),
            ('Python Syntax', self._test_python_syntax),
            ('Service Dependencies', self._test_dependencies),
            ('Database Connectivity', self._test_database)
        ]
        
        for test_name, test_func in tests:
            try:
                passed, message = test_func(context)
                test_results.append({
                    'test': test_name,
                    'passed': passed,
                    'message': message
                })
                
                if passed:
                    self.logger.info(f"‚úÖ {test_name}: {message}")
                else:
                    self.logger.error(f"‚ùå {test_name}: {message}")
                    all_passed = False
                    
            except Exception as e:
                test_results.append({
                    'test': test_name,
                    'passed': False,
                    'message': f"Exception: {str(e)}"
                })
                all_passed = False
        
        if not all_passed:
            return {
                'success': False,
                'error': 'One or more tests failed',
                'context': {'test_results': test_results}
            }
            
        return {
            'success': True,
            'message': f"All {len(test_results)} tests passed",
            'context': {'test_results': test_results}
        }
        
    def _test_file_integrity(self, context: Dict) -> Tuple[bool, str]:
        """Test that all updated files exist"""
        applied_updates = context.get('applied_updates', [])
        
        for update in applied_updates:
            file_id = self.drive_service._find_file(update['target'])
            if not file_id:
                return False, f"File not found: {update['target']}"
        
        return True, f"All {len(applied_updates)} files verified"
        
    def _test_python_syntax(self, context: Dict) -> Tuple[bool, str]:
        """Test Python syntax of updated files"""
        applied_updates = context.get('applied_updates', [])
        
        for update in applied_updates:
            if update['target'].endswith('.py'):
                try:
                    content = self.drive_service.read_file(update['target'])
                    compile(content, update['target'], 'exec')
                except SyntaxError as e:
                    return False, f"Syntax error in {update['target']}: {e}"
                except Exception as e:
                    return False, f"Error checking {update['target']}: {e}"
        
        return True, "All Python files have valid syntax"
        
    def _test_dependencies(self, context: Dict) -> Tuple[bool, str]:
        """Test that required dependencies are available"""
        try:
            required = ['flask', 'requests', 'pandas', 'numpy', 'googleapiclient']
            missing = []
            
            for module in required:
                try:
                    __import__(module.split('.')[0])
                except ImportError:
                    missing.append(module)
            
            if missing:
                return False, f"Missing dependencies: {', '.join(missing)}"
                
            return True, "All dependencies available"
            
        except Exception as e:
            return False, f"Dependency check failed: {str(e)}"
        
    def _test_database(self, context: Dict) -> Tuple[bool, str]:
        """Test database connectivity"""
        try:
            db_info = self.drive_service.get_database_info()
            if db_info['exists']:
                return True, f"Database exists: {db_info['size']} bytes"
            else:
                return False, "Database not found"
        except Exception as e:
            return False, f"Database check failed: {str(e)}"
            
    def _complete_update(self, context: Dict) -> Dict:
        """Phase 6: Complete the update process"""
        self.diary.start_phase("Phase 6: Completion")
        
        # Archive processed update files
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_folder = f"archive_{self.plan.implementation_id}_{timestamp}"
            
            # Create archive folder
            archive_folder_id = self.drive_service._find_or_create_folder(
                archive_folder,
                self.drive_service.get_subfolder_id('updates')
            )
            
            # Move update files to archive
            updates = context.get('updates', [])
            for update in updates:
                try:
                    # Move file to archive folder
                    self.drive_service.service.files().update(
                        fileId=update['file_id'],
                        addParents=archive_folder_id,
                        removeParents=self.drive_service.get_subfolder_id('updates')
                    ).execute()
                except Exception as e:
                    self.logger.warning(f"Failed to archive {update['filename']}: {e}")
            
            # Update system version info
            version_info = {
                'last_update': datetime.now().isoformat(),
                'implementation_id': self.plan.implementation_id,
                'applied_updates': context.get('applied_updates', []),
                'update_count': len(context.get('applied_updates', [])),
                'archive_folder': archive_folder
            }
            
            self.drive_service.save_json(
                'last_update_info.json',
                version_info,
                'coordination'
            )
            
            self.logger.info(f"‚úÖ Update process completed, files archived to: {archive_folder}")
            
            return {
                'success': True,
                'message': 'Update process completed successfully',
                'context': {
                    'archive_folder': archive_folder,
                    'completion_time': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Completion failed: {str(e)}'
            }
            
    def rollback(self, backup_name: str) -> bool:
        """Rollback to a previous backup"""
        try:
            self.logger.info(f"üîÑ Starting rollback to: {backup_name}")
            
            # Find backup folder
            backup_files = self.drive_service.list_files('backups')
            backup_folder = None
            
            for file in backup_files:
                if file['name'] == backup_name and file['mimeType'] == 'application/vnd.google-apps.folder':
                    backup_folder = file
                    break
            
            if not backup_folder:
                self.logger.error(f"Backup not found: {backup_name}")
                return False
            
            # List files in backup
            backup_contents = self.drive_service.service.files().list(
                q=f"parents in '{backup_folder['id']}' and trashed=false",
                fields="files(id, name)"
            ).execute().get('files', [])
            
            # Restore each file
            for file_info in backup_contents:
                try:
                    # Read backup file
                    request = self.drive_service.service.files().get_media(
                        fileId=file_info['id']
                    )
                    content = io.BytesIO()
                    downloader = MediaIoBaseDownload(content, request)
                    
                    done = False
                    while not done:
                        status, done = downloader.next_chunk()
                    
                    # Write to main directory
                    self.drive_service.write_file(
                        file_info['name'],
                        content.getvalue(),
                        None,  # Main directory
                        mime_type='text/x-python'
                    )
                    
                    self.logger.info(f"‚úÖ Restored: {file_info['name']}")
                    
                except Exception as e:
                    self.logger.error(f"Failed to restore {file_info['name']}: {e}")
            
            self.logger.info("‚úÖ Rollback completed")
            return True
            
        except Exception as e:
            self.logger.error(f"Rollback failed: {e}")
            return False


def main():
    """Main entry point with enhanced command line interface"""
    # Detect if running in Jupyter/Colab
    in_jupyter = 'ipykernel' in sys.modules
    python_cmd = "!python" if in_jupyter else "python"
    
    parser = argparse.ArgumentParser(
        description='Trading System Automated Update Process v2.0.1',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  # Execute an implementation plan
  {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --plan "Implementation Plan - NEWS-FIX - 2025-06-19.md"
  
  # Continue from interruption
  {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue
  
  # Check current status
  {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --status
  
  # Rollback failed implementation
  {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --rollback
  
  # Check for updates only
  {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --check-only
        """
    )
    
    parser.add_argument('--plan', type=str, 
                       help='Execute specified implementation plan')
    parser.add_argument('--continue', action='store_true',
                       help='Continue from last checkpoint')
    parser.add_argument('--status', action='store_true',
                       help='Check current implementation status')
    parser.add_argument('--rollback', action='store_true',
                       help='Rollback to previous state')
    parser.add_argument('--check-only', action='store_true',
                       help='Check for updates without applying')
    parser.add_argument('--retry', action='store_true',
                       help='Retry the last failed phase')
    
    # Use parse_known_args for Jupyter compatibility
    args, unknown = parser.parse_known_args()
    
    try:
        # Initialize Google Drive service
        drive_service = get_drive_service()
        
        # Handle different command options
        if args.status:
            # Check status
            state = drive_service.load_json('.update_state.json', 'coordination')
            if state:
                print(f"\nüìä IMPLEMENTATION STATUS")
                print(f"   Implementation ID: {state['implementation_id']}")
                print(f"   Plan: {state['plan_name']}")
                print(f"   Current Phase: {state['current_phase']}")
                print(f"   Status: {state['status']}")
                print(f"   Last Updated: {state['timestamp']}")
            else:
                print("‚ÑπÔ∏è No implementation currently in progress")
                
        elif args.check_only:
            # Check for updates
            print("üîç Checking for updates in Google Drive...")
            updates = drive_service.list_files('updates')
            python_updates = [f for f in updates if f['name'].endswith('.py')]
            
            if python_updates:
                print(f"\nüì¶ Found {len(python_updates)} updates:")
                for update in python_updates:
                    print(f"   - {update['name']} ({update.get('size', 0)} bytes)")
            else:
                print("‚úÖ No updates found")
                
        elif args.plan:
            # Execute implementation plan
            print(f"üìã Loading implementation plan: {args.plan}")
            
            # Load plan from Google Drive
            plan_content = drive_service.read_file(args.plan, 'project_documentation')
            if not plan_content:
                print(f"‚ùå Implementation plan not found: {args.plan}")
                sys.exit(1)
                
            plan = ImplementationPlan.from_drive_content(
                args.plan,
                plan_content.decode('utf-8')
            )
            
            print(f"‚úÖ Loaded plan: {plan.implementation_id}")
            
            # Execute update
            updater = TradingSystemUpdater(plan)
            success = updater.execute_update()
            
            sys.exit(0 if success else 1)
            
        elif args.continue or args.retry:
            # Continue or retry from saved state
            state = drive_service.load_json('.update_state.json', 'coordination')
            if not state:
                print("‚ùå No implementation in progress to continue")
                sys.exit(1)
                
            # Reload implementation plan
            plan_content = drive_service.read_file(state['plan_name'], 'project_documentation')
            if not plan_content:
                print(f"‚ùå Original plan not found: {state['plan_name']}")
                sys.exit(1)
                
            plan = ImplementationPlan.from_drive_content(
                state['plan_name'],
                plan_content.decode('utf-8')
            )
            
            print(f"‚úÖ Resuming implementation: {plan.implementation_id}")
            
            # Continue from saved phase
            updater = TradingSystemUpdater(plan)
            
            if args.retry and state['status'] == 'FAILED':
                # Retry the failed phase
                success = updater.execute_update(continue_from=state['current_phase'])
            else:
                # Continue from next phase
                phases = ["Discovery", "Documentation", "Preparation", 
                         "Implementation", "Testing", "Completion"]
                current_index = phases.index(state['current_phase'])
                if current_index < len(phases) - 1:
                    success = updater.execute_update(continue_from=phases[current_index + 1])
                else:
                    print("‚úÖ Implementation already completed")
                    success = True
                    
            sys.exit(0 if success else 1)
            
        elif args.rollback:
            # Rollback implementation
            state = drive_service.load_json('.update_state.json', 'coordination')
            if not state:
                print("‚ùå No implementation in progress to rollback")
                # Check for recent backups
                backups = drive_service.list_files('backups')
                if backups:
                    print("\nüìÅ Available backups:")
                    for backup in backups[:5]:  # Show last 5
                        print(f"   - {backup['name']}")
                sys.exit(1)
                
            # Find backup for this implementation
            backup_prefix = f"backup_{state['implementation_id']}_"
            backups = drive_service.list_files('backups')
            matching_backup = None
            
            for backup in backups:
                if backup['name'].startswith(backup_prefix):
                    matching_backup = backup['name']
                    break
                    
            if not matching_backup:
                print(f"‚ùå No backup found for implementation: {state['implementation_id']}")
                sys.exit(1)
                
            print(f"üîÑ Rolling back to: {matching_backup}")
            
            # Create minimal updater for rollback
            plan = ImplementationPlan(
                implementation_id=state['implementation_id'],
                plan_name=state['plan_name'],
                date_created=datetime.now().strftime("%Y-%m-%d"),
                phases=[],
                files_to_update=[]
            )
            
            updater = TradingSystemUpdater(plan)
            success = updater.rollback(matching_backup)
            
            if success:
                # Clear state
                drive_service.delete_file('.update_state.json', 'coordination')
                print("‚úÖ Rollback completed successfully")
            else:
                print("‚ùå Rollback failed")
                
            sys.exit(0 if success else 1)
            
        else:
            # No arguments - show help
            parser.print_help()
            
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================================================
FILE: ./web_dashboard_service.py
================================================================================

"""
Name of Service: TRADING SYSTEM PHASE 1 - WEB DASHBOARD SERVICE
Version: 1.0.4
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.4 (2025-06-19) - Added Trade tab with trading controls and schedule display
v1.0.3 (2025-06-19) - Enhanced with fallback status checking and auto-registration
v1.0.2 (2025-06-17) - Fixed real-time updates and WebSocket functionality
v1.0.1 (2025-06-15) - Added comprehensive monitoring dashboard
v1.0.0 (2025-06-15) - Initial web dashboard implementation

Web Dashboard Service - Provides real-time monitoring and control interface
"""

import os
import requests
import logging
from datetime import datetime, timedelta
from flask import Flask, render_template_string, jsonify, request
from flask_cors import CORS
import json
import time

class WebDashboardService:
    def __init__(self, port=5010):
        self.app = Flask(__name__)
        CORS(self.app)
        self.port = port
        self.logger = self._setup_logging()
        self.coordination_service_url = "http://localhost:5000"
        
        # Status cache to reduce load
        self.status_cache = {
            'data': None,
            'timestamp': None,
            'cache_duration': 10  # seconds
        }
        
        self._setup_routes()
        self._register_with_coordination()
        self._initialize_service_registry()
        
    def _setup_logging(self):
        import os
        os.makedirs('./logs', exist_ok=True)
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('WebDashboardService')
        
        handler = logging.FileHandler('./logs/web_dashboard_service.log')
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    def _initialize_service_registry(self):
        """Force registration of all healthy services on startup"""
        try:
            response = requests.post(f"{self.coordination_service_url}/force_register_all", timeout=5)
            if response.status_code == 200:
                self.logger.info("Initialized service registry")
        except Exception as e:
            self.logger.warning(f"Could not initialize registry: {e}")
    
    def _register_with_coordination(self):
        """Register with coordination service"""
        try:
            registration_data = {
                "service_name": "dashboard",
                "port": self.port,
                "endpoints": ["/", "/api/status", "/api/services", "/api/trading_cycle", 
                             "/api/trade/start_cycle", "/api/schedule/status", "/api/schedule/config"]
            }
            response = requests.post(f"{self.coordination_service_url}/register_service", 
                                   json=registration_data, timeout=5)
            if response.status_code == 200:
                self.logger.info("Successfully registered with coordination service")
        except Exception as e:
            self.logger.warning(f"Could not register with coordination service: {e}")
    
    def _check_service_health_direct(self, service_name: str, port: int) -> bool:
        """Check service health directly"""
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def _get_service_status_with_fallback(self):
        """Get comprehensive service status with fallback to direct health checks"""
        # Check cache first
        if self.status_cache['data'] and self.status_cache['timestamp']:
            cache_age = (datetime.now() - self.status_cache['timestamp']).total_seconds()
            if cache_age < self.status_cache['cache_duration']:
                return self.status_cache['data']
        
        services_info = {}
        
        # Known services and their ports
        known_services = {
            "coordination": {"port": 5000, "name": "Coordination Service"},
            "scanner": {"port": 5001, "name": "Security Scanner"},
            "pattern": {"port": 5002, "name": "Pattern Analysis"},
            "technical": {"port": 5003, "name": "Technical Analysis"},
            "trading": {"port": 5005, "name": "Paper Trading"},
            "pattern_rec": {"port": 5006, "name": "Pattern Recognition"},
            "news": {"port": 5008, "name": "News Service"},
            "reporting": {"port": 5009, "name": "Reporting Service"},
            "dashboard": {"port": 5010, "name": "Web Dashboard"},
            "scheduler": {"port": 5011, "name": "Trading Scheduler"}
        }
        
        # Try to get status from coordination service first
        try:
            response = requests.get(f"{self.coordination_service_url}/service_status", timeout=3)
            if response.status_code == 200:
                coord_status = response.json()
                
                # Merge with our known services
                for service_id, service_info in known_services.items():
                    if service_id in coord_status:
                        services_info[service_id] = {
                            **service_info,
                            **coord_status[service_id],
                            'registered': True
                        }
                    else:
                        # Not in coordination service, check directly
                        is_healthy = self._check_service_health_direct(service_id, service_info['port'])
                        services_info[service_id] = {
                            **service_info,
                            'healthy': is_healthy,
                            'registered': False,
                            'status': 'active' if is_healthy else 'inactive'
                        }
                        
                        # If healthy but not registered, try to register it
                        if is_healthy and service_id != 'dashboard':
                            self._attempt_service_registration(service_id, service_info['port'])
        except Exception as e:
            self.logger.error(f"Could not get coordination status: {e}")
            
            # Fallback: check all services directly
            for service_id, service_info in known_services.items():
                is_healthy = self._check_service_health_direct(service_id, service_info['port'])
                services_info[service_id] = {
                    **service_info,
                    'healthy': is_healthy,
                    'registered': False,
                    'status': 'active' if is_healthy else 'inactive'
                }
        
        # Cache the result
        self.status_cache['data'] = services_info
        self.status_cache['timestamp'] = datetime.now()
        
        return services_info
    
    def _attempt_service_registration(self, service_name: str, port: int):
        """Attempt to register a healthy but unregistered service"""
        try:
            registration_data = {
                "service_name": service_name,
                "port": port
            }
            requests.post(f"{self.coordination_service_url}/register_service", 
                         json=registration_data, timeout=2)
        except:
            pass  # Silent fail, will retry on next check
    
    def _setup_routes(self):
        @self.app.route('/')
        def dashboard():
            return render_template_string(DASHBOARD_HTML)
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({"status": "healthy", "service": "dashboard"})
        
        @self.app.route('/api/status', methods=['GET'])
        def api_status():
            """Get overall system status"""
            services = self._get_service_status_with_fallback()
            
            total_services = len(services)
            active_services = sum(1 for s in services.values() if s.get('healthy', False))
            
            # Determine overall status
            if active_services == total_services:
                overall_status = "Fully Operational"
                status_class = "success"
            elif active_services >= total_services * 0.7:
                overall_status = "Degraded Performance"
                status_class = "warning"
            elif active_services > 0:
                overall_status = "Partial Outage"
                status_class = "danger"
            else:
                overall_status = "System Offline"
                status_class = "danger"
            
            return jsonify({
                "overall_status": overall_status,
                "status_class": status_class,
                "active_services": active_services,
                "total_services": total_services,
                "timestamp": datetime.now().isoformat()
            })
        
        @self.app.route('/api/services', methods=['GET'])
        def api_services():
            """Get detailed service status"""
            services = self._get_service_status_with_fallback()
            return jsonify(services)
        
        @self.app.route('/api/trading_cycle', methods=['GET'])
        def api_trading_cycle():
            """Get latest trading cycle info"""
            try:
                response = requests.get(f"{self.coordination_service_url}/latest_cycle", timeout=5)
                if response.status_code == 200:
                    return jsonify(response.json())
            except:
                pass
            
            return jsonify({"status": "No active cycle"})
        
        @self.app.route('/api/trade/start_cycle', methods=['POST'])
        def start_trading_cycle():
            """Start a new trading cycle"""
            try:
                response = requests.post(f"{self.coordination_service_url}/start_trading_cycle", timeout=30)
                if response.status_code == 200:
                    return jsonify(response.json())
                else:
                    return jsonify({"error": "Failed to start trading cycle"}), 500
            except Exception as e:
                return jsonify({"error": f"Error starting cycle: {str(e)}"}), 500
        
        @self.app.route('/api/schedule/status', methods=['GET'])
        def get_schedule_status():
            """Get trading schedule status"""
            try:
                response = requests.get(f"{self.coordination_service_url}/schedule/status", timeout=5)
                if response.status_code == 200:
                    return jsonify(response.json())
            except:
                pass
            
            # Default if scheduler not available
            return jsonify({
                "enabled": False,
                "message": "Trading scheduler not available",
                "next_run": None
            })
        
        @self.app.route('/api/schedule/config', methods=['GET', 'POST'])
        def schedule_config():
            """Get or set trading schedule configuration"""
            if request.method == 'GET':
                try:
                    response = requests.get(f"{self.coordination_service_url}/schedule/config", timeout=5)
                    if response.status_code == 200:
                        return jsonify(response.json())
                except:
                    pass
                
                # Default configuration
                return jsonify({
                    "enabled": False,
                    "interval_minutes": 30,
                    "market_hours_only": True,
                    "start_time": "09:30",
                    "end_time": "16:00"
                })
            
            else:  # POST
                try:
                    response = requests.post(f"{self.coordination_service_url}/schedule/config", 
                                           json=request.json, timeout=5)
                    if response.status_code == 200:
                        return jsonify(response.json())
                except Exception as e:
                    return jsonify({"error": str(e)}), 500
    
    def run(self):
        """Start the dashboard service"""
        self.logger.info(f"Starting Web Dashboard Service on port {self.port}")
        
        # Run in production mode
        from waitress import serve
        serve(self.app, host='0.0.0.0', port=self.port, threads=4)

# Enhanced Dashboard HTML with Trade tab
DASHBOARD_HTML = '''
<!DOCTYPE html>
<html>
<head>
    <title>Trading System Dashboard</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
    <style>
        .status-card {
            transition: all 0.3s ease;
        }
        .status-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }
        .service-indicator {
            width: 15px;
            height: 15px;
            border-radius: 50%;
            display: inline-block;
            margin-right: 10px;
            animation: pulse 2s infinite;
        }
        .service-healthy {
            background-color: #28a745;
        }
        .service-unhealthy {
            background-color: #dc3545;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        .cycle-info {
            background-color: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin-top: 20px;
        }
        .main-status {
            font-size: 2.5rem;
            font-weight: bold;
            margin-bottom: 20px;
        }
        .btn-trade {
            font-size: 1.2rem;
            padding: 10px 30px;
            margin: 10px;
        }
        .schedule-info {
            background-color: #e9ecef;
            border-radius: 10px;
            padding: 15px;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
        <div class="container">
            <a class="navbar-brand" href="#">Trading System Dashboard</a>
            <div class="navbar-nav ms-auto">
                <span class="navbar-text" id="current-time"></span>
            </div>
        </div>
    </nav>

    <div class="container mt-4">
        <!-- Nav tabs -->
        <ul class="nav nav-tabs" id="dashboardTabs" role="tablist">
            <li class="nav-item" role="presentation">
                <button class="nav-link active" id="overview-tab" data-bs-toggle="tab" data-bs-target="#overview" type="button">Overview</button>
            </li>
            <li class="nav-item" role="presentation">
                <button class="nav-link" id="services-tab" data-bs-toggle="tab" data-bs-target="#services" type="button">Services</button>
            </li>
            <li class="nav-item" role="presentation">
                <button class="nav-link" id="trade-tab" data-bs-toggle="tab" data-bs-target="#trade" type="button">Trade</button>
            </li>
            <li class="nav-item" role="presentation">
                <button class="nav-link" id="history-tab" data-bs-toggle="tab" data-bs-target="#history" type="button">Trading History</button>
            </li>
            <li class="nav-item" role="presentation">
                <button class="nav-link" id="logs-tab" data-bs-toggle="tab" data-bs-target="#logs" type="button">Logs</button>
            </li>
        </ul>

        <!-- Tab content -->
        <div class="tab-content" id="dashboardTabContent">
            <!-- Overview Tab -->
            <div class="tab-pane fade show active" id="overview" role="tabpanel">
                <div class="row mt-4">
                    <div class="col-md-12">
                        <div class="card status-card">
                            <div class="card-body text-center">
                                <h5 class="card-title">System Status</h5>
                                <div id="overall-status" class="main-status">Loading...</div>
                                <p class="card-text">
                                    <span id="active-services">0</span> / <span id="total-services">0</span> Services Active
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="cycle-info" id="cycle-info">
                    <h5>Latest Trading Cycle</h5>
                    <div id="cycle-details">No cycle data available</div>
                </div>
            </div>

            <!-- Services Tab -->
            <div class="tab-pane fade" id="services" role="tabpanel">
                <div class="row mt-4" id="services-grid">
                    <!-- Services will be populated here -->
                </div>
            </div>

            <!-- Trade Tab -->
            <div class="tab-pane fade" id="trade" role="tabpanel">
                <div class="row mt-4">
                    <div class="col-md-12 text-center">
                        <h3>Trading Controls</h3>
                        
                        <div class="mt-4">
                            <button class="btn btn-success btn-lg btn-trade" onclick="startTradingCycle()">
                                Start Trading Cycle
                            </button>
                        </div>
                        
                        <div class="schedule-info">
                            <h5>Trading Schedule</h5>
                            <div id="schedule-status">Loading schedule...</div>
                            
                            <div class="mt-3">
                                <button class="btn btn-primary" onclick="showScheduleConfig()">
                                    Configure Schedule
                                </button>
                            </div>
                        </div>
                        
                        <div id="trading-result" class="mt-4"></div>
                    </div>
                </div>
                
                <!-- Schedule Configuration Modal -->
                <div class="modal fade" id="scheduleModal" tabindex="-1">
                    <div class="modal-dialog">
                        <div class="modal-content">
                            <div class="modal-header">
                                <h5 class="modal-title">Trading Schedule Configuration</h5>
                                <button type="button" class="btn-close" data-bs-dismiss="modal"></button>
                            </div>
                            <div class="modal-body">
                                <form id="scheduleForm">
                                    <div class="mb-3">
                                        <div class="form-check form-switch">
                                            <input class="form-check-input" type="checkbox" id="scheduleEnabled">
                                            <label class="form-check-label" for="scheduleEnabled">
                                                Enable Automated Trading
                                            </label>
                                        </div>
                                    </div>
                                    
                                    <div class="mb-3">
                                        <label for="intervalMinutes" class="form-label">Trading Interval (minutes)</label>
                                        <input type="number" class="form-control" id="intervalMinutes" value="30" min="5" max="240">
                                    </div>
                                    
                                    <div class="mb-3">
                                        <div class="form-check">
                                            <input class="form-check-input" type="checkbox" id="marketHoursOnly" checked>
                                            <label class="form-check-label" for="marketHoursOnly">
                                                Trade only during market hours
                                            </label>
                                        </div>
                                    </div>
                                    
                                    <div class="row">
                                        <div class="col">
                                            <label for="startTime" class="form-label">Start Time</label>
                                            <input type="time" class="form-control" id="startTime" value="09:30">
                                        </div>
                                        <div class="col">
                                            <label for="endTime" class="form-label">End Time</label>
                                            <input type="time" class="form-control" id="endTime" value="16:00">
                                        </div>
                                    </div>
                                </form>
                            </div>
                            <div class="modal-footer">
                                <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Cancel</button>
                                <button type="button" class="btn btn-primary" onclick="saveScheduleConfig()">Save Schedule</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Trading History Tab -->
            <div class="tab-pane fade" id="history" role="tabpanel">
                <div class="mt-4">
                    <h5>Recent Trading Activity</h5>
                    <p>Trading history will be displayed here once trading begins.</p>
                </div>
            </div>

            <!-- Logs Tab -->
            <div class="tab-pane fade" id="logs" role="tabpanel">
                <div class="mt-4">
                    <h5>System Logs</h5>
                    <div class="bg-dark text-light p-3 rounded" style="height: 400px; overflow-y: auto;">
                        <pre id="log-content">Logs will appear here...</pre>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        let scheduleModal;
        
        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            scheduleModal = new bootstrap.Modal(document.getElementById('scheduleModal'));
            updateDashboard();
            setInterval(updateDashboard, 5000);
            setInterval(updateTime, 1000);
            updateTime();
        });

        function updateTime() {
            const now = new Date();
            document.getElementById('current-time').textContent = now.toLocaleString();
        }

        async function updateDashboard() {
            try {
                // Update overall status
                const statusResponse = await axios.get('/api/status');
                const status = statusResponse.data;
                
                const statusElement = document.getElementById('overall-status');
                statusElement.textContent = status.overall_status;
                statusElement.className = 'main-status text-' + status.status_class;
                
                document.getElementById('active-services').textContent = status.active_services;
                document.getElementById('total-services').textContent = status.total_services;
                
                // Update services grid
                const servicesResponse = await axios.get('/api/services');
                updateServicesGrid(servicesResponse.data);
                
                // Update cycle info
                const cycleResponse = await axios.get('/api/trading_cycle');
                updateCycleInfo(cycleResponse.data);
                
                // Update schedule status
                updateScheduleStatus();
                
            } catch (error) {
                console.error('Error updating dashboard:', error);
            }
        }

        function updateServicesGrid(services) {
            const grid = document.getElementById('services-grid');
            grid.innerHTML = '';
            
            Object.entries(services).forEach(([id, service]) => {
                const isHealthy = service.healthy || false;
                const isRegistered = service.registered || false;
                
                const card = document.createElement('div');
                card.className = 'col-md-4 mb-3';
                card.innerHTML = `
                    <div class="card status-card ${isHealthy ? 'border-success' : 'border-danger'}">
                        <div class="card-body">
                            <h5 class="card-title">
                                <span class="service-indicator ${isHealthy ? 'service-healthy' : 'service-unhealthy'}"></span>
                                ${service.name}
                            </h5>
                            <p class="card-text">
                                Port: ${service.port}<br>
                                Status: ${isHealthy ? 'Healthy' : 'Unhealthy'}<br>
                                Registered: ${isRegistered ? 'Yes' : 'No'}
                            </p>
                        </div>
                    </div>
                `;
                grid.appendChild(card);
            });
        }

        function updateCycleInfo(cycleData) {
            const cycleDetails = document.getElementById('cycle-details');
            
            if (cycleData && cycleData.cycle_id) {
                cycleDetails.innerHTML = `
                    <p><strong>Cycle ID:</strong> ${cycleData.cycle_id}</p>
                    <p><strong>Status:</strong> ${cycleData.status}</p>
                    <p><strong>Started:</strong> ${cycleData.started_at || 'N/A'}</p>
                    <p><strong>Securities Scanned:</strong> ${cycleData.securities_scanned || 0}</p>
                    <p><strong>Patterns Analyzed:</strong> ${cycleData.patterns_analyzed || 0}</p>
                    <p><strong>Signals Generated:</strong> ${cycleData.signals_generated || 0}</p>
                    <p><strong>Trades Executed:</strong> ${cycleData.trades_executed || 0}</p>
                `;
            } else {
                cycleDetails.innerHTML = '<p>No active trading cycle</p>';
            }
        }
        
        async function startTradingCycle() {
            const resultDiv = document.getElementById('trading-result');
            resultDiv.innerHTML = '<div class="alert alert-info">Starting trading cycle...</div>';
            
            try {
                const response = await axios.post('/api/trade/start_cycle');
                resultDiv.innerHTML = `
                    <div class="alert alert-success">
                        <h5>Trading Cycle Started Successfully!</h5>
                        <p>Cycle ID: ${response.data.cycle_id}</p>
                        <p>Status: ${response.data.status}</p>
                        <p>Securities Scanned: ${response.data.securities_scanned}</p>
                        <p>Trades Executed: ${response.data.trades_executed}</p>
                    </div>
                `;
            } catch (error) {
                resultDiv.innerHTML = `
                    <div class="alert alert-danger">
                        <h5>Error Starting Trading Cycle</h5>
                        <p>${error.response?.data?.error || error.message}</p>
                    </div>
                `;
            }
        }
        
        async function updateScheduleStatus() {
            try {
                const response = await axios.get('/api/schedule/status');
                const status = response.data;
                
                const scheduleDiv = document.getElementById('schedule-status');
                if (status.enabled) {
                    scheduleDiv.innerHTML = `
                        <p class="text-success"><strong>Schedule Active</strong></p>
                        <p>Next run: ${status.next_run || 'Calculating...'}</p>
                        <p>Interval: ${status.interval_minutes || 30} minutes</p>
                    `;
                } else {
                    scheduleDiv.innerHTML = `
                        <p class="text-muted">Automated trading is disabled</p>
                        <p>Click "Configure Schedule" to enable</p>
                    `;
                }
            } catch (error) {
                console.error('Error updating schedule status:', error);
            }
        }
        
        async function showScheduleConfig() {
            try {
                const response = await axios.get('/api/schedule/config');
                const config = response.data;
                
                document.getElementById('scheduleEnabled').checked = config.enabled || false;
                document.getElementById('intervalMinutes').value = config.interval_minutes || 30;
                document.getElementById('marketHoursOnly').checked = config.market_hours_only !== false;
                document.getElementById('startTime').value = config.start_time || '09:30';
                document.getElementById('endTime').value = config.end_time || '16:00';
                
                scheduleModal.show();
            } catch (error) {
                alert('Error loading schedule configuration');
            }
        }
        
        async function saveScheduleConfig() {
            const config = {
                enabled: document.getElementById('scheduleEnabled').checked,
                interval_minutes: parseInt(document.getElementById('intervalMinutes').value),
                market_hours_only: document.getElementById('marketHoursOnly').checked,
                start_time: document.getElementById('startTime').value,
                end_time: document.getElementById('endTime').value
            };
            
            try {
                await axios.post('/api/schedule/config', config);
                scheduleModal.hide();
                updateScheduleStatus();
                alert('Schedule configuration saved successfully!');
            } catch (error) {
                alert('Error saving schedule configuration');
            }
        }
    </script>
</body>
</html>
'''

if __name__ == "__main__":
    dashboard = WebDashboardService()
    dashboard.run()


================================================================================
FILE: ./project_documentation/TRADING SYSTEM PROJECT METHODOLOGY.md
================================================================================

# Trading System Project Methodology - GitHub Codespaces Edition

**Document**: TRADING SYSTEM PROJECT METHODOLOGY - GITHUB CODESPACES  
**Version**: 1.0.0  
**Last Updated**: 2025-01-13  
**Author**: Trading System Development Team  

## REVISION HISTORY
- v1.0.0 (2025-01-13) - Initial GitHub Codespaces version adapted from v3.0.3
  - Removed all Google Drive dependencies
  - Replaced with GitHub repository structure
  - Uses Git for version control and backups
  - Maintains all 6-phase process requirements

## 1. OVERVIEW

This methodology defines the standardized process for developing and maintaining the Trading System within GitHub Codespaces environment. It ensures consistency, traceability, and quality across all development activities.

### Key Differences from Google Drive Version:
- **Documentation Storage**: GitHub repository instead of Google Drive
- **File Discovery**: Git status and repository scanning instead of Drive API
- **Backups**: Git branches and tags instead of Drive folders
- **Collaboration**: GitHub Pull Requests instead of Drive sharing
- **Version Control**: Native Git instead of Drive file versions

## 2. PROJECT STRUCTURE

### Repository Organization
```
trading-system/
‚îú‚îÄ‚îÄ src/                           # Source code
‚îÇ   ‚îú‚îÄ‚îÄ services/                  # Core services
‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Utility modules
‚îÇ   ‚îî‚îÄ‚îÄ tests/                     # Test files
‚îú‚îÄ‚îÄ project_documentation/         # All project docs
‚îÇ   ‚îú‚îÄ‚îÄ implementation_plans/      # Approved plans
‚îÇ   ‚îú‚îÄ‚îÄ change_diaries/           # Change records
‚îÇ   ‚îú‚îÄ‚îÄ specifications/           # Technical specs
‚îÇ   ‚îî‚îÄ‚îÄ archive/                  # Historical docs
‚îú‚îÄ‚îÄ logs/                         # System logs
‚îú‚îÄ‚îÄ backups/                      # Local backups
‚îú‚îÄ‚îÄ updates/                      # Pending updates
‚îî‚îÄ‚îÄ .github/                      # GitHub configs
    ‚îú‚îÄ‚îÄ workflows/                # GitHub Actions
    ‚îî‚îÄ‚îÄ CODEOWNERS               # Code ownership
```

## 3. WORKFLOW OVERVIEW

### 3.1 Development Workflow
1. **Requirements Analysis** ‚Üí Create/Update Functional Specification
2. **Solution Design** ‚Üí Create Technical Specification  
3. **Implementation Planning** ‚Üí Create Implementation Plan with unique ID
4. **Automated Execution** ‚Üí Run TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
5. **Change Documentation** ‚Üí Automatic Change Diary creation
6. **Testing & Validation** ‚Üí Automated and manual testing
7. **Git Commit & Push** ‚Üí Version control with meaningful commits

### 3.2 Documentation Flow
```
Functional Spec ‚Üí Technical Spec ‚Üí Implementation Plan ‚Üí Change Diary
     ‚Üì                ‚Üì                    ‚Üì                   ‚Üì
  (defines)       (designs)           (executes)          (records)
```

## 4. AUTOMATED UPDATE PROCESS

### 4.1 Core Components

#### TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
The main automation script adapted for GitHub Codespaces:
- Reads implementation plans from local repository
- Executes 6-phase update process
- Creates Change Diaries automatically
- Uses Git for version control and backups
- No Google Drive dependencies

### 4.2 Six-Phase Execution Process

#### Phase 1: Discovery
- Scan repository for updates using Git status
- Check updates/ directory for new files
- Parse implementation plan for files to update
- Identify version conflicts

#### Phase 2: Documentation  
- Create Change Diary with Implementation ID
- Document all planned changes
- Generate pre-implementation report
- Create Git branch for changes

#### Phase 3: Preparation
- Create Git backup branch
- Save current state as Git tag
- Verify disk space
- Check system dependencies

#### Phase 4: Implementation
- Apply updates from implementation plan
- Copy new files to target locations
- Update version numbers
- Maintain Git history

#### Phase 5: Testing
- Run automated tests
- Verify file integrity
- Check service functionality
- Validate against specifications

#### Phase 6: Completion
- Update Change Diary with results
- Commit changes to Git
- Create pull request if configured
- Clean up temporary files

## 5. IMPLEMENTATION PLANS

### 5.1 Naming Convention
```
Implementation Plan - [IMPLEMENTATION_ID] - [DATE].md
```
Example: `Implementation Plan - IMP-2025-001 - 2025-01-13.md`

### 5.2 Required Sections
1. **Header** - Document metadata
2. **Objective** - Clear goal statement
3. **Scope** - What's included/excluded
4. **Risk Assessment** - Potential issues
5. **Files to Update** - Explicit file list with versions
6. **Testing Strategy** - How to validate
7. **Rollback Plan** - Recovery procedures
8. **Dependencies** - Required components

### 5.3 File Update Format
```markdown
## Files to Update
- news_service.py ‚Üí news_service_v114.py
- database_migration.py ‚Üí database_migration_v104.py
- config.json ‚Üí config_v203.json
```

## 6. CHANGE DIARIES

### 6.1 Naming Convention
```
Change Diary - [IMPLEMENTATION_ID] - [DATE].md
```
Must match the Implementation Plan ID.

### 6.2 Auto-Generated Content
- Implementation summary
- Phase-by-phase progress
- What's Next Task List
- Error logs and resolutions
- Final outcomes

## 7. ARTIFACT DELIVERY

### 7.1 File Naming Requirements
All updated files MUST include version suffix:
- Format: `[service_name]_v[version].[extension]`
- Example: `news_service_v114.py`

### 7.2 Version Tracking
- Increment version for each update
- Document version history in file header
- Maintain VERSION constant in code

## 8. GITHUB CODESPACES SPECIFIC FEATURES

### 8.1 Git Integration
```bash
# Create feature branch for implementation
git checkout -b implementation/IMP-2025-001

# Tag before implementation
git tag -a pre-IMP-2025-001 -m "Backup before IMP-2025-001"

# Commit after each phase
git commit -m "Phase 1: Discovery complete for IMP-2025-001"
```

### 8.2 GitHub Actions Integration
Optional workflow triggers:
- Auto-run tests on implementation
- Create PR after completion
- Notify team of changes

### 8.3 Local Development Commands
```bash
# Check implementation plan
./TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --check-only

# Execute with specific plan
./TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --plan "Implementation Plan - IMP-2025-001 - 2025-01-13.md"

# Continue interrupted process
./TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue

# Rollback if needed
./TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --rollback
```

## 9. ENVIRONMENT SETUP

### 9.1 Required Environment Variables
```bash
# In .env or Codespaces secrets
TRADING_SYSTEM_ENV=development
PROJECT_ROOT=/workspaces/trading-system
LOG_LEVEL=INFO
```

### 9.2 Dependencies
```bash
# Install Python dependencies
pip install -r requirements.txt

# Install system dependencies
sudo apt-get update
sudo apt-get install -y git python3-pip
```

## 10. ERROR HANDLING

### 10.1 Common Issues
1. **File Not Found**: Check repository structure
2. **Permission Denied**: Verify file permissions
3. **Git Conflicts**: Resolve before continuing
4. **Test Failures**: Check logs in logs/

### 10.2 Recovery Procedures
```bash
# Check current state
./TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --status

# View detailed logs
tail -f logs/update_process.log

# Rollback to previous state
git checkout pre-IMP-2025-001
```

## 11. BEST PRACTICES

### 11.1 Development Guidelines
1. Always create implementation plan before changes
2. Use meaningful Implementation IDs
3. Test in development branch first
4. Document all decisions in Change Diary
5. Commit frequently with clear messages

### 11.2 Code Review Process
1. Create PR after implementation
2. Link to Implementation Plan and Change Diary
3. Require approval before merging
4. Update main documentation after merge

### 11.3 Maintenance
- Regular repository cleanup
- Archive old change diaries
- Update this methodology as needed
- Monitor GitHub Actions for failures

## 12. COMPLIANCE CHECKLIST

Before starting any implementation:
- [ ] Functional Specification exists and is current
- [ ] Technical Specification is approved
- [ ] Implementation Plan created with unique ID
- [ ] Repository is up to date (`git pull`)
- [ ] Development branch created
- [ ] All tests passing on main branch

After implementation:
- [ ] Change Diary documents all phases
- [ ] All tests pass
- [ ] Code committed with proper messages
- [ ] PR created and linked to documentation
- [ ] Team notified of changes

================================================================================
FILE: ./project_documentation/Trading Application Architecturec.md
================================================================================

# Trading Application Architecture - GitHub Codespaces Edition

**Document: Trading Application Architecture**  
**Version: 3.1.2 (GitHub Codespaces Edition)**  
**Last Updated: 2025-06-23**  
**Platform: GitHub Codespaces**  
**Author: Trading System Architecture Team**  

## REVISION HISTORY
- v3.1.2 (2025-06-23) - Removed Google Drive dependencies, adapted for GitHub Codespaces
- v3.1.1 (2025-06-19) - Added database utilities layer for concurrent access handling
- v3.1.0 (2025-06-19) - Enhanced with database connection management and retry logic
- v3.0.0 (2025-06-15) - Hybrid architecture implementation with simplified service management
- v2.1.0 (2025-06-15) - Consolidated version combining runtime management concepts with 8-service architecture
- v2.0.0 (2025-06-14) - Complete rewrite with 8-service REST API architecture
- v1.0.0 (2025-06-01) - Initial 4-service architecture with threading runtime

## ARCHITECTURE EVOLUTION
This document represents the enhanced hybrid architecture that:
1. **Uses v2.0's 8 REST API services** with database utilities integration
2. **Implements database connection management** with automatic retry logic
3. **Provides concurrent access handling** through WAL mode and connection pooling
4. **Optimized for GitHub Codespaces** with persistent storage and integrated development

**Current Implementation**: Enhanced hybrid architecture for GitHub Codespaces

---

## Executive Summary

This document outlines the enhanced microservices-based architecture for an intelligent day trading system. The system uses the proven 8-service REST API architecture with a database utilities layer that provides robust concurrent access handling and automatic retry logic for database operations. The architecture is optimized for GitHub Codespaces, providing persistent storage and seamless development workflow.

### Current Architecture (v3.1.2 Enhanced)
- **8 Independent Services** with database utilities integration
- **Database Utilities Layer** providing connection management and retry logic
- **WAL Mode Database** for better concurrent access
- **Automatic Retry Logic** with exponential backoff for locked database handling
- **Connection Pooling** and proper transaction management
- **Professional Logging** with database operation tracking
- **GitHub Codespaces Optimized** with persistent file storage

### Key Enhancements
- **Reliability**: Automatic handling of database locking issues
- **Performance**: WAL mode enables concurrent reads while writing
- **Maintainability**: Centralized database operations through utilities
- **Debugging**: Comprehensive logging of database operations and retries
- **Development**: Seamless GitHub Codespaces integration

---

## Table of Contents

1. [System Overview](#1-system-overview)
2. [Architecture Principles](#2-architecture-principles)
3. [Current Service Architecture](#3-current-service-architecture)
4. [Database Architecture](#4-database-architecture)
5. [Database Utilities Layer](#5-database-utilities-layer)
6. [Service Integration Patterns](#6-service-integration-patterns)
7. [Error Handling and Recovery](#7-error-handling-and-recovery)
8. [Monitoring and Observability](#8-monitoring-and-observability)
9. [Deployment Strategy](#9-deployment-strategy)
10. [GitHub Codespaces Integration](#10-github-codespaces-integration)

---

## 1. System Overview

### 1.1 Architecture Type
**Enhanced Hybrid Microservices Architecture** designed for GitHub Codespaces

### 1.2 Core Components
1. **8 REST API Services** (ports 5000-5010)
2. **SQLite Database** with WAL mode
3. **Database Utilities Layer** with retry logic
4. **Hybrid Service Manager** for orchestration
5. **Web Dashboard** with real-time monitoring
6. **Automated Update Process** for maintenance
7. **Diagnostic Toolkit** for troubleshooting

### 1.3 Development Platform
- **Primary**: GitHub Codespaces
- **Storage**: Persistent workspace storage
- **Version Control**: Integrated Git workflow
- **Dependencies**: Managed via requirements.txt

---

## 2. Architecture Principles

### 2.1 Design Principles
1. **Service Independence**: Each service can run and fail independently
2. **Database Resilience**: Automatic retry for locked database scenarios
3. **Graceful Degradation**: System continues with reduced functionality
4. **Observable Operations**: Comprehensive logging and monitoring
5. **Automated Recovery**: Self-healing capabilities for common issues
6. **Codespaces Native**: Optimized for cloud development environment

### 2.2 Technology Stack
- **Language**: Python 3.10+
- **Web Framework**: Flask (REST APIs)
- **Database**: SQLite with WAL mode
- **Process Management**: subprocess with monitoring
- **Service Communication**: HTTP REST
- **Development Environment**: GitHub Codespaces

---

## 3. Current Service Architecture

### 3.1 Service Topology

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     GitHub Codespaces Workspace                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ Hybrid Manager  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Coordination    ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  (Orchestrator) ‚îÇ         ‚îÇ Service (5000)  ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                                       ‚îÇ                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ                Service Communication Bus              ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ         ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ      ‚îÇ            ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îê‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ    ‚îÇScanner‚îÇ‚îÇPattern‚îÇ‚îÇTech  ‚îÇ‚îÇTrade‚îÇ‚îÇNews ‚îÇ‚îÇPat ‚îÇ‚îÇReport‚îÇ     ‚îÇ
‚îÇ    ‚îÇ (5001)‚îÇ‚îÇ (5002)‚îÇ‚îÇ(5003)‚îÇ‚îÇ(5005)‚îÇ‚îÇ(5008)‚îÇ‚îÇRec‚îÇ‚îÇ(5009)‚îÇ     ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò‚îî‚î¨‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                            ‚îÇ                                    ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ                    ‚îÇ SQLite DB      ‚îÇ                          ‚îÇ
‚îÇ                    ‚îÇ (WAL Mode)     ‚îÇ                          ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ Web Dashboard  ‚îÇ                  ‚îÇ Database Utils   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   (Port 5010)  ‚îÇ                  ‚îÇ (Retry Logic)    ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Service Registry

| Service | Port | Purpose | Critical |
|---------|------|---------|----------|
| Coordination | 5000 | Workflow orchestration | Yes |
| Security Scanner | 5001 | Market scanning | Yes |
| Pattern Analysis | 5002 | Technical patterns | Yes |
| Technical Analysis | 5003 | Indicators & signals | Yes |
| Paper Trading | 5005 | Trade execution | Yes |
| Pattern Recognition | 5006 | Advanced patterns | No |
| News Service | 5008 | Sentiment analysis | No |
| Reporting | 5009 | Analytics & reports | No |
| Web Dashboard | 5010 | User interface | Yes |

---

## 4. Database Architecture

### 4.1 Database Configuration
```python
DATABASE_CONFIG = {
    'path': '/workspaces/trading-system/trading_system.db',
    'mode': 'WAL',  # Write-Ahead Logging
    'journal_mode': 'WAL',
    'synchronous': 'NORMAL',
    'cache_size': -64000,  # 64MB
    'busy_timeout': 30000,  # 30 seconds
    'foreign_keys': 'ON'
}
```

### 4.2 Schema Design
The database schema includes:
- Service coordination tables
- Trading data tables
- Pattern analysis results
- Performance metrics
- System logs and audit trails

---

## 5. Database Utilities Layer

### 5.1 DatabaseServiceMixin Class
```python
class DatabaseServiceMixin:
    """Provides database operations with automatic retry logic"""
    
    def get_db_connection(self, retries=5, timeout=30):
        """Get database connection with retry logic"""
        
    def execute_with_retry(self, query, params=None, retries=5):
        """Execute query with automatic retry on lock"""
        
    def bulk_insert_with_transaction(self, table, records):
        """Bulk insert with transaction management"""
```

### 5.2 Key Features
1. **Automatic Retry**: Exponential backoff for locked database
2. **Connection Pooling**: Reuse connections efficiently
3. **Transaction Management**: Proper commit/rollback handling
4. **Error Recovery**: Graceful handling of database errors
5. **Performance Monitoring**: Track query execution times

---

## 6. Service Integration Patterns

### 6.1 Communication Flow
```
User Request ‚Üí Web Dashboard ‚Üí Coordination Service ‚Üí Individual Services ‚Üí Database
```

### 6.2 Integration Patterns
1. **Service Discovery**: Services register on startup
2. **Health Checks**: Regular endpoint monitoring
3. **Circuit Breaker**: Prevent cascade failures
4. **Retry Logic**: Automatic retry for transient failures
5. **Timeout Management**: Configurable timeouts per service

---

## 7. Error Handling and Recovery

### 7.1 Error Categories
1. **Database Locks**: Automatic retry with backoff
2. **Service Failures**: Restart with monitoring
3. **Network Issues**: Timeout and retry
4. **Data Validation**: Input sanitization
5. **System Resources**: Resource monitoring

### 7.2 Recovery Strategies
```python
RECOVERY_STRATEGIES = {
    'database_locked': 'retry_with_backoff',
    'service_down': 'restart_service',
    'network_timeout': 'retry_request',
    'invalid_data': 'log_and_skip',
    'resource_exhausted': 'throttle_requests'
}
```

---

## 8. Monitoring and Observability

### 8.1 Logging Architecture
```
/workspaces/trading-system/logs/
‚îú‚îÄ‚îÄ coordination_service.log
‚îú‚îÄ‚îÄ security_scanner.log
‚îú‚îÄ‚îÄ pattern_analysis.log
‚îú‚îÄ‚îÄ technical_analysis.log
‚îú‚îÄ‚îÄ paper_trading.log
‚îú‚îÄ‚îÄ news_service.log
‚îú‚îÄ‚îÄ reporting_service.log
‚îú‚îÄ‚îÄ web_dashboard.log
‚îú‚îÄ‚îÄ database_operations.log
‚îî‚îÄ‚îÄ system_health.log
```

### 8.2 Metrics Collection
- Service health status
- Response times
- Error rates
- Database performance
- Trading performance

---

## 9. Deployment Strategy

### 9.1 GitHub Codespaces Setup
```bash
# Clone repository
git clone https://github.com/yourusername/trading-system.git

# Install dependencies
pip install -r requirements.txt

# Initialize database
python database_migration.py

# Start system
python setup_trading_system.py
```

### 9.2 Service Startup Sequence
1. Database migration and validation
2. Start Coordination Service
3. Start critical trading services
4. Start auxiliary services
5. Start Web Dashboard
6. Verify health status

---

## 10. GitHub Codespaces Integration

### 10.1 Workspace Structure
```
/workspaces/trading-system/
‚îú‚îÄ‚îÄ services/              # All service files
‚îú‚îÄ‚îÄ logs/                  # Service logs
‚îú‚îÄ‚îÄ data/                  # Database and data files
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ tests/                 # Test suites
‚îú‚îÄ‚îÄ .devcontainer/         # Codespaces configuration
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies
```

### 10.2 Development Workflow
1. **Code Changes**: Direct editing in Codespaces
2. **Testing**: Run tests in integrated terminal
3. **Debugging**: Use VS Code debugger
4. **Version Control**: Integrated Git commands
5. **Deployment**: Push to repository

### 10.3 Persistence
- All files in `/workspaces` are persistent
- Database stored in workspace
- Logs maintained across sessions
- Configuration preserved

### 10.4 Environment Variables
```bash
# Set in Codespaces settings or .env file
TRADING_ENV=development
DATABASE_PATH=/workspaces/trading-system/trading_system.db
LOG_LEVEL=INFO
ALPACA_PAPER_API_KEY=your_key
ALPACA_PAPER_API_SECRET=your_secret
```

---

## Architecture Benefits

### For GitHub Codespaces
1. **No External Dependencies**: Everything runs in workspace
2. **Persistent Storage**: Files preserved across sessions
3. **Integrated Development**: Full IDE features
4. **Version Control**: Seamless Git integration
5. **Scalable Resources**: Adjustable compute power

### For Trading System
1. **High Reliability**: Automatic error recovery
2. **Performance**: Optimized database operations
3. **Maintainability**: Clear service boundaries
4. **Observability**: Comprehensive logging
5. **Flexibility**: Easy to extend and modify

---

## Migration from Google Colab

### Key Changes
1. **Storage**: Local workspace instead of Google Drive
2. **Dependencies**: requirements.txt instead of pip installs
3. **Configuration**: .env files instead of Colab secrets
4. **Startup**: Direct execution instead of mounting drives
5. **Persistence**: Workspace storage instead of Drive backups

### Migration Steps
1. Copy all service files to repository
2. Update file paths to use workspace paths
3. Remove Google Drive mounting code
4. Update configuration for local storage
5. Test all services in Codespaces

---

## Conclusion

This architecture provides a robust, scalable, and maintainable trading system optimized for GitHub Codespaces. The removal of Google Drive dependencies simplifies deployment and improves performance while maintaining all critical functionality.

================================================================================
FILE: ./project_documentation/Trading System Functional Specification - Service Classes and Procedures.md
================================================================================

# Trading System Functional Specification - Service Classes and Procedures

**Name of Service**: TRADING SYSTEM FUNCTIONAL SPECIFICATION - SERVICE CLASSES AND PROCEDURES  
**Version**: 1.1.0 (GitHub Codespaces Edition)  
**Last Updated**: 2025-06-23  
**Platform**: GitHub Codespaces  

## REVISION HISTORY
- v1.1.0 (2025-06-23) - Updated for GitHub Codespaces environment, removed Google Drive/Colab dependencies
- v1.0.0 (2025-06-15) - Initial functional specification documenting all service classes and procedures

---

## Executive Summary

This document provides a comprehensive technical reference for all services in the Trading System v3.0 Hybrid Architecture, now optimized for GitHub Codespaces. It catalogs each service Python file, the classes they contain, their methods/procedures, and describes the high-level functional flows that enable automated trading operations.

**Implementation Note**: The system uses manual mathematical implementations for all technical analysis and pattern detection. While the code includes optional TA-Lib support, the current implementation relies on custom algorithms for RSI, MACD, Bollinger Bands, moving averages, and candlestick pattern recognition.

**Platform Note**: All services are designed to run within the GitHub Codespaces environment with persistent workspace storage, integrated Git version control, and no external cloud storage dependencies.

---

## Table of Contents

1. [Core Trading Services](#1-core-trading-services)
2. [Supporting Services](#2-supporting-services)
3. [Management Services](#3-management-services)
4. [High-Level Functional Flows](#4-high-level-functional-flows)
5. [Service Integration Patterns](#5-service-integration-patterns)

---

## 1. Core Trading Services

### 1.1 coordination_service.py

**Main Class**: `CoordinationService`

**Purpose**: Central orchestrator managing the entire trading workflow and service coordination.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes coordination service, sets up Flask app and service registry
  - Default db_path: `/workspaces/trading-system/trading_system.db`
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/coordination_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for workflow control
- **`_call_service(self, service_name, endpoint, method, data)`**: HTTP client for inter-service communication
- **`_save_service_registration(self, service_name, port)`**: Persists service registration to database
- **`_start_trading_cycle_record(self, cycle_id)`**: Creates new trading cycle record in database
- **`_update_cycle_progress(self, cycle_id, **kwargs)`**: Updates trading cycle progress metrics
- **`_complete_trading_cycle(self, cycle_id)`**: Marks trading cycle as completed
- **`_get_recent_cycles(self)`**: Retrieves recent trading cycle history
- **`run(self)`**: Starts Flask web server on port 5000

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/register_service`**: Service registration endpoint
- **`/start_trading_cycle`**: Initiates complete trading workflow
- **`/service_status`**: Returns registered service status
- **`/trading_cycles`**: Returns recent trading cycle history

#### High-Level Function - Trading Cycle Orchestration:
1. **Cycle Initialization**: Creates unique cycle ID and database record
2. **Security Scanning**: Calls security scanner to identify trading candidates
3. **Pattern Analysis**: Requests pattern analysis for each security
4. **Signal Generation**: Calls technical analysis to generate trading signals
5. **Trade Execution**: Submits signals to paper trading service
6. **Cycle Completion**: Updates database with final results and metrics

---

### 1.2 security_scanner.py

**Main Class**: `SecurityScannerService`

**Purpose**: Scans market for securities meeting predefined trading criteria.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes scanner with trading criteria and database connection
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/security_scanner_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for scanning operations
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_scan_securities(self)`**: Main scanning logic that evaluates watchlist securities
- **`_get_watchlist(self)`**: Returns list of symbols to analyze
- **`_analyze_security(self, symbol)`**: Performs detailed analysis of individual security
- **`_meets_criteria(self, security)`**: Evaluates if security meets trading criteria
- **`_get_news_sentiment(self, symbol)`**: Retrieves sentiment analysis from news service
- **`_save_selected_security(self, security_data)`**: Persists selected security to database
- **`run(self)`**: Starts Flask web server on port 5001

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/scan_securities`**: Main scanning endpoint
- **`/criteria`**: Get/update trading criteria

#### High-Level Function - Security Selection:
1. **Watchlist Evaluation**: Iterates through predefined symbol list
2. **Market Data Retrieval**: Fetches current and historical price/volume data
3. **Criteria Assessment**: Applies trading rules (price range, volume, momentum)
4. **Sentiment Integration**: Incorporates news sentiment analysis
5. **Selection Persistence**: Saves qualifying securities to database with selection rationale

---

### 1.3 pattern_analysis.py

**Main Class**: `PatternAnalysisService`

**Purpose**: Detects technical patterns in price data using manual calculation methods and pattern recognition algorithms.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes pattern analysis with manual pattern detection
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/pattern_analysis_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for pattern analysis
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_analyze_patterns(self, symbol)`**: Main pattern analysis orchestrator
- **`_get_historical_data(self, symbol, period)`**: Retrieves market data for analysis
- **`_detect_basic_patterns_fallback(self, symbol, data)`**: Manual pattern detection using price movement analysis
- **`_detect_basic_patterns_talib(self, symbol, data)`**: Optional TA-Lib patterns (not used in current implementation)
- **`_get_enhanced_patterns(self, symbol)`**: Calls pattern recognition service for advanced patterns
- **`_calculate_confidence(self, patterns)`**: Computes overall pattern confidence score
- **`_save_pattern_analysis(self, symbol, analysis_data)`**: Persists analysis to database
- **`_get_supported_patterns(self)`**: Returns list of supported pattern types
- **`run(self)`**: Starts Flask web server on port 5002

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/analyze_patterns/<symbol>`**: Main pattern analysis endpoint
- **`/supported_patterns`**: Lists available pattern types

#### High-Level Function - Pattern Detection:
1. **Data Acquisition**: Retrieves 30-day historical price data
2. **Manual Pattern Analysis**: Applies mathematical pattern detection algorithms (Doji, Hammer, Shooting Star, trend analysis)
3. **Price Movement Analysis**: Analyzes body-to-shadow ratios, trend slopes, and momentum indicators
4. **Enhanced Pattern Integration**: Calls advanced pattern recognition service
5. **Confidence Scoring**: Calculates weighted confidence based on pattern strength
6. **Result Compilation**: Combines all patterns with metadata and saves to database

---

### 1.4 technical_analysis.py

**Main Class**: `TechnicalAnalysisService`

**Purpose**: Generates BUY/SELL/HOLD trading signals using manual technical indicator calculations and rule-based analysis.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes technical analysis with manual indicator calculations
- **`_init_ml_models(self)`**: Sets up RandomForest and GradientBoosting models if scikit-learn available
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/technical_analysis_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for signal generation
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_generate_signals(self, securities_with_patterns)`**: Main signal generation orchestrator
- **`_analyze_single_security(self, security)`**: Analyzes individual security for trading signals
- **`_get_market_data(self, symbol)`**: Retrieves market data for indicator calculation
- **`_calculate_indicators_manual(self, data)`**: Manual calculation of RSI, MACD, Bollinger Bands, moving averages
- **`_calculate_indicators_talib(self, data)`**: Optional TA-Lib indicators (not used in current implementation)
- **`_generate_rule_based_signal(self, symbol, indicators, patterns)`**: Applies trading rules to generate signals
- **`_save_trading_signal(self, symbol, signal_data)`**: Persists trading signal to database
- **`run(self)`**: Starts Flask web server on port 5003

#### Key REST Endpoints:
- **`/health`**: Service health check with ML/scikit-learn availability
- **`/generate_signals`**: Main signal generation endpoint
- **`/analyze/<symbol>`**: Single symbol analysis endpoint

#### High-Level Function - Signal Generation:
1. **Market Data Processing**: Retrieves and validates price/volume data
2. **Manual Technical Indicator Calculation**: Computes RSI using delta analysis, MACD using EMA calculations, Bollinger Bands using standard deviation, and moving averages
3. **Pattern Integration**: Incorporates pattern analysis results
4. **Rule-Based Analysis**: Applies scoring algorithm based on indicator convergence and thresholds
5. **Signal Classification**: Determines BUY/SELL/HOLD with confidence level based on combined indicator signals
6. **Signal Persistence**: Saves signals to database with supporting rationale

---

### 1.5 paper_trading.py

**Main Class**: `PaperTradingService`

**Purpose**: Executes trades using Alpaca Paper Trading API or simulation mode.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes paper trading with Alpaca API configuration
- **`_setup_alpaca_api(self)`**: Establishes connection to Alpaca Paper Trading API
  - API credentials stored in Codespaces secrets or .env file
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/paper_trading_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for trade execution
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_execute_trades(self, trading_signals)`**: Main trade execution orchestrator
- **`_execute_single_trade(self, signal)`**: Executes individual trade via Alpaca API
- **`_simulate_trade_execution(self, signal)`**: Simulates trade when API unavailable
- **`_save_trade_record(self, trade_data)`**: Persists trade record to database
- **`_get_account_info(self)`**: Retrieves account balance and status
- **`_get_positions(self)`**: Returns current portfolio positions
- **`run(self)`**: Starts Flask web server on port 5005

#### Key REST Endpoints:
- **`/health`**: Service health check with Alpaca connection status
- **`/execute_trades`**: Main trade execution endpoint
- **`/account`**: Account information endpoint
- **`/positions`**: Current positions endpoint

#### High-Level Function - Trade Execution:
1. **Signal Processing**: Validates incoming trading signals
2. **Account Verification**: Checks available buying power and account status
3. **Price Discovery**: Retrieves current market price for each symbol
4. **Order Placement**: Submits market orders via Alpaca Paper Trading API
5. **Trade Recording**: Saves execution details to database
6. **Portfolio Tracking**: Updates position and P&L calculations

---

## 2. Supporting Services

### 2.1 pattern_recognition_service.py

**Main Class**: `PatternRecognitionService`

**Purpose**: Advanced pattern detection using ML techniques and comprehensive technical analysis.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes advanced pattern recognition
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/pattern_recognition_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for advanced pattern detection
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_detect_advanced_patterns(self, symbol)`**: Main advanced pattern detection orchestrator
- **`_get_market_data(self, symbol, period)`**: Retrieves extended market data
- **`_detect_candlestick_patterns(self, symbol, data)`**: Manual candlestick pattern detection using price analysis algorithms
- **`_detect_chart_patterns(self, symbol, data)`**: Support/resistance and trend pattern detection
- **`_detect_volume_patterns(self, symbol, data)`**: Volume-based pattern analysis
- **`_calculate_obv(self, prices, volumes)`**: On-Balance Volume calculation
- **`_calculate_pattern_score(self, candlestick, chart, volume)`**: Weighted pattern scoring
- **`_save_pattern_analysis(self, analysis_data)`**: Persists advanced patterns to database
- **`run(self)`**: Starts Flask web server on port 5006

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/detect_advanced_patterns/<symbol>`**: Main advanced pattern endpoint
- **`/candlestick_patterns/<symbol>`**: Candlestick pattern analysis
- **`/chart_patterns/<symbol>`**: Chart pattern analysis

#### High-Level Function - Advanced Pattern Recognition:
1. **Extended Data Analysis**: Analyzes multiple timeframes and extended history
2. **Multi-Dimensional Pattern Detection**: Combines candlestick, chart, and volume patterns
3. **Pattern Validation**: Cross-validates patterns using multiple techniques
4. **Confidence Scoring**: Applies weighted scoring across pattern categories
5. **Pattern Persistence**: Saves comprehensive pattern analysis with metadata

---

### 2.2 news_service.py

**Main Class**: `NewsService`

**Purpose**: Provides sentiment analysis of news articles using NLP techniques.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes news service with sentiment analysis capabilities
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/news_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for sentiment analysis
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_analyze_news_sentiment(self, symbol)`**: Main sentiment analysis orchestrator
- **`_analyze_text_sentiment(self, text)`**: Multi-method text sentiment analysis
- **`_keyword_based_sentiment(self, text)`**: Financial keyword sentiment analysis
- **`_save_sentiment_analysis(self, sentiment_data)`**: Persists sentiment data to database
- **`run(self)`**: Starts Flask web server on port 5008

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/news_sentiment/<symbol>`**: Sentiment analysis for specific symbol
- **`/bulk_news_sentiment`**: Bulk sentiment analysis endpoint

#### High-Level Function - News Sentiment Analysis:
1. **News Retrieval**: Fetches recent news articles via Yahoo Finance API
2. **Content Processing**: Extracts and cleans article titles and summaries
3. **Sentiment Analysis**: Applies TextBlob and keyword-based sentiment scoring
4. **Sentiment Aggregation**: Calculates weighted average sentiment scores
5. **Sentiment Classification**: Assigns positive/negative/neutral labels with confidence

---

### 2.3 reporting_service.py

**Main Class**: `ReportingService`

**Purpose**: Generates comprehensive trading analytics, performance reports, and system health metrics.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes reporting service with database analytics
- **`_setup_logging(self)`**: Configures logging to `/workspaces/trading-system/logs/reporting_service.log`
- **`_setup_routes(self)`**: Defines REST API endpoints for various reports
- **`_register_with_coordination(self)`**: Registers service with coordination service
- **`_generate_daily_summary(self)`**: Creates daily trading activity summary
- **`_analyze_pattern_effectiveness(self)`**: Analyzes which patterns generate best returns
- **`_generate_performance_report(self, days)`**: Creates comprehensive performance analytics
- **`_analyze_trading_cycles(self)`**: Analyzes trading cycle efficiency and success rates
- **`_generate_system_health_report(self)`**: Creates system and service health report
- **`run(self)`**: Starts Flask web server on port 5009

#### Key REST Endpoints:
- **`/health`**: Service health check
- **`/daily_summary`**: Daily trading summary report
- **`/pattern_effectiveness`**: Pattern performance analysis
- **`/trading_performance`**: Comprehensive performance report
- **`/system_health`**: System health and service status
- **`/cycle_analysis`**: Trading cycle analysis

#### High-Level Function - Performance Analytics:
1. **Data Aggregation**: Collects trading data across multiple timeframes
2. **Performance Calculation**: Computes win rates, P&L, Sharpe ratios, drawdowns
3. **Pattern Analysis**: Evaluates effectiveness of different trading patterns
4. **System Monitoring**: Tracks service health and system performance metrics
5. **Report Generation**: Creates formatted reports for stakeholder consumption

---

## 3. Management Services

### 3.1 database_migration.py

**Main Class**: `DatabaseMigration`

**Purpose**: Manages database schema creation, migrations, and initial data seeding.

#### Class Methods:
- **`__init__(self, db_path)`**: Initializes database migration handler
  - Default path: `/workspaces/trading-system/trading_system.db`
- **`ensure_directories(self)`**: Creates required directory structure in workspace
- **`get_connection(self)`**: Returns SQLite connection with foreign keys enabled
- **`create_tables(self)`**: Creates all required database tables
- **`seed_initial_data(self)`**: Populates database with initial configuration data
- **`backup_database(self)`**: Creates timestamped database backups in `/workspaces/trading-system/backups/`
- **`verify_schema(self)`**: Validates that all required tables exist
- **`get_schema_info(self)`**: Returns detailed database schema information
- **`run_migration(self)`**: Executes complete migration process
- **`main(self)`**: Command-line entry point for migration

#### High-Level Function - Database Initialization:
1. **Schema Creation**: Creates all required tables with proper relationships
2. **Index Creation**: Adds performance indexes for key queries
3. **Initial Data Seeding**: Populates service registry and configuration tables
4. **Backup Creation**: Creates safety backup before any migrations (stored in workspace)
5. **Schema Validation**: Verifies database integrity after migration

---

### 3.2 hybrid_manager.py

**Main Class**: Multiple classes in hybrid_components package

**Purpose**: Automated service lifecycle management optimized for GitHub Codespaces.

#### Main Classes:
- **`HybridServiceManager`**: Primary service orchestrator
- **`LifecycleManager`**: Service start/stop/restart management
- **`RecoveryManager`**: Checkpoint and recovery functionality  
- **`MonitoringEngine`**: Health monitoring and auto-restart
- **`ConfigurationManager`**: System and service configuration

#### Key Methods (HybridServiceManager):
- **`__init__(self, config)`**: Initializes hybrid manager with configuration
  - Config path: `/workspaces/trading-system/config/`
- **`start(self, recovery_mode)`**: Starts complete trading system
- **`stop(self)`**: Graceful shutdown of all services
- **`restart_service(self, service_name)`**: Restarts specific service
- **`get_status(self)`**: Returns comprehensive system status
- **`_run_database_migration(self)`**: Executes database setup
- **`_save_checkpoint(self)`**: Saves system state to workspace (Git-tracked)
- **`_load_checkpoint(self)`**: Loads previous system state

#### High-Level Function - System Management:
1. **Service Orchestration**: Starts services in dependency order
2. **Health Monitoring**: Continuously monitors service health via HTTP endpoints
3. **Auto-Recovery**: Automatically restarts failed services with exponential backoff
4. **State Persistence**: Saves system checkpoints to workspace for recovery
5. **Resource Management**: Manages subprocess lifecycle and resource cleanup

---

## 4. High-Level Functional Flows

### 4.1 Complete Trading Cycle Flow

**Orchestrator**: CoordinationService

**Flow Steps**:
1. **Cycle Initiation** ‚Üí CoordinationService creates unique cycle ID
2. **Security Selection** ‚Üí SecurityScannerService identifies trading candidates  
3. **Pattern Analysis** ‚Üí PatternAnalysisService detects technical patterns
4. **Signal Generation** ‚Üí TechnicalAnalysisService creates trading signals
5. **Trade Execution** ‚Üí PaperTradingService executes trades via Alpaca API
6. **Cycle Completion** ‚Üí CoordinationService updates final metrics

**Data Flow**: Database ‚Üí Services ‚Üí Database (with HTTP REST API communication)
**Storage**: All data persisted in `/workspaces/trading-system/trading_system.db`

---

### 4.2 System Startup Flow

**Orchestrator**: HybridServiceManager

**Flow Steps**:
1. **Environment Setup** ‚Üí Validates GitHub Codespaces workspace structure
2. **Database Migration** ‚Üí Ensures schema is current
3. **Configuration Loading** ‚Üí Loads service and system configurations from workspace
4. **Service Startup** ‚Üí Starts services in dependency order with health checks
5. **Registration** ‚Üí Services register with coordination service
6. **Monitoring Activation** ‚Üí Begins health monitoring and checkpoint saving

**Workspace Requirements**: Persistent storage in `/workspaces/trading-system/`

---

### 4.3 Pattern Analysis Flow

**Orchestrator**: PatternAnalysisService

**Flow Steps**:
1. **Data Acquisition** ‚Üí Retrieves 30-day price/volume history
2. **Manual Pattern Detection** ‚Üí Applies mathematical pattern recognition algorithms
3. **Advanced Pattern Integration** ‚Üí Calls PatternRecognitionService
4. **Confidence Calculation** ‚Üí Weighted scoring across pattern types
5. **Result Persistence** ‚Üí Saves comprehensive analysis to database

---

### 4.4 Trade Execution Flow

**Orchestrator**: PaperTradingService

**Flow Steps**:
1. **Signal Validation** ‚Üí Verifies signal format and requirements
2. **Account Verification** ‚Üí Checks buying power and account status
3. **Price Discovery** ‚Üí Retrieves current market prices
4. **Order Placement** ‚Üí Submits market orders via Alpaca Paper API
5. **Trade Recording** ‚Üí Persists execution details and updates portfolio

**API Credentials**: Stored in Codespaces secrets or .env file

---

## 5. Service Integration Patterns

### 5.1 Service Registration Pattern
All services register with CoordinationService on startup, providing:
- Service name and port
- Health check endpoint
- Version information
- Dependency requirements

### 5.2 REST API Communication Pattern
Services communicate via HTTP REST APIs with:
- JSON request/response format
- Standardized error handling
- Timeout configuration
- Retry logic for critical operations

### 5.3 Database Integration Pattern
All services share SQLite database with:
- Connection pooling
- Transaction management
- Foreign key relationships
- Indexed queries for performance
- WAL mode for concurrent access

### 5.4 Health Monitoring Pattern
HybridServiceManager monitors services via:
- Process status checks
- HTTP health endpoint validation
- Restart policies with exponential backoff
- Critical vs non-critical service designation

### 5.5 Error Handling Pattern
Services implement consistent error handling:
- Structured logging with levels
- Graceful degradation for non-critical failures
- Error propagation to coordination service
- Recovery mechanisms for transient failures

### 5.6 GitHub Codespaces Integration Pattern
All services utilize Codespaces features:
- Persistent workspace storage (no external drives)
- Integrated terminal for service management
- VS Code debugging capabilities
- Git-based version control for configurations
- Environment variables via Codespaces secrets

---

**Document Status**: Complete Technical Reference - GitHub Codespaces Edition  
**Coverage**: All 9 services with detailed class and method documentation  
**Integration**: Comprehensive functional flow documentation for GitHub Codespaces environment  
**Storage**: All data persisted in `/workspaces/trading-system/` workspace

================================================================================
FILE: ./Diagnosis/Diagnosis_Full_Reporting.py
================================================================================

import os
import sys


# Configuration
GOOGLE_DRIVE_SOURCE = '/content/drive/MyDrive/Business/Trade/software/Code'
COLAB_BASE_DIR = '/content/trading_system'
DIAGNOSIS_SOURCE = '/content/drive/Business/Trade/software/Code/Diagnosis'

#Mount Google Drive
"""Mount Google Drive in Colab"""
print("\nüìÅ Mounting Google Drive...")
try:
	from google.colab import drive
        drive.mount('/content/drive', force_remount=True)
        print("‚úÖ Google Drive mounted successfully")
        return True
except ImportError:
        print("‚ùå Not running in Google Colab environment")
        return False
except Exception as e:
        print(f"‚ùå Error mounting Google Drive: {str(e)}")
        return False



================================================================================
FILE: ./Diagnosis/Run_Diagnosis_Toolkit.py
================================================================================

'''
#run Diagnosis toolk
#has different levels of diagnosis
python3 diagnostic_toolkit.py --quick
python3 diagnostic_toolkit.py --report
python3 diagnostic_toolkit.py --service pattern --report
python3 diagnostic_toolkit.py --service pattern --report && echo "Report saved to /content/diagnostic_reports/"

These 4 options require it to have the following format (expample shows the full i.e no parameter like --quick):

!cd /conten./Diagnosis && python3 diagnostic_toolkit.py

'''

!cd /conten./Diagnosis && python3 diagnostic_toolkit.py


================================================================================
FILE: ./Diagnosis/diagnostic_log_analysis.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC - LOG ANALYSIS
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic script for log analysis and error detection

USAGE: python diagnostic_log_analysis.py [--service SERVICE_NAME] [--errors-only] [--last-minutes N]
PURPOSE: Analyze service logs for errors, patterns, and performance issues
"""

import os
import re
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import defaultdict

class LogAnalysisDiagnostic:
    """Analyze trading system service logs for issues and patterns"""
    
    def __init__(self):
        self.log_dir = Path('/content/logs')
        self.log_files = {
            'coordination': 'coordination_service.log',
            'scanner': 'security_scanner.log',
            'pattern': 'pattern_analysis_service.log', 
            'technical': 'technical_analysis_service.log',
            'trading': 'paper_trading_service.log',
            'pattern_rec': 'pattern_recognition_service.log',
            'news': 'news_service.log',
            'reporting': 'reporting_service.log',
            'dashboard': 'web_dashboard_service.log'
        }
        
        # Error patterns to look for
        self.error_patterns = {
            'critical': [
                r'CRITICAL',
                r'FATAL',
                r'Exception.*not.*serializable',
                r'Connection refused',
                r'Database.*error',
                r'Failed to start'
            ],
            'error': [
                r'ERROR',
                r'Exception',
                r'Traceback',
                r'Failed',
                r'Could not',
                r'Unable to'
            ],
            'warning': [
                r'WARNING',
                r'WARN',
                r'deprecated',
                r'timeout',
                r'retry'
            ],
            'performance': [
                r'slow',
                r'timeout',
                r'took.*\d+.*seconds',
                r'response.*time.*\d+ms'
            ]
        }
    
    def run_full_analysis(self, service_filter: Optional[str] = None, 
                         errors_only: bool = False, 
                         last_minutes: Optional[int] = None) -> Dict:
        """Run complete log analysis"""
        print("üìã TRADING SYSTEM LOG ANALYSIS")
        print("=" * 50)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if service_filter:
            print(f"Service Filter: {service_filter}")
        if errors_only:
            print("Mode: Errors only")
        if last_minutes:
            print(f"Time Filter: Last {last_minutes} minutes")
        
        results = {}
        
        # 1. Check log file status
        file_status = self.check_log_files()
        results["file_status"] = file_status
        
        # 2. Analyze each service's logs
        service_analyses = {}
        services_to_analyze = [service_filter] if service_filter else self.log_files.keys()
        
        for service in services_to_analyze:
            if service in self.log_files:
                analysis = self.analyze_service_log(service, errors_only, last_minutes)
                service_analyses[service] = analysis
        
        results["service_analyses"] = service_analyses
        
        # 3. Cross-service analysis
        cross_analysis = self.cross_service_analysis(service_analyses)
        results["cross_analysis"] = cross_analysis
        
        # 4. Generate summary
        summary = self.generate_summary(results)
        results["summary"] = summary
        
        return results
    
    def check_log_files(self) -> Dict:
        """Check status of all log files"""
        print(f"\nüìÅ LOG FILE STATUS:")
        print("-" * 30)
        
        file_status = {}
        
        if not self.log_dir.exists():
            print("‚ùå Log directory does not exist!")
            return {"error": "Log directory missing"}
        
        for service, filename in self.log_files.items():
            log_path = self.log_dir / filename
            
            if log_path.exists():
                try:
                    stat = log_path.stat()
                    size_kb = stat.st_size / 1024
                    modified = datetime.fromtimestamp(stat.st_mtime)
                    age_minutes = (datetime.now() - modified).total_seconds() / 60
                    
                    file_status[service] = {
                        "exists": True,
                        "size_kb": round(size_kb, 1),
                        "last_modified": modified.strftime('%Y-%m-%d %H:%M:%S'),
                        "age_minutes": round(age_minutes, 1)
                    }
                    
                    age_indicator = "üü¢" if age_minutes < 5 else "üü°" if age_minutes < 60 else "üî¥"
                    print(f"{age_indicator} {service:12}: {size_kb:6.1f}KB, {age_minutes:4.1f}min ago")
                
                except Exception as e:
                    file_status[service] = {"exists": True, "error": str(e)}
                    print(f"‚ùå {service:12}: Error reading file - {e}")
            
            else:
                file_status[service] = {"exists": False}
                print(f"‚ùå {service:12}: Log file not found")
        
        return file_status
    
    def analyze_service_log(self, service: str, errors_only: bool = False, 
                           last_minutes: Optional[int] = None) -> Dict:
        """Analyze a single service's log file"""
        log_path = self.log_dir / self.log_files[service]
        
        analysis = {
            "service": service,
            "log_file": str(log_path),
            "total_lines": 0,
            "error_counts": defaultdict(int),
            "recent_errors": [],
            "performance_issues": [],
            "patterns": defaultdict(int),
            "status": "unknown"
        }
        
        if not log_path.exists():
            analysis["status"] = "no_log_file"
            return analysis
        
        try:
            # Calculate time filter
            time_filter = None
            if last_minutes:
                time_filter = datetime.now() - timedelta(minutes=last_minutes)
            
            with open(log_path, 'r') as f:
                lines = f.readlines()
            
            analysis["total_lines"] = len(lines)
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue
                
                # Parse timestamp if available
                log_time = self.parse_log_timestamp(line)
                
                # Apply time filter
                if time_filter and log_time and log_time < time_filter:
                    continue
                
                # Skip non-error lines if errors_only is True
                if errors_only and not any(re.search(pattern, line, re.IGNORECASE) 
                                         for patterns in self.error_patterns.values() 
                                         for pattern in patterns):
                    continue
                
                # Categorize the line
                for category, patterns in self.error_patterns.items():
                    for pattern in patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            analysis["error_counts"][category] += 1
                            
                            # Store recent critical errors and errors
                            if category in ['critical', 'error'] and len(analysis["recent_errors"]) < 10:
                                analysis["recent_errors"].append({
                                    "line_number": line_num,
                                    "timestamp": log_time.isoformat() if log_time else "unknown",
                                    "category": category,
                                    "message": line
                                })
                            
                            # Store performance issues
                            if category == 'performance' and len(analysis["performance_issues"]) < 5:
                                analysis["performance_issues"].append({
                                    "line_number": line_num,
                                    "timestamp": log_time.isoformat() if log_time else "unknown",
                                    "message": line
                                })
                            
                            break
                
                # Look for specific patterns
                if "JSON serializable" in line:
                    analysis["patterns"]["json_serialization_errors"] += 1
                elif "websockets" in line.lower():
                    analysis["patterns"]["websockets_issues"] += 1
                elif "alpaca" in line.lower():
                    analysis["patterns"]["alpaca_mentions"] += 1
                elif "yfinance" in line.lower():
                    analysis["patterns"]["yfinance_mentions"] += 1
            
            # Determine status
            if analysis["error_counts"]["critical"] > 0:
                analysis["status"] = "critical_errors"
            elif analysis["error_counts"]["error"] > 0:
                analysis["status"] = "errors_found" 
            elif analysis["error_counts"]["warning"] > 0:
                analysis["status"] = "warnings_only"
            else:
                analysis["status"] = "clean"
        
        except Exception as e:
            analysis["status"] = "analysis_error"
            analysis["error"] = str(e)
        
        return analysis
    
    def parse_log_timestamp(self, line: str) -> Optional[datetime]:
        """Parse timestamp from log line"""
        # Common log timestamp patterns
        patterns = [
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',  # 2025-06-17 14:30:45
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',  # 2025-06-17T14:30:45
        ]
        
        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                try:
                    timestamp_str = match.group(1)
                    if 'T' in timestamp_str:
                        return datetime.fromisoformat(timestamp_str)
                    else:
                        return datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                except:
                    continue
        
        return None
    
    def cross_service_analysis(self, service_analyses: Dict) -> Dict:
        """Analyze patterns across services"""
        print(f"\nüîç CROSS-SERVICE ANALYSIS:")
        print("-" * 35)
        
        cross_analysis = {
            "total_errors": 0,
            "total_warnings": 0,
            "common_issues": defaultdict(int),
            "error_correlation": {},
            "service_health_ranking": []
        }
        
        # Aggregate error counts
        for service, analysis in service_analyses.items():
            if "error_counts" in analysis:
                cross_analysis["total_errors"] += analysis["error_counts"].get("error", 0)
                cross_analysis["total_errors"] += analysis["error_counts"].get("critical", 0)
                cross_analysis["total_warnings"] += analysis["error_counts"].get("warning", 0)
        
        # Find common patterns
        for service, analysis in service_analyses.items():
            if "patterns" in analysis:
                for pattern, count in analysis["patterns"].items():
                    cross_analysis["common_issues"][pattern] += count
        
        # Rank services by health
        service_scores = []
        for service, analysis in service_analyses.items():
            if "error_counts" in analysis:
                error_score = (analysis["error_counts"].get("critical", 0) * 3 + 
                             analysis["error_counts"].get("error", 0) * 2 + 
                             analysis["error_counts"].get("warning", 0) * 1)
                service_scores.append((service, error_score, analysis["status"]))
        
        service_scores.sort(key=lambda x: x[1])  # Sort by error score (lower is better)
        cross_analysis["service_health_ranking"] = service_scores
        
        print(f"Total Errors: {cross_analysis['total_errors']}")
        print(f"Total Warnings: {cross_analysis['total_warnings']}")
        
        if cross_analysis["common_issues"]:
            print("Common Issues:")
            for issue, count in sorted(cross_analysis["common_issues"].items(), 
                                     key=lambda x: x[1], reverse=True):
                print(f"   {issue}: {count}")
        
        print("Service Health Ranking (best to worst):")
        for service, score, status in service_scores:
            status_icon = "‚úÖ" if status == "clean" else "‚ö†Ô∏è" if status == "warnings_only" else "‚ùå"
            print(f"   {status_icon} {service}: {status} (score: {score})")
        
        return cross_analysis
    
    def generate_summary(self, results: Dict) -> Dict:
        """Generate overall summary and recommendations"""
        print(f"\nüìä LOG ANALYSIS SUMMARY:")
        print("=" * 30)
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "overall_log_health": "unknown",
            "services_analyzed": len(results.get("service_analyses", {})),
            "total_errors": results.get("cross_analysis", {}).get("total_errors", 0),
            "total_warnings": results.get("cross_analysis", {}).get("total_warnings", 0),
            "critical_issues": [],
            "recommendations": []
        }
        
        # Determine overall health
        total_errors = summary["total_errors"]
        total_warnings = summary["total_warnings"]
        
        if total_errors == 0 and total_warnings == 0:
            summary["overall_log_health"] = "excellent"
            print("üéâ LOG HEALTH: EXCELLENT (No errors or warnings)")
        elif total_errors == 0 and total_warnings < 5:
            summary["overall_log_health"] = "good"
            print(f"‚úÖ LOG HEALTH: GOOD ({total_warnings} warnings only)")
        elif total_errors < 5:
            summary["overall_log_health"] = "fair"
            print(f"üü° LOG HEALTH: FAIR ({total_errors} errors, {total_warnings} warnings)")
        else:
            summary["overall_log_health"] = "poor"
            print(f"‚ùå LOG HEALTH: POOR ({total_errors} errors, {total_warnings} warnings)")
        
        # Generate specific recommendations
        service_analyses = results.get("service_analyses", {})
        
        for service, analysis in service_analyses.items():
            if analysis.get("status") == "critical_errors":
                summary["critical_issues"].append(f"{service}_has_critical_errors")
                summary["recommendations"].append(f"Investigate {service} critical errors immediately")
            
            # Check for specific patterns
            patterns = analysis.get("patterns", {})
            if patterns.get("json_serialization_errors", 0) > 0:
                summary["critical_issues"].append(f"{service}_json_serialization_errors")
                summary["recommendations"].append(f"Fix JSON serialization in {service}")
            
            if patterns.get("websockets_issues", 0) > 0:
                summary["recommendations"].append(f"Check websockets dependency in {service}")
        
        # Common issues recommendations
        common_issues = results.get("cross_analysis", {}).get("common_issues", {})
        if common_issues.get("json_serialization_errors", 0) > 0:
            summary["recommendations"].append("Apply JSON serialization fix across services")
        
        if common_issues.get("websockets_issues", 0) > 0:
            summary["recommendations"].append("Apply websockets import fix across services")
        
        # Show top recommendations
        if summary["recommendations"]:
            print("Top Recommendations:")
            for i, rec in enumerate(summary["recommendations"][:3], 1):
                print(f"   {i}. {rec}")
        
        return summary

def main():
    """Main entry point with command line arguments"""
    parser = argparse.ArgumentParser(description='Analyze trading system logs')
    parser.add_argument('--service', help='Analyze specific service only')
    parser.add_argument('--errors-only', action='store_true', help='Show only error lines')
    parser.add_argument('--last-minutes', type=int, help='Analyze only last N minutes')
    
    args = parser.parse_args()
    
    diagnostic = LogAnalysisDiagnostic()
    results = diagnostic.run_full_analysis(
        service_filter=args.service,
        errors_only=args.errors_only,
        last_minutes=args.last_minutes
    )
    
    return results

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Diagnosis/diagnostic_process_ports.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC - PROCESS AND PORT STATUS
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic script for process and port monitoring

USAGE: python diagnostic_process_ports.py
PURPOSE: Accurate diagnosis of service processes and port status
"""

import subprocess
import requests
import time
from datetime import datetime
from typing import Dict, List, Tuple

class ProcessPortDiagnostic:
    """Accurate process and port diagnostic for trading system services"""
    
    def __init__(self):
        self.services = {
            "coordination": {"file": "coordination_service.py", "port": 5000},
            "scanner": {"file": "security_scanner.py", "port": 5001},
            "pattern": {"file": "pattern_analysis.py", "port": 5002},
            "technical": {"file": "technical_analysis.py", "port": 5003},
            "trading": {"file": "paper_trading.py", "port": 5005},
            "pattern_rec": {"file": "pattern_recognition_service.py", "port": 5006},
            "news": {"file": "news_service.py", "port": 5008},
            "reporting": {"file": "reporting_service.py", "port": 5009},
            "dashboard": {"file": "web_dashboard_service.py", "port": 5010}
        }
        
        self.hybrid_manager_file = "hybrid_manager.py"
    
    def run_full_diagnostic(self):
        """Run complete process and port diagnostic"""
        print("üîç TRADING SYSTEM PROCESS & PORT DIAGNOSTIC")
        print("=" * 60)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. Check hybrid manager
        hybrid_status = self.check_hybrid_manager()
        
        # 2. Check individual service processes
        process_results = self.check_service_processes()
        
        # 3. Check port responses
        port_results = self.check_port_responses()
        
        # 4. Cross-reference and analyze
        analysis = self.analyze_results(hybrid_status, process_results, port_results)
        
        # 5. Generate summary and recommendations
        self.generate_summary(analysis)
        
        return analysis
    
    def check_hybrid_manager(self) -> Dict:
        """Check hybrid manager process status"""
        print(f"\nü§ñ HYBRID MANAGER STATUS:")
        print("-" * 30)
        
        try:
            result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
            hybrid_processes = []
            
            for line in result.stdout.split('\n'):
                if self.hybrid_manager_file in line and 'python' in line:
                    parts = line.split()
                    if len(parts) >= 2:
                        pid = parts[1]
                        command = ' '.join(parts[10:]) if len(parts) >= 11 else line
                        hybrid_processes.append((pid, command))
            
            if hybrid_processes:
                for pid, command in hybrid_processes:
                    print(f"‚úÖ Hybrid Manager: PID {pid}")
                return {"status": "running", "processes": hybrid_processes}
            else:
                print("‚ùå Hybrid Manager: NOT FOUND")
                return {"status": "not_running", "processes": []}
                
        except Exception as e:
            print(f"‚ùå Error checking hybrid manager: {e}")
            return {"status": "error", "error": str(e)}
    
    def check_service_processes(self) -> Dict:
        """Check individual service processes with multiple detection methods"""
        print(f"\nüìä SERVICE PROCESS STATUS:")
        print("-" * 30)
        
        results = {}
        
        try:
            # Get all Python processes
            result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
            all_processes = result.stdout.split('\n')
            
            for service_name, config in self.services.items():
                service_file = config["file"]
                port = config["port"]
                
                # Method 1: Look for exact filename match
                exact_matches = []
                for line in all_processes:
                    if service_file in line and 'python' in line:
                        parts = line.split()
                        if len(parts) >= 2:
                            pid = parts[1]
                            exact_matches.append(pid)
                
                # Method 2: Look for port binding
                port_binding = self.check_port_binding(port)
                
                # Method 3: HTTP health check
                http_responsive = self.quick_health_check(port)
                
                results[service_name] = {
                    "file": service_file,
                    "port": port,
                    "process_pids": exact_matches,
                    "port_bound": port_binding,
                    "http_responsive": http_responsive,
                    "status": self.determine_process_status(exact_matches, port_binding, http_responsive)
                }
                
                # Display result
                status = results[service_name]["status"]
                if status == "running":
                    pids_str = ", ".join(exact_matches) if exact_matches else "Unknown PID"
                    print(f"‚úÖ {service_name:12}: Running (PID: {pids_str})")
                elif status == "responding":
                    print(f"üü° {service_name:12}: Responding (No PID found, but HTTP works)")
                else:
                    print(f"‚ùå {service_name:12}: Not running")
        
        except Exception as e:
            print(f"‚ùå Error checking service processes: {e}")
            results["error"] = str(e)
        
        return results
    
    def check_port_responses(self) -> Dict:
        """Check HTTP responses on all service ports"""
        print(f"\nüåê PORT RESPONSE STATUS:")
        print("-" * 30)
        
        results = {}
        
        for service_name, config in self.services.items():
            port = config["port"]
            
            try:
                start_time = time.time()
                response = requests.get(f'http://localhost:{port}/health', timeout=5)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        service_status = data.get('status', 'unknown')
                        service_type = data.get('service', 'unknown')
                        
                        results[service_name] = {
                            "port": port,
                            "status": "responding",
                            "http_code": response.status_code,
                            "response_time": round(response_time * 1000, 1),
                            "service_status": service_status,
                            "service_type": service_type
                        }
                        
                        print(f"‚úÖ {service_name:12} (port {port}): {service_status} ({response_time*1000:.1f}ms)")
                    
                    except json.JSONDecodeError:
                        results[service_name] = {
                            "port": port,
                            "status": "responding_no_json",
                            "http_code": response.status_code,
                            "response_time": round(response_time * 1000, 1)
                        }
                        print(f"üü° {service_name:12} (port {port}): Responding but no JSON")
                
                else:
                    results[service_name] = {
                        "port": port,
                        "status": "http_error",
                        "http_code": response.status_code,
                        "response_time": round(response_time * 1000, 1)
                    }
                    print(f"‚ö†Ô∏è {service_name:12} (port {port}): HTTP {response.status_code}")
            
            except requests.exceptions.ConnectionError:
                results[service_name] = {
                    "port": port,
                    "status": "connection_refused",
                    "error": "Connection refused"
                }
                print(f"‚ùå {service_name:12} (port {port}): Connection refused")
            
            except requests.exceptions.Timeout:
                results[service_name] = {
                    "port": port,
                    "status": "timeout",
                    "error": "Request timeout"
                }
                print(f"‚è±Ô∏è {service_name:12} (port {port}): Timeout")
            
            except Exception as e:
                results[service_name] = {
                    "port": port,
                    "status": "error",
                    "error": str(e)
                }
                print(f"‚ùå {service_name:12} (port {port}): {type(e).__name__}")
        
        return results
    
    def check_port_binding(self, port: int) -> bool:
        """Check if a port is bound using lsof"""
        try:
            result = subprocess.run(['lsof', '-i', f':{port}'], 
                                  capture_output=True, text=True)
            return str(port) in result.stdout
        except:
            return False
    
    def quick_health_check(self, port: int) -> bool:
        """Quick HTTP health check"""
        try:
            response = requests.get(f'http://localhost:{port}/health', timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def determine_process_status(self, pids: List[str], port_bound: bool, http_responsive: bool) -> str:
        """Determine overall process status from multiple indicators"""
        if pids and port_bound and http_responsive:
            return "running"  # Full confirmation
        elif http_responsive:
            return "responding"  # Service works but PID detection failed
        elif port_bound:
            return "bound_not_responding"  # Port occupied but not responding
        else:
            return "not_running"  # Definitely not running
    
    def analyze_results(self, hybrid_status: Dict, process_results: Dict, port_results: Dict) -> Dict:
        """Analyze all results and provide insights"""
        print(f"\nüß† ANALYSIS & INSIGHTS:")
        print("-" * 30)
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "hybrid_manager": hybrid_status,
            "overall_status": "unknown",
            "services_running": 0,
            "services_responding": 0,
            "services_total": len(self.services),
            "issues": [],
            "recommendations": []
        }
        
        # Count running and responding services
        for service_name in self.services:
            process_status = process_results.get(service_name, {}).get("status", "unknown")
            port_status = port_results.get(service_name, {}).get("status", "unknown")
            
            if process_status == "running":
                analysis["services_running"] += 1
            
            if port_status == "responding":
                analysis["services_responding"] += 1
        
        # Overall system status assessment
        if analysis["services_responding"] == analysis["services_total"]:
            analysis["overall_status"] = "fully_operational"
            print("üéâ SYSTEM STATUS: FULLY OPERATIONAL")
            print(f"   All {analysis['services_total']} services responding")
        
        elif analysis["services_responding"] >= analysis["services_total"] - 1:
            analysis["overall_status"] = "mostly_operational"
            print("üü° SYSTEM STATUS: MOSTLY OPERATIONAL")
            print(f"   {analysis['services_responding']}/{analysis['services_total']} services responding")
        
        elif analysis["services_responding"] >= analysis["services_total"] // 2:
            analysis["overall_status"] = "degraded"
            print("‚ö†Ô∏è SYSTEM STATUS: DEGRADED")
            print(f"   Only {analysis['services_responding']}/{analysis['services_total']} services responding")
        
        else:
            analysis["overall_status"] = "critical"
            print("üö® SYSTEM STATUS: CRITICAL")
            print(f"   Only {analysis['services_responding']}/{analysis['services_total']} services responding")
        
        # Identify specific issues
        if hybrid_status["status"] != "running":
            analysis["issues"].append("hybrid_manager_not_running")
            analysis["recommendations"].append("Start hybrid manager: python hybrid_manager.py start")
        
        # Check for PID detection issues
        pid_detection_issues = 0
        for service_name in self.services:
            process_status = process_results.get(service_name, {}).get("status", "unknown")
            port_status = port_results.get(service_name, {}).get("status", "unknown")
            
            if process_status != "running" and port_status == "responding":
                pid_detection_issues += 1
        
        if pid_detection_issues > 0:
            analysis["issues"].append(f"pid_detection_inaccurate_{pid_detection_issues}_services")
            analysis["recommendations"].append("PID detection has issues - trust port responses over process detection")
        
        return analysis
    
    def generate_summary(self, analysis: Dict):
        """Generate diagnostic summary and recommendations"""
        print(f"\nüìã DIAGNOSTIC SUMMARY:")
        print("=" * 30)
        
        print(f"Overall Status: {analysis['overall_status']}")
        print(f"Services Responding: {analysis['services_responding']}/{analysis['services_total']}")
        print(f"Hybrid Manager: {analysis['hybrid_manager']['status']}")
        
        if analysis["issues"]:
            print(f"\n‚ö†Ô∏è Issues Found:")
            for issue in analysis["issues"]:
                print(f"   - {issue}")
        
        if analysis["recommendations"]:
            print(f"\nüí° Recommendations:")
            for rec in analysis["recommendations"]:
                print(f"   - {rec}")
        
        print(f"\nüïê Diagnostic completed at: {analysis['timestamp']}")

def main():
    """Main entry point"""
    diagnostic = ProcessPortDiagnostic()
    analysis = diagnostic.run_full_diagnostic()
    return analysis

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Diagnosis/diagnostic_service_integration.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC - SERVICE INTEGRATION & HEALTH
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic script for service integration testing

USAGE: python diagnostic_service_integration.py
PURPOSE: Test service endpoints, inter-service communication, and functional health
"""

import requests
import json
import time
from datetime import datetime
from typing import Dict, List, Optional

class ServiceIntegrationDiagnostic:
    """Test service integration and functional health"""
    
    def __init__(self):
        self.base_urls = {
            "coordination": "http://localhost:5000",
            "scanner": "http://localhost:5001", 
            "pattern": "http://localhost:5002",
            "technical": "http://localhost:5003",
            "trading": "http://localhost:5005",
            "pattern_rec": "http://localhost:5006",
            "news": "http://localhost:5008",
            "reporting": "http://localhost:5009",
            "dashboard": "http://localhost:5010"
        }
        
        self.test_symbol = "AAPL"
        self.timeout = 10
    
    def run_full_diagnostic(self):
        """Run complete service integration diagnostic"""
        print("üîó TRADING SYSTEM SERVICE INTEGRATION DIAGNOSTIC")
        print("=" * 65)
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = {}
        
        # 1. Test individual service health endpoints
        health_results = self.test_health_endpoints()
        results["health_endpoints"] = health_results
        
        # 2. Test service-specific functionality
        functionality_results = self.test_service_functionality()
        results["functionality"] = functionality_results
        
        # 3. Test inter-service communication
        integration_results = self.test_service_integration()
        results["integration"] = integration_results
        
        # 4. Test complete workflow
        workflow_results = self.test_complete_workflow()
        results["workflow"] = workflow_results
        
        # 5. Analyze and summarize
        summary = self.analyze_and_summarize(results)
        results["summary"] = summary
        
        return results
    
    def test_health_endpoints(self) -> Dict:
        """Test /health endpoints for all services"""
        print(f"\nüíä HEALTH ENDPOINT TESTS:")
        print("-" * 40)
        
        results = {}
        
        for service_name, base_url in self.base_urls.items():
            try:
                start_time = time.time()
                response = requests.get(f"{base_url}/health", timeout=self.timeout)
                response_time = time.time() - start_time
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        status = data.get('status', 'unknown')
                        service_type = data.get('service', 'unknown')
                        
                        results[service_name] = {
                            "status": "healthy",
                            "response_time": round(response_time * 1000, 1),
                            "service_status": status,
                            "service_type": service_type,
                            "additional_info": self.extract_additional_health_info(data)
                        }
                        
                        print(f"‚úÖ {service_name:12}: {status} ({response_time*1000:.1f}ms)")
                        
                        # Show important additional info
                        if service_name == "pattern" and "yfinance_available" in data:
                            yf_status = "‚úÖ" if data["yfinance_available"] else "‚ö†Ô∏è "
                            print(f"   ‚îî‚îÄ yfinance: {yf_status}")
                        
                        if service_name == "trading" and "alpaca_connected" in data:
                            alpaca_status = "‚úÖ" if data["alpaca_connected"] else "‚ö†Ô∏è "
                            trading_mode = data.get("trading_mode", "unknown")
                            print(f"   ‚îî‚îÄ alpaca: {alpaca_status} ({trading_mode})")
                    
                    except json.JSONDecodeError:
                        results[service_name] = {
                            "status": "responding_invalid_json",
                            "response_time": round(response_time * 1000, 1),
                            "http_code": response.status_code
                        }
                        print(f"üü° {service_name:12}: Responding but invalid JSON")
                
                else:
                    results[service_name] = {
                        "status": "http_error",
                        "response_time": round(response_time * 1000, 1),
                        "http_code": response.status_code
                    }
                    print(f"‚ùå {service_name:12}: HTTP {response.status_code}")
            
            except requests.exceptions.Timeout:
                results[service_name] = {"status": "timeout"}
                print(f"‚è±Ô∏è {service_name:12}: Timeout ({self.timeout}s)")
            
            except requests.exceptions.ConnectionError:
                results[service_name] = {"status": "connection_error"}
                print(f"‚ùå {service_name:12}: Connection refused")
            
            except Exception as e:
                results[service_name] = {"status": "error", "error": str(e)}
                print(f"‚ùå {service_name:12}: {type(e).__name__}")
        
        return results
    
    def test_service_functionality(self) -> Dict:
        """Test key functionality endpoints for each service"""
        print(f"\n‚öôÔ∏è SERVICE FUNCTIONALITY TESTS:")
        print("-" * 40)
        
        results = {}
        
        # Test Scanner
        print("üîç Testing Scanner...")
        try:
            response = requests.get(f"{self.base_urls['scanner']}/scan_securities", timeout=self.timeout)
            if response.status_code == 200:
                securities = response.json()
                count = len(securities) if isinstance(securities, list) else 0
                results["scanner"] = {"status": "working", "securities_found": count}
                print(f"   ‚úÖ Found {count} securities")
            else:
                results["scanner"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
        except Exception as e:
            results["scanner"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        # Test Pattern Analysis
        print("üìä Testing Pattern Analysis...")
        try:
            response = requests.get(f"{self.base_urls['pattern']}/analyze_patterns/{self.test_symbol}", timeout=self.timeout)
            if response.status_code == 200:
                patterns = response.json()
                pattern_count = len(patterns.get('patterns', []))
                confidence = patterns.get('confidence_score', 0)
                results["pattern"] = {
                    "status": "working", 
                    "patterns_found": pattern_count,
                    "confidence": confidence
                }
                print(f"   ‚úÖ Found {pattern_count} patterns (confidence: {confidence:.3f})")
            else:
                results["pattern"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
                # Try to get error details
                try:
                    error_text = response.text[:200]
                    print(f"   ‚îî‚îÄ Error: {error_text}")
                except:
                    pass
        except Exception as e:
            results["pattern"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        # Test Technical Analysis
        print("üìà Testing Technical Analysis...")
        try:
            test_data = {"securities": [{"symbol": self.test_symbol, "patterns": []}]}
            response = requests.post(f"{self.base_urls['technical']}/generate_signals", 
                                   json=test_data, timeout=self.timeout)
            if response.status_code == 200:
                signals = response.json()
                signal_count = len(signals) if isinstance(signals, list) else 0
                results["technical"] = {"status": "working", "signals_generated": signal_count}
                print(f"   ‚úÖ Generated {signal_count} signals")
            else:
                results["technical"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
        except Exception as e:
            results["technical"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        # Test News Service
        print("üì∞ Testing News Service...")
        try:
            response = requests.get(f"{self.base_urls['news']}/news_sentiment/{self.test_symbol}", timeout=self.timeout)
            if response.status_code == 200:
                sentiment = response.json()
                sentiment_score = sentiment.get('sentiment_score', 0)
                news_count = sentiment.get('news_count', 0)
                results["news"] = {
                    "status": "working", 
                    "sentiment_score": sentiment_score,
                    "news_count": news_count
                }
                print(f"   ‚úÖ Sentiment: {sentiment_score:.3f} ({news_count} articles)")
            else:
                results["news"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
        except Exception as e:
            results["news"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        # Test Paper Trading
        print("üí∞ Testing Paper Trading...")
        try:
            response = requests.get(f"{self.base_urls['trading']}/account", timeout=self.timeout)
            if response.status_code == 200:
                account = response.json()
                buying_power = account.get('buying_power', 0)
                trading_mode = account.get('mode', 'unknown')
                results["trading"] = {
                    "status": "working",
                    "buying_power": buying_power,
                    "trading_mode": trading_mode
                }
                print(f"   ‚úÖ Account: ${buying_power:,.2f} ({trading_mode})")
            else:
                results["trading"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
        except Exception as e:
            results["trading"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        # Test Reporting
        print("üìã Testing Reporting...")
        try:
            response = requests.get(f"{self.base_urls['reporting']}/daily_summary", timeout=self.timeout)
            if response.status_code == 200:
                summary = response.json()
                trades = summary.get('trading_performance', {}).get('total_trades', 0)
                results["reporting"] = {"status": "working", "daily_trades": trades}
                print(f"   ‚úÖ Daily summary: {trades} trades")
            else:
                results["reporting"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå HTTP {response.status_code}")
        except Exception as e:
            results["reporting"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå {type(e).__name__}: {e}")
        
        return results
    
    def test_service_integration(self) -> Dict:
        """Test communication between services"""
        print(f"\nüîó INTER-SERVICE COMMUNICATION TESTS:")
        print("-" * 45)
        
        results = {}
        
        # Test Coordination -> Scanner
        print("ü§ù Testing Coordination ‚Üî Scanner...")
        try:
            response = requests.get(f"{self.base_urls['coordination']}/service_status", timeout=self.timeout)
            if response.status_code == 200:
                status = response.json()
                registered_services = len(status)
                results["coordination_registry"] = {
                    "status": "working",
                    "registered_services": registered_services
                }
                print(f"   ‚úÖ Service registry: {registered_services} services")
            else:
                results["coordination_registry"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå Service registry: HTTP {response.status_code}")
        except Exception as e:
            results["coordination_registry"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå Service registry: {type(e).__name__}")
        
        # Test Pattern -> Pattern Recognition integration
        print("ü§ù Testing Pattern ‚Üî Pattern Recognition...")
        try:
            response = requests.get(f"{self.base_urls['pattern_rec']}/detect_advanced_patterns/{self.test_symbol}", timeout=self.timeout)
            if response.status_code == 200:
                advanced_patterns = response.json()
                pattern_score = advanced_patterns.get('overall_pattern_score', 0)
                results["pattern_integration"] = {
                    "status": "working",
                    "pattern_score": pattern_score
                }
                print(f"   ‚úÖ Advanced patterns: score {pattern_score:.3f}")
            else:
                results["pattern_integration"] = {"status": "error", "http_code": response.status_code}
                print(f"   ‚ùå Advanced patterns: HTTP {response.status_code}")
        except Exception as e:
            results["pattern_integration"] = {"status": "error", "error": str(e)}
            print(f"   ‚ùå Advanced patterns: {type(e).__name__}")
        
        return results
    
    def test_complete_workflow(self) -> Dict:
        """Test the complete trading workflow"""
        print(f"\nüîÑ COMPLETE WORKFLOW TEST:")
        print("-" * 35)
        
        try:
            print("üöÄ Starting trading cycle...")
            start_time = time.time()
            
            response = requests.post(f"{self.base_urls['coordination']}/start_trading_cycle", timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                cycle_time = time.time() - start_time
                
                workflow_result = {
                    "status": "completed",
                    "cycle_time": round(cycle_time, 1),
                    "cycle_id": result.get('cycle_id', 'N/A'),
                    "securities_scanned": result.get('securities_scanned', 0),
                    "patterns_analyzed": result.get('patterns_analyzed', 0),
                    "signals_generated": result.get('signals_generated', 0),
                    "trades_executed": result.get('trades_executed', 0)
                }
                
                print(f"   ‚úÖ Workflow completed in {cycle_time:.1f}s")
                print(f"   üìä Securities: {workflow_result['securities_scanned']}")
                print(f"   üìà Patterns: {workflow_result['patterns_analyzed']}")
                print(f"   üì° Signals: {workflow_result['signals_generated']}")
                print(f"   üí∞ Trades: {workflow_result['trades_executed']}")
                
                return workflow_result
            
            else:
                print(f"   ‚ùå Workflow failed: HTTP {response.status_code}")
                return {"status": "failed", "http_code": response.status_code}
        
        except Exception as e:
            print(f"   ‚ùå Workflow error: {type(e).__name__}")
            return {"status": "error", "error": str(e)}
    
    def extract_additional_health_info(self, health_data: Dict) -> Dict:
        """Extract additional useful info from health endpoint"""
        additional = {}
        
        # Common additional fields
        for key in ["yfinance_available", "alpaca_connected", "trading_mode", 
                   "ml_available", "data_source", "implementation"]:
            if key in health_data:
                additional[key] = health_data[key]
        
        return additional
    
    def analyze_and_summarize(self, results: Dict) -> Dict:
        """Analyze all test results and provide summary"""
        print(f"\nüìä INTEGRATION DIAGNOSTIC SUMMARY:")
        print("=" * 40)
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "overall_health": "unknown",
            "services_healthy": 0,
            "services_functional": 0,
            "workflow_status": "unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": []
        }
        
        # Count healthy services
        health_results = results.get("health_endpoints", {})
        for service, health in health_results.items():
            if health.get("status") == "healthy":
                summary["services_healthy"] += 1
        
        # Count functional services
        func_results = results.get("functionality", {})
        for service, func in func_results.items():
            if func.get("status") == "working":
                summary["services_functional"] += 1
        
        # Workflow status
        workflow = results.get("workflow", {})
        summary["workflow_status"] = workflow.get("status", "unknown")
        
        # Determine overall health
        total_services = len(self.base_urls)
        if summary["services_healthy"] == total_services and summary["workflow_status"] == "completed":
            summary["overall_health"] = "excellent"
            print("üéâ OVERALL HEALTH: EXCELLENT")
        elif summary["services_healthy"] >= total_services - 1 and summary["workflow_status"] == "completed":
            summary["overall_health"] = "good"
            print("‚úÖ OVERALL HEALTH: GOOD")
        elif summary["services_healthy"] >= total_services // 2:
            summary["overall_health"] = "fair"
            print("üü° OVERALL HEALTH: FAIR")
        else:
            summary["overall_health"] = "poor"
            print("‚ùå OVERALL HEALTH: POOR")
        
        print(f"Healthy Services: {summary['services_healthy']}/{total_services}")
        print(f"Functional Services: {summary['services_functional']}/{len(func_results)}")
        print(f"Workflow: {summary['workflow_status']}")
        
        # Generate recommendations
        if summary["workflow_status"] != "completed":
            summary["critical_issues"].append("Trading workflow not completing")
            summary["recommendations"].append("Check coordination service and inter-service communication")
        
        if summary["services_healthy"] < total_services:
            unhealthy_count = total_services - summary["services_healthy"]
            summary["warnings"].append(f"{unhealthy_count} services not responding to health checks")
        
        # Service-specific recommendations
        pattern_func = func_results.get("pattern", {})
        if pattern_func.get("status") == "error":
            summary["critical_issues"].append("Pattern analysis not working")
            summary["recommendations"].append("Check pattern analysis logs for JSON serialization errors")
        
        return summary

def main():
    """Main entry point"""
    diagnostic = ServiceIntegrationDiagnostic()
    results = diagnostic.run_full_diagnostic()
    return results

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Diagnosis/diagnostic_toolkit.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC TOOLKIT - MAIN ORCHESTRATOR
Version: 1.0.0
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.0 (2025-06-17) - Initial diagnostic toolkit orchestrator

USAGE: 
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check only
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --integration     # Service integration test only
  python diagnostic_toolkit.py --processes       # Process/port check only
  python diagnostic_toolkit.py --service NAME    # Single service focus
  python diagnostic_toolkit.py --report          # Generate detailed report

PURPOSE: Unified diagnostic toolkit for comprehensive Trading System health analysis
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Import diagnostic modules
try:
    from diagnostic_service_integration import ServiceIntegrationDiagnostic
    from diagnostic_log_analysis import LogAnalysisDiagnostic  
    from diagnostic_process_ports import ProcessPortDiagnostic
except ImportError as e:
    print(f"‚ùå Error importing diagnostic modules: {e}")
    print("   Ensure all diagnostic scripts are in the same directory")
    sys.exit(1)

class TradingSystemDiagnosticToolkit:
    """Comprehensive diagnostic toolkit orchestrator"""
    
    def __init__(self):
        self.timestamp = datetime.now()
        self.results = {}
        
        # Initialize diagnostic modules
        self.integration_diagnostic = ServiceIntegrationDiagnostic()
        self.log_diagnostic = LogAnalysisDiagnostic()
        self.process_diagnostic = ProcessPortDiagnostic()
        
        # Output directory
        self.output_dir = Path('/content/diagnostic_reports')
        self.output_dir.mkdir(exist_ok=True)
    
    def run_comprehensive_diagnostic(self, options: Dict) -> Dict:
        """Run complete comprehensive diagnostic"""
        print("üè• TRADING SYSTEM COMPREHENSIVE DIAGNOSTIC TOOLKIT")
        print("=" * 70)
        print(f"Started: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Mode: {'Quick' if options.get('quick') else 'Comprehensive'}")
        
        if options.get('service_filter'):
            print(f"Service Focus: {options['service_filter']}")
        
        comprehensive_results = {
            "diagnostic_metadata": {
                "timestamp": self.timestamp.isoformat(),
                "version": "1.0.0",
                "mode": "quick" if options.get('quick') else "comprehensive",
                "service_filter": options.get('service_filter'),
                "modules_run": []
            }
        }
        
        # 1. Process and Port Analysis (Always run first)
        if not options.get('logs_only') and not options.get('integration_only'):
            print(f"\n{'='*20} PHASE 1: PROCESS & PORT ANALYSIS {'='*20}")
            try:
                process_results = self.process_diagnostic.run_full_diagnostic()
                comprehensive_results["process_analysis"] = process_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("process_ports")
            except Exception as e:
                print(f"‚ùå Process analysis failed: {e}")
                comprehensive_results["process_analysis"] = {"error": str(e)}
        
        # 2. Service Integration Testing
        if not options.get('logs_only') and not options.get('processes_only'):
            print(f"\n{'='*20} PHASE 2: SERVICE INTEGRATION TESTING {'='*18}")
            try:
                integration_results = self.integration_diagnostic.run_full_diagnostic()
                comprehensive_results["integration_analysis"] = integration_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("service_integration")
            except Exception as e:
                print(f"‚ùå Integration analysis failed: {e}")
                comprehensive_results["integration_analysis"] = {"error": str(e)}
        
        # 3. Log Analysis (Skip in quick mode unless specifically requested)
        if not options.get('quick') or options.get('logs_only'):
            if not options.get('integration_only') and not options.get('processes_only'):
                print(f"\n{'='*20} PHASE 3: LOG ANALYSIS {'='*31}")
                try:
                    log_results = self.log_diagnostic.run_full_analysis(
                        service_filter=options.get('service_filter'),
                        errors_only=options.get('errors_only', False),
                        last_minutes=options.get('last_minutes')
                    )
                    comprehensive_results["log_analysis"] = log_results
                    comprehensive_results["diagnostic_metadata"]["modules_run"].append("log_analysis")
                except Exception as e:
                    print(f"‚ùå Log analysis failed: {e}")
                    comprehensive_results["log_analysis"] = {"error": str(e)}
        
        # 4. Comprehensive Analysis and Recommendations
        print(f"\n{'='*20} PHASE 4: COMPREHENSIVE ANALYSIS {'='*23}")
        comprehensive_analysis = self.generate_comprehensive_analysis(comprehensive_results)
        comprehensive_results["comprehensive_analysis"] = comprehensive_analysis
        
        # 5. Save Report (if requested)
        if options.get('save_report'):
            report_path = self.save_diagnostic_report(comprehensive_results)
            comprehensive_results["report_path"] = str(report_path)
        
        return comprehensive_results
    
    def generate_comprehensive_analysis(self, results: Dict) -> Dict:
        """Generate unified analysis across all diagnostic modules"""
        print("üß† COMPREHENSIVE SYSTEM ANALYSIS")
        print("-" * 40)
        
        analysis = {
            "overall_system_health": "unknown",
            "confidence_level": "unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": [],
            "system_scores": {},
            "health_indicators": {},
            "summary": {}
        }
        
        # Collect health indicators from each module
        process_health = self.extract_process_health(results.get("process_analysis", {}))
        integration_health = self.extract_integration_health(results.get("integration_analysis", {}))
        log_health = self.extract_log_health(results.get("log_analysis", {}))
        
        analysis["health_indicators"] = {
            "process_health": process_health,
            "integration_health": integration_health,
            "log_health": log_health
        }
        
        # Calculate weighted system scores
        scores = []
        weights = []
        
        if process_health.get("score") is not None:
            scores.append(process_health["score"])
            weights.append(0.4)  # Process health is most important
        
        if integration_health.get("score") is not None:
            scores.append(integration_health["score"])
            weights.append(0.4)  # Integration health is equally important
        
        if log_health.get("score") is not None:
            scores.append(log_health["score"])
            weights.append(0.2)  # Log health is supporting indicator
        
        # Calculate overall score
        if scores and weights:
            overall_score = sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)
            analysis["system_scores"]["overall"] = round(overall_score, 1)
        else:
            overall_score = 0
        
        # Determine overall health
        if overall_score >= 90:
            analysis["overall_system_health"] = "excellent"
            analysis["confidence_level"] = "high"
            print("üéâ OVERALL SYSTEM HEALTH: EXCELLENT")
        elif overall_score >= 75:
            analysis["overall_system_health"] = "good"
            analysis["confidence_level"] = "high"
            print("‚úÖ OVERALL SYSTEM HEALTH: GOOD")
        elif overall_score >= 60:
            analysis["overall_system_health"] = "fair"
            analysis["confidence_level"] = "medium"
            print("üü° OVERALL SYSTEM HEALTH: FAIR")
        elif overall_score >= 40:
            analysis["overall_system_health"] = "poor"
            analysis["confidence_level"] = "high"
            print("‚ö†Ô∏è OVERALL SYSTEM HEALTH: POOR")
        else:
            analysis["overall_system_health"] = "critical"
            analysis["confidence_level"] = "high"
            print("üö® OVERALL SYSTEM HEALTH: CRITICAL")
        
        print(f"System Score: {overall_score:.1f}/100")
        
        # Collect issues and recommendations from all modules
        self.collect_unified_issues_and_recommendations(results, analysis)
        
        # Generate summary
        analysis["summary"] = {
            "total_services": 9,
            "services_healthy": process_health.get("services_responding", 0),
            "workflow_functional": integration_health.get("workflow_works", False),
            "log_errors": log_health.get("total_errors", 0),
            "critical_issues_count": len(analysis["critical_issues"]),
            "recommendations_count": len(analysis["recommendations"])
        }
        
        # Display key findings
        print(f"\nKey Findings:")
        print(f"   Services Responding: {analysis['summary']['services_healthy']}/9")
        print(f"   Workflow Functional: {'Yes' if analysis['summary']['workflow_functional'] else 'No'}")
        print(f"   Log Errors: {analysis['summary']['log_errors']}")
        print(f"   Critical Issues: {analysis['summary']['critical_issues_count']}")
        
        return analysis
    
    def extract_process_health(self, process_results: Dict) -> Dict:
        """Extract health indicators from process analysis"""
        if not process_results or "error" in process_results:
            return {"score": None, "status": "unavailable"}
        
        services_total = process_results.get("services_total", 9)
        services_responding = process_results.get("services_responding", 0)
        
        # Calculate score based on service availability
        score = (services_responding / services_total) * 100 if services_total > 0 else 0
        
        return {
            "score": round(score, 1),
            "status": process_results.get("overall_status", "unknown"),
            "services_responding": services_responding,
            "services_total": services_total,
            "hybrid_manager_running": process_results.get("hybrid_manager", {}).get("status") == "running"
        }
    
    def extract_integration_health(self, integration_results: Dict) -> Dict:
        """Extract health indicators from integration analysis"""
        if not integration_results or "error" in integration_results:
            return {"score": None, "status": "unavailable"}
        
        summary = integration_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 80, "fair": 60, "poor": 30}
        
        score = health_score_map.get(summary.get("overall_health"), 0)
        workflow_result = integration_results.get("workflow", {})
        workflow_works = workflow_result.get("status") == "completed"
        
        return {
            "score": score,
            "status": summary.get("overall_health", "unknown"),
            "workflow_works": workflow_works,
            "services_functional": summary.get("services_functional", 0)
        }
    
    def extract_log_health(self, log_results: Dict) -> Dict:
        """Extract health indicators from log analysis"""
        if not log_results or "error" in log_results:
            return {"score": None, "status": "unavailable"}
        
        summary = log_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 85, "fair": 60, "poor": 20}
        
        score = health_score_map.get(summary.get("overall_log_health"), 50)
        
        return {
            "score": score,
            "status": summary.get("overall_log_health", "unknown"),
            "total_errors": summary.get("total_errors", 0),
            "total_warnings": summary.get("total_warnings", 0)
        }
    
    def collect_unified_issues_and_recommendations(self, results: Dict, analysis: Dict):
        """Collect and unify issues and recommendations from all modules"""
        # Process issues
        process_analysis = results.get("process_analysis", {})
        if process_analysis.get("issues"):
            analysis["critical_issues"].extend(process_analysis["issues"])
        if process_analysis.get("recommendations"):
            analysis["recommendations"].extend(process_analysis["recommendations"])
        
        # Integration issues
        integration_summary = results.get("integration_analysis", {}).get("summary", {})
        if integration_summary.get("critical_issues"):
            analysis["critical_issues"].extend(integration_summary["critical_issues"])
        if integration_summary.get("recommendations"):
            analysis["recommendations"].extend(integration_summary["recommendations"])
        
        # Log issues
        log_summary = results.get("log_analysis", {}).get("summary", {})
        if log_summary.get("critical_issues"):
            analysis["critical_issues"].extend(log_summary["critical_issues"])
        if log_summary.get("recommendations"):
            analysis["recommendations"].extend(log_summary["recommendations"])
        
        # Remove duplicates while preserving order
        analysis["critical_issues"] = list(dict.fromkeys(analysis["critical_issues"]))
        analysis["recommendations"] = list(dict.fromkeys(analysis["recommendations"]))
        
        # Add unified recommendations based on patterns
        if "json_serialization" in str(analysis["critical_issues"]):
            analysis["recommendations"].insert(0, "Apply JSON serialization fix to pattern_analysis.py")
        
        if "hybrid_manager_not_running" in analysis["critical_issues"]:
            analysis["recommendations"].insert(0, "Start system: python hybrid_manager.py start")
    
    def save_diagnostic_report(self, results: Dict) -> Path:
        """Save comprehensive diagnostic report to file"""
        timestamp_str = self.timestamp.strftime('%Y%m%d_%H%M%S')
        report_filename = f"trading_system_diagnostic_{timestamp_str}.json"
        report_path = self.output_dir / report_filename
        
        try:
            with open(report_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüíæ Diagnostic report saved: {report_path}")
            
            # Also create a summary text report
            summary_path = self.output_dir / f"diagnostic_summary_{timestamp_str}.txt"
            self.create_text_summary(results, summary_path)
            
            return report_path
            
        except Exception as e:
            print(f"‚ùå Error saving report: {e}")
            return None
    
    def create_text_summary(self, results: Dict, summary_path: Path):
        """Create human-readable text summary"""
        try:
            with open(summary_path, 'w') as f:
                f.write("TRADING SYSTEM DIAGNOSTIC SUMMARY\n")
                f.write("=" * 50 + "\n")
                f.write(f"Generated: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # Overall health
                comp_analysis = results.get("comprehensive_analysis", {})
                f.write(f"Overall Health: {comp_analysis.get('overall_system_health', 'unknown').upper()}\n")
                f.write(f"System Score: {comp_analysis.get('system_scores', {}).get('overall', 'N/A')}/100\n\n")
                
                # Key metrics
                summary = comp_analysis.get("summary", {})
                f.write("Key Metrics:\n")
                f.write(f"  Services Responding: {summary.get('services_healthy', 0)}/9\n")
                f.write(f"  Workflow Functional: {'Yes' if summary.get('workflow_functional') else 'No'}\n")
                f.write(f"  Log Errors: {summary.get('log_errors', 0)}\n")
                f.write(f"  Critical Issues: {summary.get('critical_issues_count', 0)}\n\n")
                
                # Critical issues
                critical_issues = comp_analysis.get("critical_issues", [])
                if critical_issues:
                    f.write("Critical Issues:\n")
                    for issue in critical_issues:
                        f.write(f"  - {issue}\n")
                    f.write("\n")
                
                # Top recommendations
                recommendations = comp_analysis.get("recommendations", [])
                if recommendations:
                    f.write("Top Recommendations:\n")
                    for i, rec in enumerate(recommendations[:5], 1):
                        f.write(f"  {i}. {rec}\n")
                
            print(f"üìÑ Text summary saved: {summary_path}")
            
        except Exception as e:
            print(f"‚ùå Error saving text summary: {e}")
    
    def run_quick_health_check(self) -> Dict:
        """Run quick health check (process + integration only)"""
        print("‚ö° QUICK HEALTH CHECK")
        print("-" * 30)
        
        return self.run_comprehensive_diagnostic({"quick": True})

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Trading System Diagnostic Toolkit',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --service scanner # Focus on scanner service
  python diagnostic_toolkit.py --report          # Save detailed report
        """
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Quick health check (skip log analysis)')
    parser.add_argument('--logs-only', action='store_true',
                       help='Run log analysis only')
    parser.add_argument('--integration-only', action='store_true',
                       help='Run integration tests only')
    parser.add_argument('--processes-only', action='store_true',
                       help='Run process/port checks only')
    parser.add_argument('--service', 
                       help='Focus on specific service')
    parser.add_argument('--errors-only', action='store_true',
                       help='Show only errors in log analysis')
    parser.add_argument('--last-minutes', type=int,
                       help='Analyze only last N minutes of logs')
    parser.add_argument('--report', action='store_true',
                       help='Save detailed diagnostic report')
    
    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_arguments()
    
    # Convert args to options dict
    options = {
        'quick': args.quick,
        'logs_only': args.logs_only,
        'integration_only': args.integration_only,
        'processes_only': args.processes_only,
        'service_filter': args.service,
        'errors_only': args.errors_only,
        'last_minutes': args.last_minutes,
        'save_report': args.report
    }
    
    # Initialize toolkit
    toolkit = TradingSystemDiagnosticToolkit()
    
    try:
        # Run diagnostic
        if args.quick:
            results = toolkit.run_quick_health_check()
        else:
            results = toolkit.run_comprehensive_diagnostic(options)
        
        # Print completion message
        print(f"\nüèÅ DIAGNOSTIC COMPLETED")
        print("=" * 30)
        
        comp_analysis = results.get("comprehensive_analysis", {})
        overall_health = comp_analysis.get("overall_system_health", "unknown")
        
        if overall_health in ["excellent", "good"]:
            print("‚úÖ Your trading system is healthy and ready for operation!")
        elif overall_health == "fair":
            print("üü° Your trading system has minor issues but is operational.")
        else:
            print("‚ùå Your trading system has significant issues requiring attention.")
        
        # Show next steps
        recommendations = comp_analysis.get("recommendations", [])
        if recommendations:
            print(f"\nüéØ Next Steps:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        return results
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Diagnostic interrupted by user")
        return None
    except Exception as e:
        print(f"\n‚ùå Diagnostic failed: {e}")
        return None

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Diagnosis/diagnostic_toolkit_v101.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC TOOLKIT - MAIN ORCHESTRATOR
Version: 1.0.1
Last Updated: 2025-06-17
REVISION HISTORY:
v1.0.1 (2025-06-17) - Fixed import paths for Google Drive deployment
v1.0.0 (2025-06-17) - Initial diagnostic toolkit orchestrator

USAGE: 
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check only
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --integration     # Service integration test only
  python diagnostic_toolkit.py --processes       # Process/port check only
  python diagnostic_toolkit.py --service NAME    # Single service focus
  python diagnostic_toolkit.py --report          # Generate detailed report

PURPOSE: Unified diagnostic toolkit for comprehensive Trading System health analysis
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# Add diagnostic directory to Python path for imports
DIAGNOSTIC_DIR = Path(__file__).parent
sys.path.insert(0, str(DIAGNOSTIC_DIR))

# Import diagnostic modules
try:
    from diagnostic_service_integration import ServiceIntegrationDiagnostic
    from diagnostic_log_analysis import LogAnalysisDiagnostic  
    from diagnostic_process_ports import ProcessPortDiagnostic
except ImportError as e:
    print(f"‚ùå Error importing diagnostic modules: {e}")
    print(f"   Looking in directory: {DIAGNOSTIC_DIR}")
    print(f"   Available files: {list(DIAGNOSTIC_DIR.glob('*.py'))}")
    sys.exit(1)

class TradingSystemDiagnosticToolkit:
    """Comprehensive diagnostic toolkit orchestrator"""
    
    def __init__(self):
        self.timestamp = datetime.now()
        self.results = {}
        
        # Initialize diagnostic modules
        self.integration_diagnostic = ServiceIntegrationDiagnostic()
        self.log_diagnostic = LogAnalysisDiagnostic()
        self.process_diagnostic = ProcessPortDiagnostic()
        
        # Output directory
        self.output_dir = Path('/content/diagnostic_reports')
        self.output_dir.mkdir(exist_ok=True)
    
    def run_comprehensive_diagnostic(self, options: Dict) -> Dict:
        """Run complete comprehensive diagnostic"""
        print("üè• TRADING SYSTEM COMPREHENSIVE DIAGNOSTIC TOOLKIT")
        print("=" * 70)
        print(f"Started: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Mode: {'Quick' if options.get('quick') else 'Comprehensive'}")
        
        if options.get('service_filter'):
            print(f"Service Focus: {options['service_filter']}")
        
        comprehensive_results = {
            "diagnostic_metadata": {
                "timestamp": self.timestamp.isoformat(),
                "version": "1.0.1",
                "mode": "quick" if options.get('quick') else "comprehensive",
                "service_filter": options.get('service_filter'),
                "modules_run": []
            }
        }
        
        # 1. Process and Port Analysis (Always run first)
        if not options.get('logs_only') and not options.get('integration_only'):
            print(f"\n{'='*20} PHASE 1: PROCESS & PORT ANALYSIS {'='*20}")
            try:
                process_results = self.process_diagnostic.run_full_diagnostic()
                comprehensive_results["process_analysis"] = process_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("process_ports")
            except Exception as e:
                print(f"‚ùå Process analysis failed: {e}")
                comprehensive_results["process_analysis"] = {"error": str(e)}
        
        # 2. Service Integration Testing
        if not options.get('logs_only') and not options.get('processes_only'):
            print(f"\n{'='*20} PHASE 2: SERVICE INTEGRATION TESTING {'='*18}")
            try:
                integration_results = self.integration_diagnostic.run_full_diagnostic()
                comprehensive_results["integration_analysis"] = integration_results
                comprehensive_results["diagnostic_metadata"]["modules_run"].append("service_integration")
            except Exception as e:
                print(f"‚ùå Integration analysis failed: {e}")
                comprehensive_results["integration_analysis"] = {"error": str(e)}
        
        # 3. Log Analysis (Skip in quick mode unless specifically requested)
        if not options.get('quick') or options.get('logs_only'):
            if not options.get('integration_only') and not options.get('processes_only'):
                print(f"\n{'='*20} PHASE 3: LOG ANALYSIS {'='*31}")
                try:
                    log_results = self.log_diagnostic.run_full_analysis(
                        service_filter=options.get('service_filter'),
                        errors_only=options.get('errors_only', False),
                        last_minutes=options.get('last_minutes')
                    )
                    comprehensive_results["log_analysis"] = log_results
                    comprehensive_results["diagnostic_metadata"]["modules_run"].append("log_analysis")
                except Exception as e:
                    print(f"‚ùå Log analysis failed: {e}")
                    comprehensive_results["log_analysis"] = {"error": str(e)}
        
        # 4. Comprehensive Analysis and Recommendations
        print(f"\n{'='*20} PHASE 4: COMPREHENSIVE ANALYSIS {'='*23}")
        comprehensive_analysis = self.generate_comprehensive_analysis(comprehensive_results)
        comprehensive_results["comprehensive_analysis"] = comprehensive_analysis
        
        # 5. Save Report (if requested)
        if options.get('save_report'):
            report_path = self.save_diagnostic_report(comprehensive_results)
            comprehensive_results["report_path"] = str(report_path)
        
        return comprehensive_results
    
    def generate_comprehensive_analysis(self, results: Dict) -> Dict:
        """Generate unified analysis across all diagnostic modules"""
        print("üß† COMPREHENSIVE SYSTEM ANALYSIS")
        print("-" * 40)
        
        analysis = {
            "overall_system_health": "unknown",
            "confidence_level": "unknown",
            "critical_issues": [],
            "warnings": [],
            "recommendations": [],
            "system_scores": {},
            "health_indicators": {},
            "summary": {}
        }
        
        # Collect health indicators from each module
        process_health = self.extract_process_health(results.get("process_analysis", {}))
        integration_health = self.extract_integration_health(results.get("integration_analysis", {}))
        log_health = self.extract_log_health(results.get("log_analysis", {}))
        
        analysis["health_indicators"] = {
            "process_health": process_health,
            "integration_health": integration_health,
            "log_health": log_health
        }
        
        # Calculate weighted system scores
        scores = []
        weights = []
        
        if process_health.get("score") is not None:
            scores.append(process_health["score"])
            weights.append(0.4)  # Process health is most important
        
        if integration_health.get("score") is not None:
            scores.append(integration_health["score"])
            weights.append(0.4)  # Integration health is equally important
        
        if log_health.get("score") is not None:
            scores.append(log_health["score"])
            weights.append(0.2)  # Log health is supporting indicator
        
        # Calculate overall score
        if scores and weights:
            overall_score = sum(score * weight for score, weight in zip(scores, weights)) / sum(weights)
            analysis["system_scores"]["overall"] = round(overall_score, 1)
        else:
            overall_score = 0
        
        # Determine overall health
        if overall_score >= 90:
            analysis["overall_system_health"] = "excellent"
            analysis["confidence_level"] = "high"
            print("üéâ OVERALL SYSTEM HEALTH: EXCELLENT")
        elif overall_score >= 75:
            analysis["overall_system_health"] = "good"
            analysis["confidence_level"] = "high"
            print("‚úÖ OVERALL SYSTEM HEALTH: GOOD")
        elif overall_score >= 60:
            analysis["overall_system_health"] = "fair"
            analysis["confidence_level"] = "medium"
            print("üü° OVERALL SYSTEM HEALTH: FAIR")
        elif overall_score >= 40:
            analysis["overall_system_health"] = "poor"
            analysis["confidence_level"] = "high"
            print("‚ö†Ô∏è OVERALL SYSTEM HEALTH: POOR")
        else:
            analysis["overall_system_health"] = "critical"
            analysis["confidence_level"] = "high"
            print("üö® OVERALL SYSTEM HEALTH: CRITICAL")
        
        print(f"System Score: {overall_score:.1f}/100")
        
        # Collect issues and recommendations from all modules
        self.collect_unified_issues_and_recommendations(results, analysis)
        
        # Generate summary
        analysis["summary"] = {
            "total_services": 9,
            "services_healthy": process_health.get("services_responding", 0),
            "workflow_functional": integration_health.get("workflow_works", False),
            "log_errors": log_health.get("total_errors", 0),
            "critical_issues_count": len(analysis["critical_issues"]),
            "recommendations_count": len(analysis["recommendations"])
        }
        
        # Display key findings
        print(f"\nKey Findings:")
        print(f"   Services Responding: {analysis['summary']['services_healthy']}/9")
        print(f"   Workflow Functional: {'Yes' if analysis['summary']['workflow_functional'] else 'No'}")
        print(f"   Log Errors: {analysis['summary']['log_errors']}")
        print(f"   Critical Issues: {analysis['summary']['critical_issues_count']}")
        
        return analysis
    
    def extract_process_health(self, process_results: Dict) -> Dict:
        """Extract health indicators from process analysis"""
        if not process_results or "error" in process_results:
            return {"score": None, "status": "unavailable"}
        
        services_total = process_results.get("services_total", 9)
        services_responding = process_results.get("services_responding", 0)
        
        # Calculate score based on service availability
        score = (services_responding / services_total) * 100 if services_total > 0 else 0
        
        return {
            "score": round(score, 1),
            "status": process_results.get("overall_status", "unknown"),
            "services_responding": services_responding,
            "services_total": services_total,
            "hybrid_manager_running": process_results.get("hybrid_manager", {}).get("status") == "running"
        }
    
    def extract_integration_health(self, integration_results: Dict) -> Dict:
        """Extract health indicators from integration analysis"""
        if not integration_results or "error" in integration_results:
            return {"score": None, "status": "unavailable"}
        
        summary = integration_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 80, "fair": 60, "poor": 30}
        
        score = health_score_map.get(summary.get("overall_health"), 0)
        workflow_result = integration_results.get("workflow", {})
        workflow_works = workflow_result.get("status") == "completed"
        
        return {
            "score": score,
            "status": summary.get("overall_health", "unknown"),
            "workflow_works": workflow_works,
            "services_functional": summary.get("services_functional", 0)
        }
    
    def extract_log_health(self, log_results: Dict) -> Dict:
        """Extract health indicators from log analysis"""
        if not log_results or "error" in log_results:
            return {"score": None, "status": "unavailable"}
        
        summary = log_results.get("summary", {})
        health_score_map = {"excellent": 100, "good": 85, "fair": 60, "poor": 20}
        
        score = health_score_map.get(summary.get("overall_log_health"), 50)
        
        return {
            "score": score,
            "status": summary.get("overall_log_health", "unknown"),
            "total_errors": summary.get("total_errors", 0),
            "total_warnings": summary.get("total_warnings", 0)
        }
    
    def collect_unified_issues_and_recommendations(self, results: Dict, analysis: Dict):
        """Collect and unify issues and recommendations from all modules"""
        # Process issues
        process_analysis = results.get("process_analysis", {})
        if process_analysis.get("issues"):
            analysis["critical_issues"].extend(process_analysis["issues"])
        if process_analysis.get("recommendations"):
            analysis["recommendations"].extend(process_analysis["recommendations"])
        
        # Integration issues
        integration_summary = results.get("integration_analysis", {}).get("summary", {})
        if integration_summary.get("critical_issues"):
            analysis["critical_issues"].extend(integration_summary["critical_issues"])
        if integration_summary.get("recommendations"):
            analysis["recommendations"].extend(integration_summary["recommendations"])
        
        # Log issues
        log_summary = results.get("log_analysis", {}).get("summary", {})
        if log_summary.get("critical_issues"):
            analysis["critical_issues"].extend(log_summary["critical_issues"])
        if log_summary.get("recommendations"):
            analysis["recommendations"].extend(log_summary["recommendations"])
        
        # Remove duplicates while preserving order
        analysis["critical_issues"] = list(dict.fromkeys(analysis["critical_issues"]))
        analysis["recommendations"] = list(dict.fromkeys(analysis["recommendations"]))
        
        # Add unified recommendations based on patterns
        if "json_serialization" in str(analysis["critical_issues"]):
            analysis["recommendations"].insert(0, "Apply JSON serialization fix to pattern_analysis.py")
        
        if "hybrid_manager_not_running" in analysis["critical_issues"]:
            analysis["recommendations"].insert(0, "Start system: python hybrid_manager.py start")
    
    def save_diagnostic_report(self, results: Dict) -> Path:
        """Save comprehensive diagnostic report to file"""
        timestamp_str = self.timestamp.strftime('%Y%m%d_%H%M%S')
        report_filename = f"trading_system_diagnostic_{timestamp_str}.json"
        report_path = self.output_dir / report_filename
        
        try:
            with open(report_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüíæ Diagnostic report saved: {report_path}")
            
            # Also create a summary text report
            summary_path = self.output_dir / f"diagnostic_summary_{timestamp_str}.txt"
            self.create_text_summary(results, summary_path)
            
            return report_path
            
        except Exception as e:
            print(f"‚ùå Error saving report: {e}")
            return None
    
    def create_text_summary(self, results: Dict, summary_path: Path):
        """Create human-readable text summary"""
        try:
            with open(summary_path, 'w') as f:
                f.write("TRADING SYSTEM DIAGNOSTIC SUMMARY\n")
                f.write("=" * 50 + "\n")
                f.write(f"Generated: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # Overall health
                comp_analysis = results.get("comprehensive_analysis", {})
                f.write(f"Overall Health: {comp_analysis.get('overall_system_health', 'unknown').upper()}\n")
                f.write(f"System Score: {comp_analysis.get('system_scores', {}).get('overall', 'N/A')}/100\n\n")
                
                # Key metrics
                summary = comp_analysis.get("summary", {})
                f.write("Key Metrics:\n")
                f.write(f"  Services Responding: {summary.get('services_healthy', 0)}/9\n")
                f.write(f"  Workflow Functional: {'Yes' if summary.get('workflow_functional') else 'No'}\n")
                f.write(f"  Log Errors: {summary.get('log_errors', 0)}\n")
                f.write(f"  Critical Issues: {summary.get('critical_issues_count', 0)}\n\n")
                
                # Critical issues
                critical_issues = comp_analysis.get("critical_issues", [])
                if critical_issues:
                    f.write("Critical Issues:\n")
                    for issue in critical_issues:
                        f.write(f"  - {issue}\n")
                    f.write("\n")
                
                # Top recommendations
                recommendations = comp_analysis.get("recommendations", [])
                if recommendations:
                    f.write("Top Recommendations:\n")
                    for i, rec in enumerate(recommendations[:5], 1):
                        f.write(f"  {i}. {rec}\n")
                
            print(f"üìÑ Text summary saved: {summary_path}")
            
        except Exception as e:
            print(f"‚ùå Error saving text summary: {e}")
    
    def run_quick_health_check(self) -> Dict:
        """Run quick health check (process + integration only)"""
        print("‚ö° QUICK HEALTH CHECK")
        print("-" * 30)
        
        return self.run_comprehensive_diagnostic({"quick": True})

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Trading System Diagnostic Toolkit',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python diagnostic_toolkit.py                    # Full comprehensive diagnostic
  python diagnostic_toolkit.py --quick           # Quick health check
  python diagnostic_toolkit.py --logs-only       # Log analysis only
  python diagnostic_toolkit.py --service scanner # Focus on scanner service
  python diagnostic_toolkit.py --report          # Save detailed report
        """
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Quick health check (skip log analysis)')
    parser.add_argument('--logs-only', action='store_true',
                       help='Run log analysis only')
    parser.add_argument('--integration-only', action='store_true',
                       help='Run integration tests only')
    parser.add_argument('--processes-only', action='store_true',
                       help='Run process/port checks only')
    parser.add_argument('--service', 
                       help='Focus on specific service')
    parser.add_argument('--errors-only', action='store_true',
                       help='Show only errors in log analysis')
    parser.add_argument('--last-minutes', type=int,
                       help='Analyze only last N minutes of logs')
    parser.add_argument('--report', action='store_true',
                       help='Save detailed diagnostic report')
    
    return parser.parse_args()

def main():
    """Main entry point"""
    args = parse_arguments()
    
    # Convert args to options dict
    options = {
        'quick': args.quick,
        'logs_only': args.logs_only,
        'integration_only': args.integration_only,
        'processes_only': args.processes_only,
        'service_filter': args.service,
        'errors_only': args.errors_only,
        'last_minutes': args.last_minutes,
        'save_report': args.report
    }
    
    # Initialize toolkit
    toolkit = TradingSystemDiagnosticToolkit()
    
    try:
        # Run diagnostic
        if args.quick:
            results = toolkit.run_quick_health_check()
        else:
            results = toolkit.run_comprehensive_diagnostic(options)
        
        # Print completion message
        print(f"\nüèÅ DIAGNOSTIC COMPLETED")
        print("=" * 30)
        
        comp_analysis = results.get("comprehensive_analysis", {})
        overall_health = comp_analysis.get("overall_system_health", "unknown")
        
        if overall_health in ["excellent", "good"]:
            print("‚úÖ Your trading system is healthy and ready for operation!")
        elif overall_health == "fair":
            print("üü° Your trading system has minor issues but is operational.")
        else:
            print("‚ùå Your trading system has significant issues requiring attention.")
        
        # Show next steps
        recommendations = comp_analysis.get("recommendations", [])
        if recommendations:
            print(f"\nüéØ Next Steps:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        return results
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Diagnostic interrupted by user")
        return None
    except Exception as e:
        print(f"\n‚ùå Diagnostic failed: {e}")
        return None

if __name__ == "__main__":
    main()

================================================================================
FILE: ./Diagnosis/setup_diagnostic_toolkit_v106.py
================================================================================

#!/usr/bin/env python3
"""
Name of Service: TRADING SYSTEM DIAGNOSTIC TOOLKIT SETUP - ENHANCED VERSION
Version: 1.0.6
Last Updated: 2025-06-19
REVISION HISTORY:
v1.0.6 (2025-06-19) - Enhanced with automatic report generation, better analysis display, and quick issue detection
v1.0.5 (2025-06-17) - Incorporated v1.0.3 Jupyter/Colab notebook execution fix + v1.0.4 report generation fixes
v1.0.4 (2025-06-17) - Fixed report generation feedback and verification
v1.0.3 (2025-06-17) - Enhanced error handling and path detection + Fixed Jupyter/Colab notebook execution error
v1.0.2 (2025-06-17) - Initial setup script for diagnostic toolkit
v1.0.1 (2025-06-17) - Fixed import paths for Google Drive deployment
v1.0.0 (2025-06-17) - Initial diagnostic toolkit orchestrator

This enhanced version:
1. Automatically runs with report generation
2. Displays key findings immediately
3. Provides quick access to common troubleshooting tasks
4. Shows real-time analysis of critical issues
"""

import os
import sys
import shutil
import subprocess
import argparse
import json
import time
from pathlib import Path
from datetime import datetime

# Configuration
GOOGLE_DRIVE_SOURCE = '/content/drive/MyDrive/Business/Trade/software/Code/Diagnosis'
COLAB_DIAGNOSTIC_DIR = '/content/diagnostic_toolkit'
COLAB_TRADING_DIR = '/content/trading_system'
REPORTS_DIR = '/content/diagnostic_reports'

# Diagnostic files to copy
DIAGNOSTIC_FILES = [
    'diagnostic_toolkit.py',
    'diagnostic_service_integration.py',
    'diagnostic_log_analysis.py',
    'diagnostic_process_ports.py'
]

# Required Python packages for diagnostics
REQUIRED_PACKAGES = [
    'psutil',
    'tabulate',
    'requests'
]

# ANSI color codes for better output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def print_banner():
    """Print enhanced setup banner"""
    banner = f"""
{Colors.HEADER}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë     TRADING SYSTEM DIAGNOSTIC TOOLKIT v1.0.6 - ENHANCED    ‚ïë
‚ïë                                                            ‚ïë
‚ïë  Features:                                                 ‚ïë
‚ïë  ‚Ä¢ Automatic report generation                             ‚ïë
‚ïë  ‚Ä¢ Real-time issue detection                              ‚ïë
‚ïë  ‚Ä¢ Quick troubleshooting options                          ‚ïë
‚ïë  ‚Ä¢ Enhanced analysis display                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
    """
    print(banner)

def mount_google_drive():
    """Mount Google Drive in Colab"""
    print(f"\n{Colors.OKBLUE}üìÅ Mounting Google Drive...{Colors.ENDC}")
    try:
        from google.colab import drive
        drive.mount('/content/drive', force_remount=False)
        print(f"{Colors.OKGREEN}‚úÖ Google Drive mounted successfully{Colors.ENDC}")
        return True
    except ImportError:
        print(f"{Colors.FAIL}‚ùå Not running in Google Colab environment{Colors.ENDC}")
        return False
    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Error mounting Google Drive: {str(e)}{Colors.ENDC}")
        return False

def create_directory_structure():
    """Create required directories"""
    print(f"\n{Colors.OKBLUE}üìÇ Creating directory structure...{Colors.ENDC}")
    
    directories = [
        COLAB_DIAGNOSTIC_DIR,
        REPORTS_DIR,
        '/content/logs'
    ]
    
    for directory in directories:
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            print(f"  ‚úì Created: {directory}")
        except Exception as e:
            print(f"  {Colors.FAIL}‚úó Error creating {directory}: {str(e)}{Colors.ENDC}")

def copy_diagnostic_files():
    """Copy diagnostic files from Google Drive to Colab"""
    print(f"\n{Colors.OKBLUE}üìã Copying diagnostic files from Google Drive...{Colors.ENDC}")
    
    # Check both possible path formats
    possible_paths = [
        Path(GOOGLE_DRIVE_SOURCE),
        Path('/content/drive/My Drive/Business/Trade/software/Code/Diagnosis')
    ]
    
    source_path = None
    for path in possible_paths:
        if path.exists():
            source_path = path
            break
    
    if source_path is None:
        print(f"{Colors.FAIL}‚ùå Source directory not found{Colors.ENDC}")
        return False
    
    print(f"{Colors.OKGREEN}‚úÖ Found source directory: {source_path}{Colors.ENDC}")
    
    copied_count = 0
    for file_name in DIAGNOSTIC_FILES:
        source_file = source_path / file_name
        dest_path = Path(COLAB_DIAGNOSTIC_DIR) / file_name
        
        if source_file.exists():
            try:
                shutil.copy2(source_file, dest_path)
                print(f"  ‚úì Copied: {file_name}")
                copied_count += 1
            except Exception as e:
                print(f"  {Colors.FAIL}‚úó Error copying {file_name}: {str(e)}{Colors.ENDC}")
        else:
            print(f"  {Colors.WARNING}‚ö† Not found: {file_name}{Colors.ENDC}")
    
    print(f"\nüìä Copied {copied_count}/{len(DIAGNOSTIC_FILES)} files")
    return copied_count > 0

def install_dependencies():
    """Install required Python packages"""
    print(f"\n{Colors.OKBLUE}üì¶ Installing diagnostic dependencies...{Colors.ENDC}")
    
    for package in REQUIRED_PACKAGES:
        print(f"  Installing {package}...", end='', flush=True)
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', package], 
                         check=True, capture_output=True)
            print(" ‚úì")
        except subprocess.CalledProcessError:
            print(f" {Colors.FAIL}‚úó{Colors.ENDC}")

def run_diagnostic_with_analysis(args):
    """Run diagnostic and analyze results"""
    print(f"\n{Colors.HEADER}üè• Running Comprehensive Diagnostic...{Colors.ENDC}")
    print("=" * 70)
    
    try:
        os.chdir(COLAB_DIAGNOSTIC_DIR)
        
        # Always run with report generation
        cmd = [sys.executable, 'diagnostic_toolkit.py', '--report']
        
        # Add additional arguments if specified
        if args.quick:
            cmd.append('--quick')
        if args.service:
            cmd.extend(['--service', args.service])
        if args.errors_only:
            cmd.append('--errors-only')
        if args.last_minutes:
            cmd.extend(['--last-minutes', str(args.last_minutes)])
        
        # Run diagnostic
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Display output
        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(f"{Colors.FAIL}{result.stderr}{Colors.ENDC}")
        
        # Analyze results
        if result.returncode == 0:
            print(f"\n{Colors.OKGREEN}‚úÖ Diagnostic completed successfully!{Colors.ENDC}")
            return analyze_diagnostic_results()
        else:
            print(f"\n{Colors.FAIL}‚ùå Diagnostic failed with return code: {result.returncode}{Colors.ENDC}")
            return False
            
    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Error running diagnostic: {str(e)}{Colors.ENDC}")
        return False

def analyze_diagnostic_results():
    """Analyze the generated diagnostic results"""
    print(f"\n{Colors.HEADER}üìä Analyzing Diagnostic Results...{Colors.ENDC}")
    print("-" * 50)
    
    # Find the latest JSON report
    reports_dir = Path(REPORTS_DIR)
    json_reports = list(reports_dir.glob('trading_system_diagnostic_*.json'))
    
    if not json_reports:
        print(f"{Colors.FAIL}‚ùå No diagnostic reports found{Colors.ENDC}")
        return False
    
    latest_report = max(json_reports, key=lambda x: x.stat().st_mtime)
    
    try:
        with open(latest_report, 'r') as f:
            data = json.load(f)
        
        # Extract key information
        comprehensive = data.get('comprehensive_analysis', {})
        process_analysis = data.get('process_analysis', {})
        log_analysis = data.get('log_analysis', {})
        
        # Overall health
        overall_health = comprehensive.get('overall_system_health', 'unknown')
        system_score = comprehensive.get('system_scores', {}).get('overall', 0)
        
        # Color code the health status
        if overall_health == 'excellent':
            health_color = Colors.OKGREEN
        elif overall_health == 'good':
            health_color = Colors.OKCYAN
        elif overall_health == 'fair':
            health_color = Colors.WARNING
        else:
            health_color = Colors.FAIL
        
        print(f"\n{Colors.BOLD}System Health Summary:{Colors.ENDC}")
        print(f"  Overall Health: {health_color}{overall_health.upper()}{Colors.ENDC} ({system_score:.0f}/100)")
        
        # Service status
        services_total = process_analysis.get('services_total', 0)
        services_responding = process_analysis.get('services_responding', 0)
        print(f"  Services Responding: {services_responding}/{services_total}")
        
        # Critical issues
        critical_issues = comprehensive.get('critical_issues', [])
        if critical_issues:
            print(f"\n{Colors.FAIL}üö® Critical Issues Found:{Colors.ENDC}")
            for issue in critical_issues:
                print(f"     ‚Ä¢ {issue}")
        
        # Log errors
        total_errors = log_analysis.get('summary', {}).get('total_errors', 0)
        if total_errors > 0:
            print(f"\n{Colors.WARNING}‚ö†Ô∏è  Log Errors: {total_errors} errors detected{Colors.ENDC}")
            
            # Show top error services
            service_analyses = log_analysis.get('service_analyses', {})
            error_services = []
            for service, analysis in service_analyses.items():
                error_count = analysis.get('error_counts', {}).get('error', 0)
                if error_count > 0:
                    error_services.append((service, error_count))
            
            if error_services:
                error_services.sort(key=lambda x: x[1], reverse=True)
                print("     Top error services:")
                for service, count in error_services[:3]:
                    print(f"       - {service}: {count} errors")
        
        # Recommendations
        recommendations = comprehensive.get('recommendations', [])
        if recommendations:
            print(f"\n{Colors.OKBLUE}üí° Recommendations:{Colors.ENDC}")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"     {i}. {rec}")
        
        # Report location
        print(f"\n{Colors.OKGREEN}üìÑ Full Reports Generated:{Colors.ENDC}")
        print(f"     ‚Ä¢ HTML Report: {reports_dir}/diagnostic_report_latest.html")
        print(f"     ‚Ä¢ JSON Data: {latest_report.name}")
        
        # Quick actions based on findings
        print(f"\n{Colors.HEADER}üîß Quick Actions:{Colors.ENDC}")
        
        if total_errors > 0 and 'database is locked' in str(data):
            print(f"     1. {Colors.WARNING}Database locking detected{Colors.ENDC} - Run: fix_database_locking()")
        
        if 'pattern analysis' in str(critical_issues).lower():
            print(f"     2. {Colors.WARNING}Pattern analysis issues{Colors.ENDC} - Run: check_pattern_service()")
        
        if total_errors > 10:
            print(f"     3. {Colors.WARNING}High error count{Colors.ENDC} - Run: analyze_service_logs()")
        
        return True
        
    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Error analyzing results: {str(e)}{Colors.ENDC}")
        return False

def show_quick_commands():
    """Display quick diagnostic commands"""
    print(f"\n{Colors.HEADER}üìö Quick Diagnostic Commands:{Colors.ENDC}")
    print("-" * 50)
    
    commands = [
        ("Full diagnostic with report", "!python setup_diagnostic_toolkit_v106.py"),
        ("Quick health check", "!python setup_diagnostic_toolkit_v106.py --quick"),
        ("Check specific service", "!python setup_diagnostic_toolkit_v106.py --service pattern"),
        ("Analyze recent logs only", "!python setup_diagnostic_toolkit_v106.py --last-minutes 5"),
        ("Show errors only", "!python setup_diagnostic_toolkit_v106.py --errors-only"),
    ]
    
    for desc, cmd in commands:
        print(f"  {desc:.<35} {Colors.OKCYAN}{cmd}{Colors.ENDC}")

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Enhanced Trading System Diagnostic Toolkit Setup',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Diagnostic options
    parser.add_argument('--quick', action='store_true', 
                       help='Quick health check (skip detailed analysis)')
    parser.add_argument('--service', 
                       help='Focus on specific service')
    parser.add_argument('--errors-only', action='store_true',
                       help='Show only errors in analysis')
    parser.add_argument('--last-minutes', type=int,
                       help='Analyze only last N minutes')
    
    # Setup options
    parser.add_argument('--skip-mount', action='store_true',
                       help='Skip Google Drive mounting')
    parser.add_argument('--skip-deps', action='store_true',
                       help='Skip dependency installation')
    parser.add_argument('--skip-copy', action='store_true',
                       help='Skip file copying (use existing files)')
    
    return parser.parse_args()

def main(args=None):
    """Main setup and execution function"""
    if args is None:
        args = parse_arguments()
    
    print_banner()
    
    # Setup steps
    if not args.skip_mount:
        if not mount_google_drive():
            print(f"\n{Colors.FAIL}‚ùå Setup failed: Could not mount Google Drive{Colors.ENDC}")
            return False
    
    create_directory_structure()
    
    if not args.skip_copy:
        if not copy_diagnostic_files():
            print(f"\n{Colors.FAIL}‚ùå Setup failed: Could not copy diagnostic files{Colors.ENDC}")
            return False
    
    if not args.skip_deps:
        install_dependencies()
    
    # Run diagnostic with analysis
    success = run_diagnostic_with_analysis(args)
    
    if success:
        show_quick_commands()
    
    return success

if __name__ == "__main__":
    # Check if running in notebook
    try:
        __IPYTHON__
        in_notebook = True
    except NameError:
        in_notebook = False
    
    if in_notebook:
        # Notebook execution - create simple args object
        class Args:
            def __init__(self):
                self.quick = False
                self.service = None
                self.errors_only = False
                self.last_minutes = None
                self.skip_mount = False
                self.skip_deps = False
                self.skip_copy = False
        
        args = Args()
        
        # Check if user wants quick mode
        print("üîß Running Enhanced Diagnostic Toolkit Setup")
        print("\nOptions:")
        print("1. Full diagnostic (recommended)")
        print("2. Quick health check")
        print("3. Skip setup, run diagnostic only")
        
        choice = input("\nEnter choice (1-3) or press Enter for option 1: ").strip()
        
        if choice == "2":
            args.quick = True
        elif choice == "3":
            args.skip_mount = True
            args.skip_copy = True
            args.skip_deps = True
        
        success = main(args)
    else:
        # Command line execution
        success = main()
    
    if not success:
        sys.exit(1)


================================================================================
FILE: ./Startup/enhanced_startup_manager_v201.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: enhanced_startup_manager.py
Version: 2.0.1
Last Updated: 2025-01-11

REVISION HISTORY:
- v2.0.1 (2025-01-11) - Integrated with google_drive_service_v101.py
- v2.0.0 (2025-01-10) - Enhanced startup sequence with comprehensive checks
- v1.0.0 (2024-12-28) - Initial version

PURPOSE:
Provides comprehensive startup management for the Trading System with
integrated Google Drive support via the unified service.
"""

import os
import sys
import time
import psutil
import sqlite3
import subprocess
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# Import the Google Drive service
from google_drive_service_v101 import get_drive_service

class EnhancedStartupManager:
    """
    Enhanced startup manager with comprehensive pre-flight checks,
    dependency management, and Google Drive integration.
    """
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """Initialize startup manager"""
        self.logger = logger or self._setup_default_logger()
        self.start_time = datetime.now()
        
        # Initialize Google Drive service
        self.drive_service = get_drive_service(self.logger)
        
        # System configuration
        self.services = {
            'coordination': {
                'name': 'coordination_service',
                'port': 5000,
                'critical': True,
                'dependencies': [],
                'startup_delay': 5
            },
            'scanner': {
                'name': 'security_scanner',
                'port': 5001,
                'critical': True,
                'dependencies': ['coordination'],
                'startup_delay': 3
            },
            'pattern': {
                'name': 'pattern_analysis',
                'port': 5002,
                'critical': True,
                'dependencies': ['coordination'],
                'startup_delay': 3
            },
            'technical': {
                'name': 'technical_analysis',
                'port': 5003,
                'critical': True,
                'dependencies': ['coordination'],
                'startup_delay': 3
            },
            'trading': {
                'name': 'paper_trading',
                'port': 5005,
                'critical': True,
                'dependencies': ['technical'],
                'startup_delay': 3
            },
            'pattern_rec': {
                'name': 'pattern_recognition_service',
                'port': 5006,
                'critical': True,
                'dependencies': ['coordination'],
                'startup_delay': 3
            },
            'news': {
                'name': 'news_service',
                'port': 5008,
                'critical': False,
                'dependencies': [],
                'startup_delay': 3
            },
            'reporting': {
                'name': 'reporting_service',
                'port': 5009,
                'critical': False,
                'dependencies': ['coordination'],
                'startup_delay': 3
            },
            'dashboard': {
                'name': 'web_dashboard',
                'port': 8080,
                'critical': False,
                'dependencies': ['coordination'],
                'startup_delay': 3
            }
        }
        
        # Track startup state
        self.startup_state = {
            'phase': 'initialization',
            'checks_passed': {},
            'services_started': {},
            'errors': [],
            'warnings': []
        }
        
    def _setup_default_logger(self) -> logging.Logger:
        """Setup default logger"""
        logger = logging.getLogger('EnhancedStartupManager')
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger
    
    def run_startup_sequence(self) -> bool:
        """Run the complete startup sequence"""
        try:
            self.logger.info("="*60)
            self.logger.info("üöÄ TRADING SYSTEM ENHANCED STARTUP SEQUENCE v2.0.1")
            self.logger.info("="*60)
            
            # Save initial state
            self._save_startup_state()
            
            # Phase 1: Pre-flight checks
            if not self._run_preflight_checks():
                return False
            
            # Phase 2: Database preparation
            if not self._prepare_database():
                return False
            
            # Phase 3: Service startup
            if not self._start_services():
                return False
            
            # Phase 4: Post-startup validation
            if not self._validate_system():
                return False
            
            # Phase 5: Generate report
            self._generate_startup_report()
            
            self.logger.info("‚úÖ STARTUP SEQUENCE COMPLETED SUCCESSFULLY")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Startup sequence failed: {e}")
            self.startup_state['errors'].append(f"Fatal error: {str(e)}")
            self._save_startup_state()
            return False
    
    def _run_preflight_checks(self) -> bool:
        """Run comprehensive pre-flight checks"""
        self.logger.info("\nüìã PHASE 1: PRE-FLIGHT CHECKS")
        self.startup_state['phase'] = 'preflight_checks'
        
        checks = [
            ('Google Drive Connection', self._check_google_drive),
            ('System Resources', self._check_system_resources),
            ('Port Availability', self._check_ports),
            ('Python Dependencies', self._check_dependencies),
            ('Service Files', self._check_service_files),
            ('Configuration Files', self._check_config_files)
        ]
        
        all_passed = True
        for check_name, check_func in checks:
            try:
                passed, message = check_func()
                self.startup_state['checks_passed'][check_name] = passed
                
                if passed:
                    self.logger.info(f"‚úÖ {check_name}: {message}")
                else:
                    self.logger.error(f"‚ùå {check_name}: {message}")
                    self.startup_state['errors'].append(f"{check_name}: {message}")
                    all_passed = False
                    
            except Exception as e:
                self.logger.error(f"‚ùå {check_name}: Exception - {str(e)}")
                self.startup_state['checks_passed'][check_name] = False
                self.startup_state['errors'].append(f"{check_name}: {str(e)}")
                all_passed = False
        
        self._save_startup_state()
        return all_passed
    
    def _check_google_drive(self) -> Tuple[bool, str]:
        """Check Google Drive connection"""
        try:
            # Test by listing files in data folder
            files = self.drive_service.list_files('data')
            return True, f"Connected, {len(files)} files in data folder"
        except Exception as e:
            return False, f"Connection failed: {str(e)}"
    
    def _check_system_resources(self) -> Tuple[bool, str]:
        """Check system resources"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        issues = []
        if cpu_percent > 90:
            issues.append(f"High CPU usage: {cpu_percent}%")
        if memory.percent > 90:
            issues.append(f"High memory usage: {memory.percent}%")
        if disk.percent > 95:
            issues.append(f"Low disk space: {disk.percent}% used")
        
        if issues:
            return False, "; ".join(issues)
        
        return True, f"CPU: {cpu_percent}%, Memory: {memory.percent}%, Disk: {disk.percent}%"
    
    def _check_ports(self) -> Tuple[bool, str]:
        """Check if required ports are available"""
        occupied_ports = []
        
        for service_id, config in self.services.items():
            port = config['port']
            if self._is_port_in_use(port):
                occupied_ports.append(f"{config['name']}:{port}")
        
        if occupied_ports:
            return False, f"Ports in use: {', '.join(occupied_ports)}"
        
        return True, "All ports available"
    
    def _is_port_in_use(self, port: int) -> bool:
        """Check if a port is in use"""
        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(('localhost', port)) == 0
    
    def _check_dependencies(self) -> Tuple[bool, str]:
        """Check Python dependencies"""
        required_packages = [
            'flask', 'requests', 'pandas', 'numpy', 'yfinance',
            'psutil', 'tabulate', 'googleapiclient', 'google.auth'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package.split('.')[0])
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            return False, f"Missing packages: {', '.join(missing_packages)}"
        
        return True, "All dependencies installed"
    
    def _check_service_files(self) -> Tuple[bool, str]:
        """Check if all service files exist in Google Drive"""
        missing_files = []
        
        # List all Python files in the main directory
        files = self.drive_service.list_files()
        file_names = [f['name'] for f in files]
        
        for service_id, config in self.services.items():
            filename = f"{config['name']}.py"
            if filename not in file_names:
                missing_files.append(filename)
        
        # Check for hybrid_manager.py
        if 'hybrid_manager.py' not in file_names:
            missing_files.append('hybrid_manager.py')
        
        if missing_files:
            return False, f"Missing files: {', '.join(missing_files)}"
        
        return True, "All service files present"
    
    def _check_config_files(self) -> Tuple[bool, str]:
        """Check configuration files"""
        try:
            # Check if config exists
            config = self.drive_service.load_json('trading_config.json', 'config')
            if not config:
                # Create default config
                default_config = {
                    'system_name': 'Trading System Phase 1',
                    'version': '2.0.1',
                    'startup_manager': 'enhanced_startup_manager_v201',
                    'services': self.services,
                    'created_at': datetime.now().isoformat()
                }
                self.drive_service.save_json('trading_config.json', default_config, 'config')
                return True, "Created default configuration"
            
            return True, "Configuration loaded"
            
        except Exception as e:
            return False, f"Config error: {str(e)}"
    
    def _prepare_database(self) -> bool:
        """Prepare database for operation"""
        self.logger.info("\nüóÑÔ∏è PHASE 2: DATABASE PREPARATION")
        self.startup_state['phase'] = 'database_preparation'
        
        try:
            # Check database info
            db_info = self.drive_service.get_database_info()
            
            if not db_info['exists']:
                self.logger.info("Database not found, creating new database...")
                # Download and run database migration
                try:
                    migration_content = self.drive_service.read_file('database_migration.py')
                    # Save temporarily and execute
                    temp_migration = Path('/tmp/database_migration.py')
                    temp_migration.write_bytes(migration_content)
                    
                    result = subprocess.run(
                        [sys.executable, str(temp_migration)],
                        capture_output=True,
                        text=True
                    )
                    
                    if result.returncode != 0:
                        raise Exception(f"Migration failed: {result.stderr}")
                    
                    self.logger.info("‚úÖ Database created successfully")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Database creation failed: {e}")
                    return False
            else:
                self.logger.info(f"‚úÖ Database exists: {db_info['size']} bytes, "
                               f"last modified: {db_info['last_modified']}")
            
            # Verify database structure
            if not self._verify_database_structure():
                return False
            
            self._save_startup_state()
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Database preparation failed: {e}")
            self.startup_state['errors'].append(f"Database error: {str(e)}")
            self._save_startup_state()
            return False
    
    def _verify_database_structure(self) -> bool:
        """Verify database has required tables"""
        # For Google Drive, we'll download the DB temporarily to check
        try:
            db_content = self.drive_service.read_file('trading.db', 'data')
            temp_db = Path('/tmp/trading_check.db')
            temp_db.write_bytes(db_content)
            
            conn = sqlite3.connect(str(temp_db))
            cursor = conn.cursor()
            
            # Check for required tables
            required_tables = [
                'services', 'api_connections', 'system_metrics',
                'security_logs', 'trading_signals', 'paper_trades'
            ]
            
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            existing_tables = [row[0] for row in cursor.fetchall()]
            
            missing_tables = [t for t in required_tables if t not in existing_tables]
            
            conn.close()
            temp_db.unlink()
            
            if missing_tables:
                self.logger.warning(f"‚ö†Ô∏è Missing tables: {', '.join(missing_tables)}")
                return False
            
            self.logger.info("‚úÖ Database structure verified")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Database verification failed: {e}")
            return False
    
    def _start_services(self) -> bool:
        """Start all services in dependency order"""
        self.logger.info("\nüöÄ PHASE 3: SERVICE STARTUP")
        self.startup_state['phase'] = 'service_startup'
        
        # Kill any existing processes first
        self._cleanup_existing_processes()
        
        # Determine startup method
        if self._should_use_hybrid_manager():
            return self._start_with_hybrid_manager()
        else:
            return self._start_services_directly()
    
    def _cleanup_existing_processes(self):
        """Clean up any existing service processes"""
        self.logger.info("Cleaning up existing processes...")
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = proc.info.get('cmdline', [])
                if cmdline and any('_service.py' in arg for arg in cmdline):
                    self.logger.info(f"Terminating existing process: {proc.info['name']} (PID: {proc.info['pid']})")
                    proc.terminate()
                    proc.wait(timeout=5)
            except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                pass
    
    def _should_use_hybrid_manager(self) -> bool:
        """Determine if hybrid manager should be used"""
        try:
            # Check if hybrid_manager.py exists
            files = self.drive_service.list_files()
            return any(f['name'] == 'hybrid_manager.py' for f in files)
        except:
            return False
    
    def _start_with_hybrid_manager(self) -> bool:
        """Start services using hybrid manager"""
        try:
            self.logger.info("Starting services with hybrid_manager.py...")
            
            # Download hybrid manager to temp location
            hybrid_content = self.drive_service.read_file('hybrid_manager.py')
            temp_hybrid = Path('/tmp/hybrid_manager.py')
            temp_hybrid.write_bytes(hybrid_content)
            
            # Start hybrid manager
            process = subprocess.Popen(
                [sys.executable, str(temp_hybrid), 'start'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # Wait for startup
            time.sleep(10)
            
            if process.poll() is not None:
                stdout, stderr = process.communicate()
                self.logger.error(f"Hybrid manager failed to start: {stderr.decode()}")
                return False
            
            self.logger.info("‚úÖ Hybrid manager started successfully")
            
            # Verify services are running
            time.sleep(5)
            return self._verify_all_services_running()
            
        except Exception as e:
            self.logger.error(f"‚ùå Hybrid manager startup failed: {e}")
            return False
    
    def _start_services_directly(self) -> bool:
        """Start services directly without hybrid manager"""
        self.logger.info("Starting services directly...")
        
        # Start in dependency order
        startup_order = self._get_startup_order()
        
        for service_id in startup_order:
            config = self.services[service_id]
            
            try:
                # Download service file to temp
                service_content = self.drive_service.read_file(f"{config['name']}.py")
                temp_service = Path(f"/tmp/{config['name']}.py")
                temp_service.write_bytes(service_content)
                
                # Start service
                process = subprocess.Popen(
                    [sys.executable, str(temp_service)],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
                
                # Wait for startup
                time.sleep(config['startup_delay'])
                
                # Check if started
                if process.poll() is not None:
                    stdout, stderr = process.communicate()
                    if config['critical']:
                        self.logger.error(f"‚ùå Critical service {config['name']} failed: {stderr.decode()}")
                        return False
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Non-critical service {config['name']} failed")
                        self.startup_state['warnings'].append(f"{config['name']} failed to start")
                else:
                    self.logger.info(f"‚úÖ Started {config['name']}")
                    self.startup_state['services_started'][service_id] = True
                    
            except Exception as e:
                if config['critical']:
                    self.logger.error(f"‚ùå Failed to start {config['name']}: {e}")
                    return False
                else:
                    self.logger.warning(f"‚ö†Ô∏è Failed to start {config['name']}: {e}")
                    self.startup_state['warnings'].append(f"{config['name']}: {str(e)}")
        
        self._save_startup_state()
        return True
    
    def _get_startup_order(self) -> List[str]:
        """Get service startup order based on dependencies"""
        order = []
        remaining = set(self.services.keys())
        
        while remaining:
            for service_id in list(remaining):
                config = self.services[service_id]
                deps = config['dependencies']
                
                if all(dep in order for dep in deps):
                    order.append(service_id)
                    remaining.remove(service_id)
        
        return order
    
    def _verify_all_services_running(self) -> bool:
        """Verify all services are running"""
        all_running = True
        
        for service_id, config in self.services.items():
            if self._check_service_health(config['name'], config['port']):
                self.logger.info(f"‚úÖ {config['name']} is healthy")
                self.startup_state['services_started'][service_id] = True
            else:
                if config['critical']:
                    self.logger.error(f"‚ùå Critical service {config['name']} not responding")
                    all_running = False
                else:
                    self.logger.warning(f"‚ö†Ô∏è Non-critical service {config['name']} not responding")
                    self.startup_state['warnings'].append(f"{config['name']} not responding")
        
        return all_running
    
    def _check_service_health(self, service_name: str, port: int) -> bool:
        """Check if a service is healthy"""
        import requests
        
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _validate_system(self) -> bool:
        """Validate the complete system is operational"""
        self.logger.info("\nüîç PHASE 4: SYSTEM VALIDATION")
        self.startup_state['phase'] = 'validation'
        
        try:
            # Run diagnostic toolkit if available
            diagnostic_result = self._run_diagnostic_toolkit()
            
            # Check coordination service
            coord_healthy = self._check_service_health('coordination_service', 5000)
            
            # Calculate system score
            services_running = len(self.startup_state['services_started'])
            total_services = len(self.services)
            system_score = (services_running / total_services) * 100
            
            self.logger.info(f"\nüìä SYSTEM STATUS:")
            self.logger.info(f"   Services Running: {services_running}/{total_services}")
            self.logger.info(f"   System Score: {system_score:.0f}/100")
            self.logger.info(f"   Coordination Service: {'‚úÖ Healthy' if coord_healthy else '‚ùå Unhealthy'}")
            
            if diagnostic_result:
                self.logger.info(f"   Diagnostic Score: {diagnostic_result}")
            
            self._save_startup_state()
            
            return system_score >= 80 and coord_healthy
            
        except Exception as e:
            self.logger.error(f"‚ùå System validation failed: {e}")
            return False
    
    def _run_diagnostic_toolkit(self) -> Optional[str]:
        """Run diagnostic toolkit if available"""
        try:
            # Check if diagnostic toolkit exists
            files = self.drive_service.list_files()
            diagnostic_file = next((f for f in files if 'diagnostic_toolkit' in f['name']), None)
            
            if diagnostic_file:
                # Download and run
                diagnostic_content = self.drive_service.read_file(diagnostic_file['name'])
                temp_diagnostic = Path(f"/tmp/{diagnostic_file['name']}")
                temp_diagnostic.write_bytes(diagnostic_content)
                
                result = subprocess.run(
                    [sys.executable, str(temp_diagnostic), '--report'],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                if result.returncode == 0:
                    # Parse score from output
                    for line in result.stdout.split('\n'):
                        if 'System Score:' in line:
                            return line.strip()
                
        except Exception as e:
            self.logger.debug(f"Diagnostic toolkit not available: {e}")
        
        return None
    
    def _generate_startup_report(self):
        """Generate comprehensive startup report"""
        self.logger.info("\nüìÑ PHASE 5: STARTUP REPORT GENERATION")
        
        duration = (datetime.now() - self.start_time).total_seconds()
        
        report = {
            'startup_time': self.start_time.isoformat(),
            'duration_seconds': duration,
            'implementation_id': 'STARTUP-' + datetime.now().strftime('%Y%m%d-%H%M%S'),
            'checks_passed': self.startup_state['checks_passed'],
            'services_started': self.startup_state['services_started'],
            'errors': self.startup_state['errors'],
            'warnings': self.startup_state['warnings'],
            'final_status': 'SUCCESS' if not self.startup_state['errors'] else 'FAILED'
        }
        
        # Save report to Google Drive
        report_filename = f"startup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.drive_service.save_json(report_filename, report, 'reports')
        
        self.logger.info(f"\nüìä STARTUP SUMMARY:")
        self.logger.info(f"   Duration: {duration:.1f} seconds")
        self.logger.info(f"   Checks Passed: {sum(1 for v in self.startup_state['checks_passed'].values() if v)}/{len(self.startup_state['checks_passed'])}")
        self.logger.info(f"   Services Started: {len(self.startup_state['services_started'])}/{len(self.services)}")
        self.logger.info(f"   Errors: {len(self.startup_state['errors'])}")
        self.logger.info(f"   Warnings: {len(self.startup_state['warnings'])}")
        self.logger.info(f"   Report Saved: {report_filename}")
    
    def _save_startup_state(self):
        """Save current startup state to Google Drive"""
        try:
            state_data = {
                'timestamp': datetime.now().isoformat(),
                'phase': self.startup_state['phase'],
                'checks_passed': self.startup_state['checks_passed'],
                'services_started': self.startup_state['services_started'],
                'errors': self.startup_state['errors'][-10:],  # Last 10 errors
                'warnings': self.startup_state['warnings'][-10:]  # Last 10 warnings
            }
            
            self.drive_service.save_json('startup_state.json', state_data, 'coordination')
            
        except Exception as e:
            self.logger.debug(f"Failed to save startup state: {e}")


def main():
    """Main entry point"""
    print("üöÄ ENHANCED STARTUP MANAGER v2.0.1")
    print("=" * 60)
    
    # Initialize and run
    manager = EnhancedStartupManager()
    success = manager.run_startup_sequence()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()


================================================================================
FILE: ./hybrid_components/config_manager.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - CONFIGURATION MANAGER
Version: 3.0.1
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.1 (2025-06-15) - Added dashboard service to DEFAULT_SERVICES
v3.0.0 (2025-06-15) - Initial configuration manager implementation
"""

import json
import logging
from pathlib import Path
from typing import Dict, Optional, List

class ConfigurationManager:
    """Manages system and service configurations"""
    
    # Default service registry matching v2.0 architecture
    DEFAULT_SERVICES = {
        "coordination": {
            "file": "coordination_service.py",
            "port": 5000,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": True,
            "dependencies": []
        },
        "scanner": {
            "file": "security_scanner.py",
            "port": 5001,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": True,
            "dependencies": ["coordination"]
        },
        "pattern": {
            "file": "pattern_analysis.py",
            "port": 5002,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": False,
            "dependencies": ["coordination"]
        },
        "technical": {
            "file": "technical_analysis.py",
            "port": 5003,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": True,
            "dependencies": ["coordination"]
        },
        "trading": {
            "file": "paper_trading.py",
            "port": 5005,
            "health_endpoint": "/health",
            "startup_delay": 3,
            "critical": False,
            "dependencies": ["coordination", "technical"]
        },
        "pattern_rec": {
            "file": "pattern_recognition_service.py",
            "port": 5006,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": False,
            "dependencies": []
        },
        "news": {
            "file": "news_service.py",
            "port": 5008,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": False,
            "dependencies": []
        },
        "reporting": {
            "file": "reporting_service.py",
            "port": 5009,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": False,
            "dependencies": ["coordination"]
        },
        "dashboard": {
            "file": "web_dashboard_service.py",
            "port": 5010,
            "health_endpoint": "/health",
            "startup_delay": 2,
            "critical": False,
            "dependencies": ["coordination", "trading", "reporting"]
        }
    }
    
    # Default system configuration
    DEFAULT_SYSTEM_CONFIG = {
        "checkpoint_interval": 300,  # 5 minutes
        "health_check_interval": 30,  # 30 seconds
        "max_restart_attempts": 3,
        "restart_backoff_base": 10,  # seconds
        "enable_auto_restart": True,
        "enable_checkpoints": True,
        "log_level": "INFO"
    }
    
    # Default paths
    DEFAULT_PATHS = {
        "services_dir": "/content/trading_system",
        "logs_dir": "/content/logs",
        "checkpoint_dir": "/content/drive/MyDrive/TradingBot/checkpoints",
        "backup_dir": "/content/drive/MyDrive/TradingBot/backups",
        "database_path": "/content/trading_system.db"
    }
    
    def __init__(self, config_file: Optional[str] = None):
        """Initialize configuration manager"""
        self.logger = logging.getLogger('ConfigurationManager')
        self.config_file = config_file
        
        # Initialize with defaults
        self.services = self.DEFAULT_SERVICES.copy()
        self.system_config = self.DEFAULT_SYSTEM_CONFIG.copy()
        self.paths = self.DEFAULT_PATHS.copy()
        
        # Load custom configuration if provided
        if config_file:
            self.load_config(config_file)
        
        # Ensure directories exist
        self._ensure_directories()
    
    def load_config(self, config_file: str) -> bool:
        """Load configuration from file"""
        try:
            config_path = Path(config_file)
            if not config_path.exists():
                self.logger.warning(f"Config file not found: {config_file}")
                return False
            
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            # Update configurations
            if 'services' in config:
                self.services.update(config['services'])
            
            if 'system' in config:
                self.system_config.update(config['system'])
            
            if 'paths' in config:
                self.paths.update(config['paths'])
            
            self.logger.info(f"Configuration loaded from {config_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error loading config: {str(e)}")
            return False
    
    def save_config(self, config_file: Optional[str] = None) -> bool:
        """Save current configuration to file"""
        try:
            config_path = Path(config_file or self.config_file or 'hybrid_config.json')
            
            config = {
                'version': '3.0.1',
                'services': self.services,
                'system': self.system_config,
                'paths': self.paths
            }
            
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            self.logger.info(f"Configuration saved to {config_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving config: {str(e)}")
            return False
    
    def get_services(self) -> Dict:
        """Get service configurations"""
        return self.services
    
    def get_service_config(self, service_name: str) -> Optional[Dict]:
        """Get configuration for specific service"""
        return self.services.get(service_name)
    
    def get_system_config(self) -> Dict:
        """Get system configuration"""
        return self.system_config
    
    def get_paths(self) -> Dict:
        """Get path configurations"""
        return self.paths
    
    def update_service_config(self, service_name: str, config: Dict):
        """Update configuration for a specific service"""
        if service_name in self.services:
            self.services[service_name].update(config)
            self.logger.info(f"Updated configuration for service: {service_name}")
    
    def update_system_config(self, config: Dict):
        """Update system configuration"""
        self.system_config.update(config)
        self.logger.info("Updated system configuration")
    
    def to_dict(self) -> Dict:
        """Convert configuration to dictionary"""
        return {
            'services': self.services,
            'system': self.system_config,
            'paths': self.paths
        }
    
    def update_from_dict(self, config: Dict):
        """Update configuration from dictionary"""
        if 'services' in config:
            self.services.update(config['services'])
        if 'system' in config:
            self.system_config.update(config['system'])
        if 'paths' in config:
            self.paths.update(config['paths'])
    
    def validate_config(self) -> bool:
        """Validate configuration integrity"""
        try:
            # Check required fields for each service
            required_fields = ['file', 'port', 'health_endpoint']
            for service_name, service_config in self.services.items():
                for field in required_fields:
                    if field not in service_config:
                        self.logger.error(f"Missing required field '{field}' for service '{service_name}'")
                        return False
            
            # Check for port conflicts
            ports = [s['port'] for s in self.services.values()]
            if len(ports) != len(set(ports)):
                self.logger.error("Port conflict detected in service configuration")
                return False
            
            # Validate system config values
            if self.system_config['checkpoint_interval'] < 60:
                self.logger.warning("Checkpoint interval is very short (< 60s)")
            
            if self.system_config['health_check_interval'] < 10:
                self.logger.warning("Health check interval is very short (< 10s)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Configuration validation error: {str(e)}")
            return False
    
    def _ensure_directories(self):
        """Ensure all configured directories exist"""
        for path_name, path_value in self.paths.items():
            if path_name.endswith('_dir'):
                try:
                    Path(path_value).mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    self.logger.warning(f"Could not create directory {path_value}: {str(e)}")
    
    def get_service_command(self, service_name: str) -> List[str]:
        """Get command to start a service"""
        service_config = self.services.get(service_name)
        if not service_config:
            return []
        
        service_file = Path(self.paths['services_dir']) / service_config['file']
        return ['python', '-u', str(service_file)]
    
    def get_service_dependencies(self, service_name: str) -> List[str]:
        """Get dependencies for a service"""
        service_config = self.services.get(service_name)
        if not service_config:
            return []
        
        return service_config.get('dependencies', [])
    
    def is_critical_service(self, service_name: str) -> bool:
        """Check if a service is critical"""
        service_config = self.services.get(service_name)
        if not service_config:
            return False
        
        return service_config.get('critical', False)


================================================================================
FILE: ./hybrid_components/lifecycle_manager.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - LIFECYCLE MANAGER
Version: 3.0.1
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.1 (2025-06-15) - Added dashboard to startup_order and stop_order
v3.0.0 (2025-06-15) - Initial lifecycle manager implementation
"""

import subprocess
import time
import logging
import requests
import os
from datetime import datetime
from typing import Dict, Optional
from dataclasses import dataclass

@dataclass
class ProcessInfo:
    """Information about a managed subprocess"""
    name: str
    config: dict
    process: Optional[subprocess.Popen] = None
    pid: Optional[int] = None
    started_at: Optional[datetime] = None
    restart_count: int = 0
    last_health_check: Optional[datetime] = None
    status: str = "stopped"  # stopped, starting, running, crashed, unhealthy

class LifecycleManager:
    """Manages service lifecycle - start, stop, restart, health checks"""
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = logging.getLogger('LifecycleManager')
        self.processes: Dict[str, ProcessInfo] = {}
        
        # Initialize process info for each service
        for name, config in manager.services.items():
            self.processes[name] = ProcessInfo(name=name, config=config)
    
    def start_all_services(self):
        """Start all services in dependency order"""
        self.logger.info("Starting all services")
        
        # Service startup order (based on dependencies)
        startup_order = [
            'coordination',      # Must start first
            'scanner',          # Needs coordination
            'pattern',          # Needs coordination
            'technical',        # Needs coordination
            'news',            # Independent
            'pattern_rec',     # Independent
            'trading',         # Needs technical analysis
            'reporting',        # Needs all others
            'dashboard'         # Web UI - starts last
        ]
        
        for service_name in startup_order:
            if service_name in self.processes:
                success = self.start_service(service_name)
                if not success and self._is_critical_service(service_name):
                    raise Exception(f"Failed to start critical service: {service_name}")
                
                # Wait between service starts
                time.sleep(self.processes[service_name].config.get('startup_delay', 2))
    
    def start_service(self, name: str) -> bool:
        """Start individual service"""
        if name not in self.processes:
            self.logger.error(f"Unknown service: {name}")
            return False
        
        process_info = self.processes[name]
        
        # Check if already running
        if process_info.status == "running" and self._is_process_alive(process_info):
            self.logger.info(f"Service {name} is already running")
            return True
        
        self.logger.info(f"Starting service: {name}")
        process_info.status = "starting"
        
        try:
            # Prepare command
            service_file = process_info.config['file']
            cmd = ['python', '-u', service_file]  # -u for unbuffered output
            
            # Start subprocess
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd='/content/trading_system',
                env=self._get_service_env()
            )
            
            # Update process info
            process_info.process = process
            process_info.pid = process.pid
            process_info.started_at = datetime.now()
            
            # Wait for startup
            time.sleep(process_info.config.get('startup_delay', 2))
            
            # Verify service is healthy
            if self._check_service_health(name):
                process_info.status = "running"
                self.logger.info(f"Service {name} started successfully (PID: {process.pid})")
                return True
            else:
                self.logger.error(f"Service {name} failed health check after startup")
                self.stop_service(name)
                process_info.status = "crashed"
                return False
                
        except Exception as e:
            self.logger.error(f"Error starting service {name}: {str(e)}")
            process_info.status = "crashed"
            return False
    
    def stop_service(self, name: str) -> bool:
        """Stop individual service"""
        if name not in self.processes:
            return False
        
        process_info = self.processes[name]
        
        if process_info.process and self._is_process_alive(process_info):
            self.logger.info(f"Stopping service: {name}")
            
            try:
                # Try graceful shutdown first
                process_info.process.terminate()
                
                # Wait up to 5 seconds for graceful shutdown
                for _ in range(50):
                    if process_info.process.poll() is not None:
                        break
                    time.sleep(0.1)
                
                # Force kill if still running
                if process_info.process.poll() is None:
                    self.logger.warning(f"Force killing service: {name}")
                    process_info.process.kill()
                    process_info.process.wait()
                
                process_info.status = "stopped"
                self.logger.info(f"Service {name} stopped")
                return True
                
            except Exception as e:
                self.logger.error(f"Error stopping service {name}: {str(e)}")
                return False
        
        return True
    
    def stop_all_services(self):
        """Stop all services in reverse order"""
        self.logger.info("Stopping all services")
        
        # Stop in reverse order
        stop_order = ['dashboard', 'reporting', 'trading', 'pattern_rec', 'news', 
                     'technical', 'pattern', 'scanner', 'coordination']
        
        for service_name in stop_order:
            if service_name in self.processes:
                self.stop_service(service_name)
    
    def restart_service(self, name: str, reason: str = "Manual restart") -> bool:
        """Restart a service"""
        if name not in self.processes:
            return False
        
        process_info = self.processes[name]
        process_info.restart_count += 1
        
        self.logger.info(f"Restarting service {name}: {reason}")
        
        # Check restart limit
        max_restarts = self.manager.config.get_system_config()['max_restart_attempts']
        if process_info.restart_count > max_restarts:
            self.logger.error(f"Service {name} exceeded max restart attempts ({max_restarts})")
            process_info.status = "failed"
            return False
        
        # Stop the service
        self.stop_service(name)
        
        # Wait before restart (exponential backoff)
        backoff_base = self.manager.config.get_system_config()['restart_backoff_base']
        wait_time = min(backoff_base * (2 ** (process_info.restart_count - 1)), 60)
        self.logger.info(f"Waiting {wait_time}s before restart")
        time.sleep(wait_time)
        
        # Start the service
        success = self.start_service(name)
        
        if success:
            # Reset restart count on successful restart
            process_info.restart_count = 0
        
        return success
    
    def check_service_health(self, name: str) -> bool:
        """Check if a service is healthy"""
        return self._check_service_health(name)
    
    def get_service_status(self, name: str) -> Dict:
        """Get detailed status of a service"""
        if name not in self.processes:
            return {'error': 'Unknown service'}
        
        process_info = self.processes[name]
        
        return {
            'name': name,
            'status': process_info.status,
            'pid': process_info.pid,
            'port': process_info.config.get('port'),
            'started_at': process_info.started_at.isoformat() if process_info.started_at else None,
            'uptime': (datetime.now() - process_info.started_at).total_seconds() if process_info.started_at else 0,
            'restart_count': process_info.restart_count,
            'last_health_check': process_info.last_health_check.isoformat() if process_info.last_health_check else None,
            'is_alive': self._is_process_alive(process_info),
            'is_healthy': self._check_service_health(name)
        }
    
    def _is_process_alive(self, process_info: ProcessInfo) -> bool:
        """Check if subprocess is still running"""
        if process_info.process:
            return process_info.process.poll() is None
        return False
    
    def _check_service_health(self, name: str) -> bool:
        """Check service health via HTTP endpoint"""
        if name not in self.processes:
            return False
        
        process_info = self.processes[name]
        
        # First check if process is alive
        if not self._is_process_alive(process_info):
            return False
        
        # Then check HTTP health endpoint
        try:
            port = process_info.config.get('port')
            health_endpoint = process_info.config.get('health_endpoint', '/health')
            url = f"http://localhost:{port}{health_endpoint}"
            
            response = requests.get(url, timeout=5)
            is_healthy = response.status_code == 200
            
            process_info.last_health_check = datetime.now()
            
            return is_healthy
            
        except Exception:
            return False
    
    def _get_service_env(self) -> dict:
        """Get environment variables for services"""
        env = os.environ.copy()
        env['PYTHONUNBUFFERED'] = '1'  # Ensure unbuffered output
        return env
    
    def _is_critical_service(self, name: str) -> bool:
        """Check if service is critical for system operation"""
        critical_services = ['coordination', 'scanner', 'technical']
        return name in critical_services


================================================================================
FILE: ./hybrid_components/monitoring_engine.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - MONITORING ENGINE
Version: 3.0.0
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.0 (2025-06-15) - Initial monitoring engine implementation
"""

import asyncio
import logging
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List

class MonitoringEngine:
    """Monitors service health and triggers auto-restart when needed"""
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = logging.getLogger('MonitoringEngine')
        
        # Monitoring state
        self.running = False
        self.monitor_thread = None
        self.check_count = 0
        self.last_checkpoint_save = None
        
        # Get monitoring intervals from config
        system_config = manager.config.get_system_config()
        self.health_check_interval = system_config['health_check_interval']
        self.checkpoint_interval = system_config['checkpoint_interval']
        
        # Metrics storage
        self.service_metrics = {}
        self._init_metrics()
    
    def start(self):
        """Start monitoring in background thread"""
        if self.running:
            self.logger.warning("Monitoring already running")
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        self.logger.info("Monitoring engine started")
    
    def stop(self):
        """Stop monitoring"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        self.logger.info("Monitoring engine stopped")
    
    def is_running(self) -> bool:
        """Check if monitoring is active"""
        return self.running
    
    def get_check_count(self) -> int:
        """Get number of health checks performed"""
        return self.check_count
    
    def get_service_metrics(self, service_name: str) -> Dict:
        """Get metrics for a specific service"""
        return self.service_metrics.get(service_name, {})
    
    def _init_metrics(self):
        """Initialize metrics storage for each service"""
        for service_name in self.manager.services:
            self.service_metrics[service_name] = {
                'health_checks_total': 0,
                'health_checks_passed': 0,
                'crashes_detected': 0,
                'auto_restarts': 0,
                'last_crash_time': None,
                'uptime_percentage': 100.0
            }
    
    def _monitoring_loop(self):
        """Main monitoring loop"""
        self.logger.info("Starting monitoring loop")
        self.last_checkpoint_save = datetime.now()
        
        while self.running:
            try:
                # Perform health checks
                self._check_all_services()
                
                # Save checkpoint if needed
                if self._should_save_checkpoint():
                    self.manager.recovery.save_checkpoint()
                    self.last_checkpoint_save = datetime.now()
                
                # Sleep until next check
                time.sleep(self.health_check_interval)
                
            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {str(e)}")
                time.sleep(5)  # Brief pause before continuing
    
    def _check_all_services(self):
        """Check health of all services"""
        self.check_count += 1
        
        for service_name in self.manager.services:
            self._check_service(service_name)
    
    def _check_service(self, service_name: str):
        """Check individual service health"""
        try:
            lifecycle = self.manager.lifecycle
            process_info = lifecycle.processes.get(service_name)
            
            if not process_info:
                return
            
            metrics = self.service_metrics[service_name]
            metrics['health_checks_total'] += 1
            
            # Check if process is alive
            is_alive = lifecycle._is_process_alive(process_info)
            
            if not is_alive and process_info.status == 'running':
                # Service crashed
                self.logger.error(f"Service {service_name} crashed!")
                metrics['crashes_detected'] += 1
                metrics['last_crash_time'] = datetime.now().isoformat()
                process_info.status = 'crashed'
                
                # Attempt auto-restart
                if self._should_restart_service(service_name):
                    self.logger.info(f"Auto-restarting service {service_name}")
                    success = lifecycle.restart_service(service_name, "Crash detected")
                    
                    if success:
                        metrics['auto_restarts'] += 1
                    else:
                        self.logger.error(f"Failed to auto-restart {service_name}")
                        self._handle_critical_failure(service_name)
            
            elif is_alive:
                # Check HTTP health
                is_healthy = lifecycle.check_service_health(service_name)
                
                if is_healthy:
                    metrics['health_checks_passed'] += 1
                    if process_info.status != 'running':
                        process_info.status = 'running'
                else:
                    # Service is running but unhealthy
                    self.logger.warning(f"Service {service_name} is unhealthy")
                    process_info.status = 'unhealthy'
                    
                    # Consider restart if persistently unhealthy
                    if self._is_persistently_unhealthy(service_name):
                        self.logger.warning(f"Service {service_name} persistently unhealthy, restarting")
                        lifecycle.restart_service(service_name, "Persistent health check failures")
                        metrics['auto_restarts'] += 1
            
            # Update uptime percentage
            if metrics['health_checks_total'] > 0:
                metrics['uptime_percentage'] = round(
                    (metrics['health_checks_passed'] / metrics['health_checks_total']) * 100, 2
                )
                
        except Exception as e:
            self.logger.error(f"Error checking service {service_name}: {str(e)}")
    
    def _should_restart_service(self, service_name: str) -> bool:
        """Determine if a service should be auto-restarted"""
        process_info = self.manager.lifecycle.processes.get(service_name)
        if not process_info:
            return False
        
        # Check restart limit
        max_restarts = self.manager.config.get_system_config()['max_restart_attempts']
        if process_info.restart_count >= max_restarts:
            self.logger.error(f"Service {service_name} exceeded max restart attempts")
            return False
        
        # Check if it's a critical service
        critical_services = ['coordination', 'scanner', 'technical']
        if service_name in critical_services:
            return True
        
        # For non-critical services, check if system can continue
        running_services = sum(1 for p in self.manager.lifecycle.processes.values() 
                              if p.status == 'running')
        
        # Restart if we have enough other services running
        return running_services >= 4
    
    def _is_persistently_unhealthy(self, service_name: str) -> bool:
        """Check if a service has been unhealthy for too long"""
        metrics = self.service_metrics[service_name]
        
        # If last 5 health checks failed, consider it persistently unhealthy
        recent_checks = min(5, metrics['health_checks_total'])
        recent_failures = metrics['health_checks_total'] - metrics['health_checks_passed']
        
        return recent_failures >= recent_checks
    
    def _should_save_checkpoint(self) -> bool:
        """Determine if it's time to save a checkpoint"""
        if not self.last_checkpoint_save:
            return True
        
        time_since_last = datetime.now() - self.last_checkpoint_save
        return time_since_last.total_seconds() >= self.checkpoint_interval
    
    def _handle_critical_failure(self, service_name: str):
        """Handle critical service failures"""
        critical_services = ['coordination', 'scanner', 'technical']
        
        if service_name in critical_services:
            self.logger.critical(f"Critical service {service_name} failed to restart!")
            
            # Save emergency checkpoint
            self.manager.recovery.save_checkpoint()
            
            # Consider stopping the system if coordination service fails
            if service_name == 'coordination':
                self.logger.critical("Coordination service failure - stopping system")
                self.manager.stop()
    
    def get_system_health_summary(self) -> Dict:
        """Get overall system health summary"""
        total_services = len(self.manager.services)
        running_services = sum(1 for p in self.manager.lifecycle.processes.values() 
                              if p.status == 'running')
        
        # Calculate overall health score
        health_score = (running_services / total_services) * 100
        
        # Get service with most crashes
        worst_service = max(self.service_metrics.items(), 
                           key=lambda x: x[1]['crashes_detected'])
        
        return {
            'health_score': round(health_score, 1),
            'total_services': total_services,
            'running_services': running_services,
            'total_health_checks': self.check_count,
            'monitoring_uptime': self._get_monitoring_uptime(),
            'worst_performing_service': {
                'name': worst_service[0],
                'crashes': worst_service[1]['crashes_detected'],
                'uptime_percentage': worst_service[1]['uptime_percentage']
            },
            'last_checkpoint': self.last_checkpoint_save.isoformat() if self.last_checkpoint_save else None
        }
    
    def _get_monitoring_uptime(self) -> float:
        """Get monitoring engine uptime in seconds"""
        # This is a simplified calculation
        return self.check_count * self.health_check_interval


================================================================================
FILE: ./hybrid_components/recovery_manager.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - RECOVERY MANAGER
Version: 3.0.0
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.0 (2025-06-15) - Initial recovery manager implementation
"""

import json
import logging
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional

class RecoveryManager:
    """Handles checkpoints and recovery for Colab disconnections"""
    
    def __init__(self, manager):
        self.manager = manager
        self.logger = logging.getLogger('RecoveryManager')
        
        # Checkpoint paths
        checkpoint_dir = manager.config.get_paths()['checkpoint_dir']
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.latest_checkpoint = self.checkpoint_dir / 'checkpoint_latest.json'
        self.checkpoint_count = 0
        self.last_checkpoint_time = None
    
    def create_checkpoint(self) -> Dict:
        """Create a checkpoint of current system state"""
        checkpoint = {
            'version': '3.0.0',
            'timestamp': datetime.now().isoformat(),
            'system_state': {
                'uptime': self.manager.get_uptime(),
                'start_time': self.manager.start_time.isoformat() if self.manager.start_time else None,
                'services': {}
            },
            'service_metrics': {},
            'configuration': self.manager.config.to_dict()
        }
        
        # Get state of each service
        for service_name in self.manager.services:
            service_status = self.manager.lifecycle.get_service_status(service_name)
            checkpoint['system_state']['services'][service_name] = {
                'status': service_status['status'],
                'pid': service_status['pid'],
                'port': service_status['port'],
                'started_at': service_status['started_at'],
                'restart_count': service_status['restart_count']
            }
            
            # Add metrics if available
            checkpoint['service_metrics'][service_name] = {
                'uptime_seconds': service_status['uptime'],
                'health_checks_passed': service_status['is_healthy'],
                'last_health_check': service_status['last_health_check']
            }
        
        return checkpoint
    
    def save_checkpoint(self) -> bool:
        """Save checkpoint to disk"""
        try:
            checkpoint = self.create_checkpoint()
            
            # Save to latest checkpoint file
            with open(self.latest_checkpoint, 'w') as f:
                json.dump(checkpoint, f, indent=2)
            
            # Also save timestamped backup
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = self.checkpoint_dir / f'checkpoint_{timestamp}.json'
            shutil.copy2(self.latest_checkpoint, backup_file)
            
            # Save daily backup
            daily_file = self.checkpoint_dir / f'checkpoint_daily_{datetime.now().strftime("%Y%m%d")}.json'
            shutil.copy2(self.latest_checkpoint, daily_file)
            
            # Clean old checkpoints (keep last 10)
            self._cleanup_old_checkpoints()
            
            self.checkpoint_count += 1
            self.last_checkpoint_time = datetime.now()
            
            self.logger.info(f"Checkpoint saved successfully (#{self.checkpoint_count})")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving checkpoint: {str(e)}")
            return False
    
    def restore_from_checkpoint(self, checkpoint_path: Optional[Path] = None) -> bool:
        """Restore system from checkpoint"""
        try:
            # Use provided checkpoint or latest
            checkpoint_file = checkpoint_path or self.latest_checkpoint
            
            if not checkpoint_file.exists():
                self.logger.warning("No checkpoint found to restore from")
                return False
            
            # Load checkpoint
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
            
            self.logger.info(f"Restoring from checkpoint: {checkpoint['timestamp']}")
            
            # Restore configuration if different
            saved_config = checkpoint.get('configuration', {})
            if saved_config:
                self.manager.config.update_from_dict(saved_config)
            
            # Restore services that were running
            services_to_start = []
            for service_name, service_state in checkpoint['system_state']['services'].items():
                if service_state['status'] in ['running', 'starting']:
                    services_to_start.append(service_name)
            
            # Run database migration first
            self.manager._run_database_migration()
            
            # Start services
            self.logger.info(f"Restarting {len(services_to_start)} services from checkpoint")
            for service_name in services_to_start:
                success = self.manager.lifecycle.start_service(service_name)
                if not success:
                    self.logger.warning(f"Failed to restore service: {service_name}")
            
            self.logger.info("System restored from checkpoint")
            return True
            
        except Exception as e:
            self.logger.error(f"Error restoring from checkpoint: {str(e)}")
            return False
    
    def save_final_state(self):
        """Save final state before shutdown"""
        try:
            # Create final checkpoint
            checkpoint = self.create_checkpoint()
            checkpoint['shutdown_time'] = datetime.now().isoformat()
            checkpoint['shutdown_type'] = 'graceful'
            
            # Save to special file
            final_state_file = self.checkpoint_dir / 'final_state.json'
            with open(final_state_file, 'w') as f:
                json.dump(checkpoint, f, indent=2)
            
            self.logger.info("Final state saved successfully")
            
        except Exception as e:
            self.logger.error(f"Error saving final state: {str(e)}")
    
    def checkpoint_exists(self) -> bool:
        """Check if a checkpoint exists"""
        return self.latest_checkpoint.exists()
    
    def get_last_checkpoint_time(self) -> Optional[str]:
        """Get timestamp of last checkpoint"""
        if self.last_checkpoint_time:
            return self.last_checkpoint_time.isoformat()
        
        # Check file modification time
        if self.latest_checkpoint.exists():
            mtime = datetime.fromtimestamp(self.latest_checkpoint.stat().st_mtime)
            return mtime.isoformat()
        
        return None
    
    def get_checkpoint_count(self) -> int:
        """Get number of checkpoints saved in this session"""
        return self.checkpoint_count
    
    def list_available_checkpoints(self) -> list:
        """List all available checkpoint files"""
        checkpoints = []
        
        for checkpoint_file in self.checkpoint_dir.glob('checkpoint_*.json'):
            try:
                with open(checkpoint_file, 'r') as f:
                    data = json.load(f)
                
                checkpoints.append({
                    'filename': checkpoint_file.name,
                    'path': str(checkpoint_file),
                    'timestamp': data.get('timestamp'),
                    'version': data.get('version'),
                    'service_count': len(data.get('system_state', {}).get('services', {}))
                })
            except:
                pass
        
        return sorted(checkpoints, key=lambda x: x['timestamp'], reverse=True)
    
    def _cleanup_old_checkpoints(self):
        """Remove old checkpoint files, keeping the most recent ones"""
        try:
            # Get all timestamped checkpoints
            checkpoint_files = list(self.checkpoint_dir.glob('checkpoint_20*.json'))
            
            # Sort by modification time
            checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Keep only the 10 most recent
            for old_file in checkpoint_files[10:]:
                old_file.unlink()
                self.logger.debug(f"Removed old checkpoint: {old_file.name}")
                
        except Exception as e:
            self.logger.warning(f"Error cleaning up old checkpoints: {str(e)}")


================================================================================
FILE: ./hybrid_components/service_manager.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - SERVICE MANAGER
Version: 3.0.0
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.0 (2025-06-15) - Initial hybrid service manager implementation
"""

import os
import time
import logging
import asyncio
from datetime import datetime
from typing import Dict, Optional
from .lifecycle_manager import LifecycleManager
from .recovery_manager import RecoveryManager
from .monitoring_engine import MonitoringEngine
from .config_manager import ConfigurationManager

class HybridServiceManager:
    """Main manager for hybrid architecture - manages v2.0 services with simplified approach"""
    
    def __init__(self, config: ConfigurationManager):
        """Initialize the hybrid service manager"""
        self.config = config
        self.logger = logging.getLogger('HybridServiceManager')
        
        # Service registry from configuration
        self.services = config.get_services()
        
        # Component managers
        self.lifecycle = LifecycleManager(self)
        self.recovery = RecoveryManager(self)
        self.monitor = MonitoringEngine(self)
        
        # State tracking
        self.running = False
        self.start_time = None
        
        self.logger.info("Hybrid Service Manager initialized")
    
    def start(self, recovery_mode: bool = False):
        """Start the trading system"""
        try:
            self.start_time = datetime.now()
            
            if recovery_mode and self.recovery.checkpoint_exists():
                # Recover from checkpoint
                self.logger.info("Starting in recovery mode")
                self.recovery.restore_from_checkpoint()
            else:
                # Fresh start
                self.logger.info("Starting fresh system")
                
                # Run database migration first
                self._run_database_migration()
                
                # Start all services
                self.lifecycle.start_all_services()
            
            # Mark system as running
            self.running = True
            
            # Start monitoring in background
            self.monitor.start()
            
            # Save initial checkpoint
            self.recovery.save_checkpoint()
            
            self.logger.info("Trading system started successfully")
            
        except Exception as e:
            self.logger.error(f"Error starting system: {str(e)}")
            self.stop()
            raise
    
    def stop(self):
        """Graceful shutdown of all services"""
        try:
            self.logger.info("Initiating graceful shutdown")
            self.running = False
            
            # Stop monitoring
            self.monitor.stop()
            
            # Save final state
            if self.recovery:
                self.recovery.save_final_state()
            
            # Stop all services
            self.lifecycle.stop_all_services()
            
            self.logger.info("Trading system stopped successfully")
            
        except Exception as e:
            self.logger.error(f"Error during shutdown: {str(e)}")
    
    def restart_service(self, service_name: str) -> bool:
        """Restart a specific service"""
        return self.lifecycle.restart_service(service_name)
    
    def get_status(self) -> Dict:
        """Get current system status"""
        uptime = None
        if self.start_time:
            uptime = (datetime.now() - self.start_time).total_seconds()
        
        service_statuses = {}
        for name in self.services:
            service_statuses[name] = self.lifecycle.get_service_status(name)
        
        return {
            'system_status': 'running' if self.running else 'stopped',
            'uptime_seconds': uptime,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'services': service_statuses,
            'monitoring': {
                'enabled': self.monitor.is_running(),
                'checks_performed': self.monitor.get_check_count()
            },
            'recovery': {
                'last_checkpoint': self.recovery.get_last_checkpoint_time(),
                'checkpoints_saved': self.recovery.get_checkpoint_count()
            }
        }
    
    def get_uptime(self) -> float:
        """Get system uptime in seconds"""
        if self.start_time:
            return (datetime.now() - self.start_time).total_seconds()
        return 0.0
    
    def _run_database_migration(self):
        """Run database migration before starting services"""
        self.logger.info("Running database migration")
        
        import subprocess
        try:
            # Run migration script
            result = subprocess.run(
                ['python', 'database_migration.py'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode != 0:
                raise Exception(f"Database migration failed: {result.stderr}")
            
            self.logger.info("Database migration completed successfully")
            
        except subprocess.TimeoutExpired:
            raise Exception("Database migration timed out")
        except Exception as e:
            raise Exception(f"Database migration error: {str(e)}")


================================================================================
FILE: ./hybrid_components/utils_file.py
================================================================================

"""
Name of Service: TRADING SYSTEM HYBRID ARCHITECTURE - UTILITIES
Version: 3.0.0
Last Updated: 2025-06-15
REVISION HISTORY:
v3.0.0 (2025-06-15) - Initial utilities implementation
"""

import os
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Optional

def setup_logging(component_name: str, log_level: str = 'INFO') -> logging.Logger:
    """Set up logging for a component"""
    # Ensure log directory exists
    log_dir = Path('/content/logs')
    log_dir.mkdir(exist_ok=True)
    
    # Create logger
    logger = logging.getLogger(component_name)
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # Remove existing handlers
    logger.handlers = []
    
    # File handler
    log_file = log_dir / f'{component_name.lower()}.log'
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # Formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # Add handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def print_banner():
    """Print the hybrid manager banner"""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          TRADING SYSTEM HYBRID ARCHITECTURE v3.0.0         ‚ïë
‚ïë                                                            ‚ïë
‚ïë  ‚Ä¢ 8 Microservices with REST APIs                         ‚ïë
‚ïë  ‚Ä¢ Automated Lifecycle Management                          ‚ïë
‚ïë  ‚Ä¢ Checkpoint/Recovery for Colab                          ‚ïë
‚ïë  ‚Ä¢ Health Monitoring & Auto-Restart                       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)

def print_status(status: Dict):
    """Print formatted system status"""
    print("\n" + "="*60)
    print(f"System Status: {status['system_status'].upper()}")
    print(f"Uptime: {format_duration(status.get('uptime_seconds', 0))}")
    print("="*60)
    
    # Service status table
    print("\nSERVICE STATUS:")
    print("-"*60)
    print(f"{'Service':<15} {'Status':<12} {'Port':<8} {'PID':<8} {'Uptime':<15}")
    print("-"*60)
    
    for service_name, service_info in status['services'].items():
        uptime_str = format_duration(service_info.get('uptime', 0))
        print(f"{service_name:<15} {service_info['status']:<12} "
              f"{service_info.get('port', 'N/A'):<8} "
              f"{service_info.get('pid', 'N/A'):<8} {uptime_str:<15}")
    
    print("-"*60)
    
    # Monitoring info
    if 'monitoring' in status:
        print(f"\nMonitoring: {'Enabled' if status['monitoring']['enabled'] else 'Disabled'}")
        print(f"Health Checks: {status['monitoring']['checks_performed']}")
    
    # Recovery info
    if 'recovery' in status:
        print(f"\nCheckpoints Saved: {status['recovery']['checkpoints_saved']}")
        if status['recovery']['last_checkpoint']:
            print(f"Last Checkpoint: {format_timestamp(status['recovery']['last_checkpoint'])}")
    
    print("="*60)

def format_duration(seconds: float) -> str:
    """Format duration in seconds to human readable string"""
    if seconds < 60:
        return f"{int(seconds)}s"
    elif seconds < 3600:
        minutes = int(seconds / 60)
        secs = int(seconds % 60)
        return f"{minutes}m {secs}s"
    else:
        hours = int(seconds / 3600)
        minutes = int((seconds % 3600) / 60)
        return f"{hours}h {minutes}m"

def format_timestamp(timestamp_str: str) -> str:
    """Format ISO timestamp to human readable string"""
    try:
        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        now = datetime.now(dt.tzinfo)
        diff = now - dt
        
        if diff < timedelta(minutes=1):
            return "just now"
        elif diff < timedelta(hours=1):
            minutes = int(diff.total_seconds() / 60)
            return f"{minutes} minute{'s' if minutes != 1 else ''} ago"
        elif diff < timedelta(days=1):
            hours = int(diff.total_seconds() / 3600)
            return f"{hours} hour{'s' if hours != 1 else ''} ago"
        else:
            return dt.strftime("%Y-%m-%d %H:%M:%S")
    except:
        return timestamp_str

def ensure_google_drive_mounted() -> bool:
    """Ensure Google Drive is mounted in Colab"""
    drive_path = Path('/content/drive')
    
    if drive_path.exists() and any(drive_path.iterdir()):
        return True
    
    try:
        # Try to mount Google Drive
        from google.colab import drive
        drive.mount('/content/drive')
        return True
    except ImportError:
        # Not running in Colab
        return False
    except Exception as e:
        print(f"Error mounting Google Drive: {str(e)}")
        return False

def get_service_logs(service_name: str, lines: int = 50) -> Optional[str]:
    """Get recent logs from a service"""
    log_file = Path(f'/content/logs/{service_name}_service.log')
    
    if not log_file.exists():
        return None
    
    try:
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
            return ''.join(all_lines[-lines:])
    except Exception:
        return None

def check_system_requirements() -> Dict[str, bool]:
    """Check if system requirements are met"""
    requirements = {}
    
    # Check Python version
    import sys
    requirements['python_3.10+'] = sys.version_info >= (3, 10)
    
    # Check required directories
    requirements['logs_directory'] = Path('/content/logs').exists()
    requirements['services_directory'] = Path('/content/trading_system').exists()
    
    # Check Google Drive (optional but recommended)
    requirements['google_drive'] = Path('/content/drive').exists()
    
    # Check for database
    requirements['database'] = Path('/content/trading_system.db').exists()
    
    # Check for service files
    service_files = [
        'coordination_service.py',
        'security_scanner.py',
        'pattern_analysis.py',
        'technical_analysis.py',
        'paper_trading.py',
        'pattern_recognition_service.py',
        'news_service.py',
        'reporting_service.py'
    ]
    
    all_services_exist = all(
        Path(f'/conten./{f}').exists() 
        for f in service_files
    )
    requirements['all_services'] = all_services_exist
    
    return requirements

def create_directory_structure():
    """Create required directory structure"""
    directories = [
        '/content/logs',
        '/content/trading_system',
        '/content/drive/MyDrive/TradingBot/checkpoints',
        '/content/drive/MyDrive/TradingBot/backups',
        '/content/drive/MyDrive/TradingBot/logs'
    ]
    
    for directory in directories:
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"Warning: Could not create {directory}: {str(e)}")

def get_system_info() -> Dict:
    """Get system information"""
    info = {}
    
    # Memory info
    try:
        import psutil
        memory = psutil.virtual_memory()
        info['memory'] = {
            'total_gb': round(memory.total / (1024**3), 2),
            'available_gb': round(memory.available / (1024**3), 2),
            'percent_used': memory.percent
        }
        
        # CPU info
        info['cpu'] = {
            'count': psutil.cpu_count(),
            'percent': psutil.cpu_percent(interval=1)
        }
        
        # Disk info
        disk = psutil.disk_usage('/')
        info['disk'] = {
            'total_gb': round(disk.total / (1024**3), 2),
            'free_gb': round(disk.free / (1024**3), 2),
            'percent_used': disk.percent
        }
    except ImportError:
        info['error'] = 'psutil not available'
    
    return info

def cleanup_old_logs(days: int = 7):
    """Clean up log files older than specified days"""
    log_dir = Path('/content/logs')
    if not log_dir.exists():
        return
    
    cutoff_time = datetime.now() - timedelta(days=days)
    
    for log_file in log_dir.glob('*.log'):
        try:
            mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
            if mtime < cutoff_time:
                log_file.unlink()
                print(f"Removed old log: {log_file.name}")
        except Exception:
            pass

# Service health check utilities
async def check_service_health_async(service_name: str, port: int, 
                                   endpoint: str = '/health') -> bool:
    """Async health check for a service"""
    import aiohttp
    
    url = f"http://localhost:{port}{endpoint}"
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                return response.status == 200
    except:
        return False

def format_service_table(services: Dict) -> str:
    """Format service information as a table"""
    lines = []
    lines.append("‚îå" + "‚îÄ"*15 + "‚î¨" + "‚îÄ"*12 + "‚î¨" + "‚îÄ"*8 + "‚î¨" + "‚îÄ"*15 + "‚îê")
    lines.append(f"‚îÇ{'Service':<15}‚îÇ{'Status':<12}‚îÇ{'Port':<8}‚îÇ{'Health':<15}‚îÇ")
    lines.append("‚îú" + "‚îÄ"*15 + "‚îº" + "‚îÄ"*12 + "‚îº" + "‚îÄ"*8 + "‚îº" + "‚îÄ"*15 + "‚î§")
    
    for name, info in services.items():
        health = "‚úì Healthy" if info.get('is_healthy') else "‚úó Unhealthy"
        lines.append(f"‚îÇ{name:<15}‚îÇ{info['status']:<12}‚îÇ{info['port']:<8}‚îÇ{health:<15}‚îÇ")
    
    lines.append("‚îî" + "‚îÄ"*15 + "‚î¥" + "‚îÄ"*12 + "‚î¥" + "‚îÄ"*8 + "‚î¥" + "‚îÄ"*15 + "‚îò")
    
    return "\n".join(lines)


================================================================================
FILE: ./updates/TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
================================================================================

#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
Version: 2.0.6
Last Updated: 2025-06-21

REVISION HISTORY:
- v2.0.6 (2025-06-21) - Fixed syntax error in main() function argument handling
  - Reorganized if-elif structure to prevent unreachable code
  - Moved sys.exit() calls to end of main function
  - Fixed elif statement at line 1047
- v2.0.5 (2025-06-20) - Fixed directory creation issues
- v2.0.4 (2025-06-20) - Fixed file update method def _apply_updates
- v2.0.3 (2025-06-20) - Added proper implementation plan parsing
- v2.0.2 (2025-06-20) - Fixed JSON parsing error in _get_update_count method
- v2.0.1 (2025-06-19) - Fixed Jupyter/Colab compatibility issue
- v2.0.0 (2025-06-19) - Full compliance with Project Methodology v3.0.2

PURPOSE:
Automated update process for Trading System with full Google Drive integration.
Executes implementation plans following the 6-phase methodology.
"""

import os
import sys
import json
import shutil
import logging
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

# Detect if running in Jupyter/Colab
RUNNING_IN_JUPYTER = 'ipykernel' in sys.modules or 'google.colab' in sys.modules

# Import the Google Drive service
try:
    from google_drive_service_v101 import get_drive_service
    GOOGLE_DRIVE_AVAILABLE = True
except ImportError:
    GOOGLE_DRIVE_AVAILABLE = False
    print("Warning: Google Drive service not available")

@dataclass
class ImplementationPlan:
    """Represents an approved implementation plan"""
    implementation_id: str
    plan_name: str
    date_created: str
    phases: List[Dict]
    files_to_update: List[Dict]
    risk_level: str = "MEDIUM"
    rollback_strategy: Dict = field(default_factory=dict)
    raw_content: str = ""
    
    @classmethod
    def from_drive_content(cls, plan_name: str, content: str) -> 'ImplementationPlan':
        """Load implementation plan from Google Drive content"""
        # Extract Implementation ID from filename
        # Format: "Implementation Plan - [ID] - [DATE].md"
        parts = plan_name.replace("Implementation Plan - ", "").replace(".md", "").split(" - ")
        implementation_id = parts[0] if parts else "UNKNOWN"
        
        # Parse content to extract files to update
        files_to_update = []
        lines = content.split('\n')
        in_files_section = False
        
        for line in lines:
            if 'Files to Update' in line or 'Files to Deliver' in line:
                in_files_section = True
                continue
            if in_files_section and line.strip() and not line.startswith('#'):
                # Parse file update lines
                if '_v' in line and '.py' in line:
                    # Extract version and filename
                    parts = line.strip().split()
                    if len(parts) >= 1:
                        filename = parts[0].strip('- ')
                        files_to_update.append({
                            'filename': filename,
                            'source': filename,
                            'target': filename.split('_v')[0] + '.py' if '_v' in filename else filename
                        })
            elif in_files_section and line.startswith('#'):
                # End of files section
                in_files_section = False
        
        return cls(
            implementation_id=implementation_id,
            plan_name=plan_name,
            date_created=datetime.now().strftime("%Y-%m-%d"),
            phases=[
                {"name": "Discovery", "description": "Scan for updates"},
                {"name": "Documentation", "description": "Create change records"},
                {"name": "Preparation", "description": "Backup system"},
                {"name": "Implementation", "description": "Apply updates"},
                {"name": "Testing", "description": "Verify functionality"},
                {"name": "Completion", "description": "Finalize update"}
            ],
            files_to_update=files_to_update,
            raw_content=content
        )

class ChangeDiary:
    """Manages the Change Diary documentation"""
    def __init__(self, implementation_id: str, plan_name: str, drive_service):
        self.implementation_id = implementation_id
        self.plan_name = plan_name
        self.drive_service = drive_service
        self.diary_filename = f"Change Diary - {implementation_id} - {datetime.now().strftime('%Y-%m-%d')}.md"
        self.start_time = datetime.now()
        self.phases = {
            "Phase 1: Discovery": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 2: Documentation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 3: Preparation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 4: Implementation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 5: Testing": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 6: Completion": {"status": "PENDING", "start": None, "end": None, "results": []}
        }
        self.current_phase = None
        self._create_diary()
    
    def _create_diary(self):
        """Create initial diary file"""
        header = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Add phase checkboxes
        for phase_id, phase_info in self.phases.items():
            header += f"- [ ] {phase_id}\n"
            
        header += "\n## Current Status\n**Active Phase**: Starting\n"
        header += f"**Status**: INITIALIZING\n"
        header += f"**Last Updated**: {datetime.now().isoformat()}\n\n"
        header += "## What's Next Task List\nInitializing...\n\n"
        header += "## Detailed Progress\n\n"
        
        self._write_diary(header)
        
    def _write_diary(self, content: str):
        """Write content to diary file"""
        if self.drive_service and GOOGLE_DRIVE_AVAILABLE:
            self.drive_service.write_file(
                self.diary_filename,
                content.encode('utf-8'),
                'project_documentation',
                mime_type='text/markdown'
            )
        else:
            # Fallback to local file
            diary_path = Path(f'/content/trading_system/{self.diary_filename}')
            with open(diary_path, 'w') as f:
                f.write(content)
                
    def start_phase(self, phase_name: str):
        """Mark a phase as started"""
        self.current_phase = phase_name
        self.phases[phase_name]["status"] = "IN_PROGRESS"
        self.phases[phase_name]["start"] = datetime.now()
        self._update_diary()
        
    def complete_phase(self, phase_name: str, results: List[str]):
        """Mark a phase as completed"""
        self.phases[phase_name]["status"] = "COMPLETED"
        self.phases[phase_name]["end"] = datetime.now()
        self.phases[phase_name]["results"] = results
        self._update_diary()
        
    def fail_phase(self, phase_name: str, error: str):
        """Mark a phase as failed"""
        self.phases[phase_name]["status"] = "FAILED"
        self.phases[phase_name]["end"] = datetime.now()
        self.phases[phase_name]["results"] = [f"ERROR: {error}"]
        self._update_diary()
        
    def _update_diary(self):
        """Update the diary with current state"""
        content = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Update phase checkboxes
        for phase_id, phase_info in self.phases.items():
            check = "x" if phase_info["status"] == "COMPLETED" else " "
            status_icon = "‚úì" if phase_info["status"] == "COMPLETED" else "‚úó" if phase_info["status"] == "FAILED" else "‚è≥" if phase_info["status"] == "IN_PROGRESS" else ""
            content += f"- [{check}] {phase_id} {status_icon}\n"
            
        content += f"\n## Current Status\n**Active Phase**: {self.current_phase or 'None'}\n"
        
        # Find overall status
        if any(p["status"] == "FAILED" for p in self.phases.values()):
            overall_status = "FAILED - Intervention Required"
        elif all(p["status"] == "COMPLETED" for p in self.phases.values()):
            overall_status = "COMPLETED - Success"
        elif any(p["status"] == "IN_PROGRESS" for p in self.phases.values()):
            overall_status = "IN PROGRESS"
        else:
            overall_status = "PENDING"
            
        content += f"**Status**: {overall_status}\n"
        content += f"**Last Updated**: {datetime.now().isoformat()}\n\n"
        
        # Generate What's Next Task List
        content += "## What's Next Task List\n"
        content += self._generate_whats_next()
        
        content += "\n## Detailed Progress\n\n"
        
        # Add detailed phase information
        for phase_id, phase_info in self.phases.items():
            content += f"### {phase_id}\n"
            content += f"- Status: {phase_info['status']}\n"
            if phase_info['start']:
                content += f"- Started: {phase_info['start'].isoformat()}\n"
            if phase_info['end']:
                content += f"- Ended: {phase_info['end'].isoformat()}\n"
                duration = (phase_info['end'] - phase_info['start']).total_seconds()
                content += f"- Duration: {duration:.1f} seconds\n"
            if phase_info['results']:
                content += "- Results:\n"
                for result in phase_info['results']:
                    content += f"  - {result}\n"
            content += "\n"
            
        self._write_diary(content)
        
    def _generate_whats_next(self) -> str:
        """Generate the What's Next Task List based on current state"""
        task_list = []
        python_cmd = "!python" if RUNNING_IN_JUPYTER else "python"
        
        # Find current phase status
        current_phase_info = self.phases.get(self.current_phase, {})
        phase_status = current_phase_info.get("status", "PENDING")
        
        if phase_status == "IN_PROGRESS":
            task_list.append(f"**Current Phase**: {self.current_phase} - IN PROGRESS ‚è≥\n")
            task_list.append("Please wait for phase to complete...")
            
        elif phase_status == "COMPLETED":
            # Find next phase
            phase_list = list(self.phases.keys())
            current_index = phase_list.index(self.current_phase)
            
            if current_index < len(phase_list) - 1:
                next_phase = phase_list[current_index + 1]
                task_list.append(f"**Phase**: {self.current_phase} - COMPLETED ‚úì")
                task_list.append(f"**Next**: {next_phase}\n")
                task_list.append("Actions Required:")
                
                # Phase-specific instructions
                if "Discovery" in self.current_phase:
                    task_list.extend([
                        f"1. Review Discovery results in Change Diary",
                        f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                        f"3. Verify identified updates match implementation plan"
                    ])
                elif "Documentation" in self.current_phase:
                    task_list.extend([
                        f"1. Review documentation created",
                        f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                        f"3. Prepare for system backup"
                    ])
                elif "Preparation" in self.current_phase:
                    task_list.extend([
                        f"1. Verify backup completed successfully",
                        f"2. Check backup location: /content/backups/",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
                elif "Implementation" in self.current_phase:
                    task_list.extend([
                        f"1. Review applied updates",
                        f"2. Check for any failed updates",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
                elif "Testing" in self.current_phase:
                    task_list.extend([
                        f"1. Review test results",
                        f"2. Verify all services are healthy",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
            else:
                # All phases completed
                task_list.extend([
                    "‚úÖ **UPDATE PROCESS COMPLETED SUCCESSFULLY**\n",
                    "Post-Implementation Tasks:",
                    "1. Review complete Change Diary",
                    "2. Verify all services operational",
                    "3. Archive implementation plan",
                    "4. Monitor system for 24 hours"
                ])
                
        elif phase_status == "FAILED":
            task_list.append(f"**Phase**: {self.current_phase} - FAILED ‚úó\n")
            task_list.append("**Intervention Required**\n")
            task_list.append("Recovery Options:")
            task_list.extend([
                f"1. Review error details above",
                f"2. Run diagnostics: {python_cmd} diagnostic_toolkit.py --report",
                f"3. Fix identified issues",
                f"4. Retry phase: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --retry",
                f"5. OR Rollback: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --rollback"
            ])
            
        else:
            # No phase started yet
            task_list.append("Ready to begin implementation process")
            task_list.append(f"Start with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --plan \"{self.plan_name}\"")
            
        return "\n".join(task_list) + "\n"

class TradingSystemUpdater:
    """Main updater class that orchestrates the update process"""
    
    def __init__(self, implementation_plan: ImplementationPlan):
        self.plan = implementation_plan
        self.implementation_id = implementation_plan.implementation_id
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize Google Drive service
        if GOOGLE_DRIVE_AVAILABLE:
            self.drive_service = get_drive_service()
        else:
            self.drive_service = None
            self.logger.warning("Google Drive service not available - using local files only")
        
        # Initialize Change Diary
        self.diary = ChangeDiary(self.implementation_id, self.plan.plan_name, self.drive_service)
        
        # Paths
        self.base_path = Path('/content/trading_system')
        self.backup_path = Path('/content/backups')
        self.update_path = Path('/content/updates')
        self.temp_path = Path('/content/temp_updates')
        
        # Create required directories
        self._create_required_directories()
        
        # State management
        self.state_file = self.base_path / '.update_state.json'
        self.state = self._load_state()
        
    def _create_required_directories(self):
        """Ensure all required directories exist"""
        directories = [
            self.base_path,
            self.backup_path,
            self.update_path,
            self.temp_path,
            self.base_path / 'logs',
            self.base_path / 'project_documentation'
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Ensured directory exists: {directory}")
            
    def _setup_logging(self):
        """Setup logging configuration"""
        log_dir = Path('/content/logs')
        log_dir.mkdir(exist_ok=True)
        
        logger = logging.getLogger('TradingSystemUpdater')
        logger.setLevel(logging.INFO)
        
        # File handler
        fh = logging.FileHandler(log_dir / 'update_process.log')
        fh.setLevel(logging.INFO)
        
        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        logger.addHandler(fh)
        logger.addHandler(ch)
        
        return logger
        
    def _load_state(self) -> Dict:
        """Load saved state from file"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return {
            'implementation_id': self.implementation_id,
            'plan_name': self.plan.plan_name,
            'current_phase': None,
            'completed_phases': [],
            'status': 'PENDING',
            'last_update': None
        }
        
    def _save_state(self):
        """Save current state to file"""
        self.state['last_update'] = datetime.now().isoformat()
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
            
        # Also save to Google Drive if available
        if self.drive_service and GOOGLE_DRIVE_AVAILABLE:
            self.drive_service.save_json(self.state, '.update_state.json', 'coordination')
            
    def execute_update(self, continue_from: Optional[str] = None) -> bool:
        """Execute the update process"""
        try:
            phases = [
                ("Phase 1: Discovery", self._discovery_phase),
                ("Phase 2: Documentation", self._documentation_phase),
                ("Phase 3: Preparation", self._preparation_phase),
                ("Phase 4: Implementation", self._implementation_phase),
                ("Phase 5: Testing", self._testing_phase),
                ("Phase 6: Completion", self._completion_phase)
            ]
            
            # Determine starting point
            start_index = 0
            if continue_from:
                for i, (phase_name, _) in enumerate(phases):
                    if phase_name == continue_from:
                        start_index = i
                        break
                        
            # Execute phases
            for phase_name, phase_func in phases[start_index:]:
                self.logger.info(f"Starting {phase_name}")
                self.diary.start_phase(phase_name)
                
                self.state['current_phase'] = phase_name
                self.state['status'] = 'IN_PROGRESS'
                self._save_state()
                
                try:
                    results = phase_func()
                    if results.get('success', False):
                        self.diary.complete_phase(phase_name, results.get('results', []))
                        self.state['completed_phases'].append(phase_name)
                        self.logger.info(f"Completed {phase_name}")
                    else:
                        error_msg = results.get('error', 'Unknown error')
                        self.diary.fail_phase(phase_name, error_msg)
                        self.state['status'] = 'FAILED'
                        self._save_state()
                        self.logger.error(f"Failed {phase_name}: {error_msg}")
                        return False
                        
                except Exception as e:
                    self.diary.fail_phase(phase_name, str(e))
                    self.state['status'] = 'FAILED'
                    self._save_state()
                    self.logger.exception(f"Exception in {phase_name}")
                    return False
                    
                self._save_state()
                
            # All phases completed
            self.state['status'] = 'COMPLETED'
            self._save_state()
            return True
            
        except Exception as e:
            self.logger.exception("Critical error in update process")
            return False
            
    def _discovery_phase(self) -> Dict:
        """Phase 1: Discover available updates"""
        results = []
        
        try:
            # Check for updates based on implementation plan
            updates_found = len(self.plan.files_to_update)
            results.append(f"Found {updates_found} files to update from implementation plan")
            
            # List files
            for file_info in self.plan.files_to_update:
                results.append(f"- {file_info['filename']} -> {file_info['target']}")
                
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _documentation_phase(self) -> Dict:
        """Phase 2: Create documentation"""
        results = []
        
        try:
            # Documentation is handled by Change Diary
            results.append("Change Diary created and updated")
            results.append(f"Diary name: {self.diary.diary_filename}")
            results.append("Implementation plan referenced")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _preparation_phase(self) -> Dict:
        """Phase 3: Prepare for update (backup)"""
        results = []
        
        try:
            # Create backup
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = self.backup_path / f"backup_{timestamp}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # Backup current services
            services_backed_up = 0
            for service_file in self.base_path.glob("*.py"):
                if service_file.name.endswith("_service.py"):
                    shutil.copy2(service_file, backup_dir)
                    services_backed_up += 1
                    
            results.append(f"Created backup at: {backup_dir}")
            results.append(f"Backed up {services_backed_up} service files")
            
            # Stop running services (if any)
            # In production, this would actually stop services
            results.append("Services prepared for update")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _implementation_phase(self) -> Dict:
        """Phase 4: Apply updates"""
        results = []
        applied = []
        failed = []
        
        try:
            for update in self.plan.files_to_update:
                try:
                    source = update['source']
                    target = update['target']
                    
                    # Try multiple locations for source file
                    source_locations = [
                        self.update_path / source,
                        self.temp_path / source,
                        self.base_path / 'project_documentation' / source,
                        Path(f'/content/drive/MyDrive/TradingSystem/project_documentation/{source}')
                    ]
                    
                    source_found = None
                    for loc in source_locations:
                        if loc.exists():
                            source_found = loc
                            break
                            
                    if source_found:
                        target_path = self.base_path / target
                        shutil.copy2(source_found, target_path)
                        applied.append(f"{source} -> {target}")
                        self.logger.info(f"Applied update: {source} -> {target}")
                    else:
                        # If source not found, create placeholder for simulation
                        target_path = self.base_path / target
                        if not target_path.exists():
                            # Create minimal placeholder
                            with open(target_path, 'w') as f:
                                f.write(f"# {target} - Placeholder created by update process\n")
                                f.write(f"# Source file {source} not found during update\n")
                                f.write(f"# Created: {datetime.now().isoformat()}\n")
                            applied.append(f"{source} -> {target} (placeholder)")
                            self.logger.warning(f"Created placeholder for missing source: {source}")
                        else:
                            failed.append(f"{source} (source not found)")
                            
                except Exception as e:
                    failed.append(f"{update['filename']}: {str(e)}")
                    
            results.append(f"Applied {len(applied)} updates successfully")
            if applied:
                results.extend([f"  {a}" for a in applied])
            if failed:
                results.append(f"Failed to apply {len(failed)} updates:")
                results.extend([f"  {f}" for f in failed])
                
            return {'success': len(failed) == 0, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _testing_phase(self) -> Dict:
        """Phase 5: Test the system"""
        results = []
        
        try:
            # Basic health checks
            results.append("Running system health checks...")
            
            # Check if key files exist
            key_files = ['coordination_service.py', 'web_dashboard_service.py']
            for file_name in key_files:
                if (self.base_path / file_name).exists():
                    results.append(f"‚úì {file_name} exists")
                else:
                    results.append(f"‚úó {file_name} missing")
                    
            # In production, would run actual service health checks
            results.append("Health checks completed")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _completion_phase(self) -> Dict:
        """Phase 6: Complete the update"""
        results = []
        
        try:
            # Finalize update
            results.append("Update process completed successfully")
            results.append(f"Implementation ID: {self.implementation_id}")
            results.append(f"Total time: {(datetime.now() - self.diary.start_time).total_seconds():.1f} seconds")
            
            # Clean up state file
            if self.state_file.exists():
                self.state_file.unlink()
                results.append("Cleaned up state file")
                
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def get_status(self) -> Dict:
        """Get current implementation status"""
        return {
            'Implementation ID': self.state.get('implementation_id', 'None'),
            'Plan': self.state.get('plan_name', 'None'),
            'Current Phase': self.state.get('current_phase', 'None'),
            'Status': self.state.get('status', 'None'),
            'Completed Phases': ', '.join(self.state.get('completed_phases', [])),
            'Last Update': self.state.get('last_update', 'Never')
        }
        
    def rollback(self) -> bool:
        """Rollback the current implementation"""
        try:
            self.logger.info("Starting rollback process")
            
            # Find latest backup
            backups = sorted(self.backup_path.glob("backup_*"))
            if not backups:
                self.logger.error("No backups found to rollback to")
                return False
                
            latest_backup = backups[-1]
            self.logger.info(f"Rolling back from: {latest_backup}")
            
            # Restore files
            restored = 0
            for backup_file in latest_backup.glob("*.py"):
                target = self.base_path / backup_file.name
                shutil.copy2(backup_file, target)
                restored += 1
                
            self.logger.info(f"Restored {restored} files")
            
            # Update state
            self.state['status'] = 'ROLLED_BACK'
            self.state['current_phase'] = None
            self._save_state()
            
            # Update diary
            self.diary.current_phase = "Rollback"
            self.diary._update_diary()
            
            return True
            
        except Exception as e:
            self.logger.exception("Rollback failed")
            return False

def main():
    """Main entry point"""
    # Setup argument parser
    parser = argparse.ArgumentParser(
        description='Trading System Automated Update Process v2.0.6',
        epilog=f"{'Use ! prefix for commands' if RUNNING_IN_JUPYTER else ''}"
    )
    
    parser.add_argument('--plan', type=str, help='Implementation plan to execute')
    parser.add_argument('--continue', action='store_true', dest='continue_update', 
                       help='Continue from last checkpoint')
    parser.add_argument('--status', action='store_true', help='Check implementation status')
    parser.add_argument('--rollback', action='store_true', help='Rollback current implementation')
    parser.add_argument('--check-only', action='store_true', help='Check for updates without applying')
    parser.add_argument('--retry', action='store_true', help='Retry failed phase')
    
    # Use parse_known_args to handle Jupyter kernel arguments
    args, unknown = parser.parse_known_args()
    
    # Initialize return code
    return_code = 0
    
    try:
        # Handle different command options
        if args.status:
            # Check status
            state_file = Path('/content/trading_system/.update_state.json')
            if state_file.exists():
                with open(state_file, 'r') as f:
                    state = json.load(f)
                    
                print("\n=== IMPLEMENTATION STATUS ===")
                print(f"Implementation ID: {state.get('implementation_id', 'None')}")
                print(f"Plan: {state.get('plan_name', 'None')}")
                print(f"Current Phase: {state.get('current_phase', 'None')}")
                print(f"Status: {state.get('status', 'None')}")
                print(f"Completed Phases: {', '.join(state.get('completed_phases', []))}")
                print(f"Last Update: {state.get('last_update', 'Never')}")
            else:
                print("No implementation in progress")
                
        elif args.plan:
            # Execute implementation plan
            if GOOGLE_DRIVE_AVAILABLE:
                drive_service = get_drive_service()
                
                # Try to read plan from Google Drive
                plan_content = drive_service.read_file(args.plan, 'project_documentation')
                if not plan_content:
                    print(f"‚ùå Implementation plan not found in Google Drive: {args.plan}")
                    return_code = 1
                else:
                    plan = ImplementationPlan.from_drive_content(
                        args.plan,
                        plan_content.decode('utf-8')
                    )
                    
                    print(f"‚úÖ Loaded plan: {plan.implementation_id}")
                    
                    # Execute update
                    updater = TradingSystemUpdater(plan)
                    success = updater.execute_update()
                    return_code = 0 if success else 1
            else:
                print("‚ùå Google Drive service not available")
                return_code = 1
                
        elif args.continue_update or args.retry:
            # Continue or retry from saved state
            if GOOGLE_DRIVE_AVAILABLE:
                drive_service = get_drive_service()
                state = drive_service.load_json('.update_state.json', 'coordination')
                
                if not state:
                    # Try local file
                    state_file = Path('/content/trading_system/.update_state.json')
                    if state_file.exists():
                        with open(state_file, 'r') as f:
                            state = json.load(f)
                            
                if not state:
                    print("‚ùå No implementation in progress to continue")
                    return_code = 1
                else:
                    # Reload implementation plan
                    plan_content = drive_service.read_file(state['plan_name'], 'project_documentation')
                    if not plan_content:
                        print(f"‚ùå Original plan not found: {state['plan_name']}")
                        return_code = 1
                    else:
                        plan = ImplementationPlan.from_drive_content(
                            state['plan_name'],
                            plan_content.decode('utf-8')
                        )
                        
                        print(f"‚úÖ Resuming implementation: {plan.implementation_id}")
                        
                        # Continue from saved phase
                        updater = TradingSystemUpdater(plan)
                        
                        if args.retry and state['status'] == 'FAILED':
                            # Retry the failed phase
                            success = updater.execute_update(continue_from=state['current_phase'])
                        else:
                            # Continue from next phase
                            phases = ["Phase 1: Discovery", "Phase 2: Documentation", 
                                    "Phase 3: Preparation", "Phase 4: Implementation",
                                    "Phase 5: Testing", "Phase 6: Completion"]
                            current_index = phases.index(state['current_phase'])
                            if current_index < len(phases) - 1:
                                success = updater.execute_update(continue_from=phases[current_index + 1])
                            else:
                                print("‚úÖ Implementation already completed")
                                success = True
                                
                        return_code = 0 if success else 1
            else:
                print("‚ùå Google Drive service not available")
                return_code = 1
                
        elif args.rollback:
            # Rollback implementation
            state_file = Path('/content/trading_system/.update_state.json')
            if not state_file.exists():
                print("‚ùå No implementation in progress to rollback")
                return_code = 1
            else:
                # Create minimal plan for rollback
                plan = ImplementationPlan(
                    implementation_id="ROLLBACK",
                    plan_name="Rollback Operation",
                    date_created=datetime.now().strftime("%Y-%m-%d"),
                    phases=[],
                    files_to_update=[]
                )
                updater = TradingSystemUpdater(plan)
                success = updater.rollback()
                return_code = 0 if success else 1
                
        elif args.check_only:
            # Check for updates
            print("Checking for available updates...")
            # In production, this would scan for updates
            print("Check-only mode: Would scan project documentation for new implementation plans")
            print("No actual changes made")
            
        else:
            # No arguments provided
            parser.print_help()
            
    except Exception as e:
        print(f"‚ùå Critical error: {str(e)}")
        logging.exception("Critical error in main")
        return_code = 1
        
    sys.exit(return_code)

if __name__ == "__main__":
    main()#!/usr/bin/env python3
"""
TRADING SYSTEM PHASE 1
Service: TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py
Version: 2.0.6
Last Updated: 2025-06-21

REVISION HISTORY:
- v2.0.6 (2025-06-21) - Fixed syntax error in main() function argument handling
  - Reorganized if-elif structure to prevent unreachable code
  - Moved sys.exit() calls to end of main function
  - Fixed elif statement at line 1047
- v2.0.5 (2025-06-20) - Fixed directory creation issues
- v2.0.4 (2025-06-20) - Fixed file update method def _apply_updates
- v2.0.3 (2025-06-20) - Added proper implementation plan parsing
- v2.0.2 (2025-06-20) - Fixed JSON parsing error in _get_update_count method
- v2.0.1 (2025-06-19) - Fixed Jupyter/Colab compatibility issue
- v2.0.0 (2025-06-19) - Full compliance with Project Methodology v3.0.2

PURPOSE:
Automated update process for Trading System with full Google Drive integration.
Executes implementation plans following the 6-phase methodology.
"""

import os
import sys
import json
import shutil
import logging
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

# Detect if running in Jupyter/Colab
RUNNING_IN_JUPYTER = 'ipykernel' in sys.modules or 'google.colab' in sys.modules

# Import the Google Drive service
try:
    from google_drive_service_v101 import get_drive_service
    GOOGLE_DRIVE_AVAILABLE = True
except ImportError:
    GOOGLE_DRIVE_AVAILABLE = False
    print("Warning: Google Drive service not available")

@dataclass
class ImplementationPlan:
    """Represents an approved implementation plan"""
    implementation_id: str
    plan_name: str
    date_created: str
    phases: List[Dict]
    files_to_update: List[Dict]
    risk_level: str = "MEDIUM"
    rollback_strategy: Dict = field(default_factory=dict)
    raw_content: str = ""
    
    @classmethod
    def from_drive_content(cls, plan_name: str, content: str) -> 'ImplementationPlan':
        """Load implementation plan from Google Drive content"""
        # Extract Implementation ID from filename
        # Format: "Implementation Plan - [ID] - [DATE].md"
        parts = plan_name.replace("Implementation Plan - ", "").replace(".md", "").split(" - ")
        implementation_id = parts[0] if parts else "UNKNOWN"
        
        # Parse content to extract files to update
        files_to_update = []
        lines = content.split('\n')
        in_files_section = False
        
        for line in lines:
            if 'Files to Update' in line or 'Files to Deliver' in line:
                in_files_section = True
                continue
            if in_files_section and line.strip() and not line.startswith('#'):
                # Parse file update lines
                if '_v' in line and '.py' in line:
                    # Extract version and filename
                    parts = line.strip().split()
                    if len(parts) >= 1:
                        filename = parts[0].strip('- ')
                        files_to_update.append({
                            'filename': filename,
                            'source': filename,
                            'target': filename.split('_v')[0] + '.py' if '_v' in filename else filename
                        })
            elif in_files_section and line.startswith('#'):
                # End of files section
                in_files_section = False
        
        return cls(
            implementation_id=implementation_id,
            plan_name=plan_name,
            date_created=datetime.now().strftime("%Y-%m-%d"),
            phases=[
                {"name": "Discovery", "description": "Scan for updates"},
                {"name": "Documentation", "description": "Create change records"},
                {"name": "Preparation", "description": "Backup system"},
                {"name": "Implementation", "description": "Apply updates"},
                {"name": "Testing", "description": "Verify functionality"},
                {"name": "Completion", "description": "Finalize update"}
            ],
            files_to_update=files_to_update,
            raw_content=content
        )

class ChangeDiary:
    """Manages the Change Diary documentation"""
    def __init__(self, implementation_id: str, plan_name: str, drive_service):
        self.implementation_id = implementation_id
        self.plan_name = plan_name
        self.drive_service = drive_service
        self.diary_filename = f"Change Diary - {implementation_id} - {datetime.now().strftime('%Y-%m-%d')}.md"
        self.start_time = datetime.now()
        self.phases = {
            "Phase 1: Discovery": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 2: Documentation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 3: Preparation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 4: Implementation": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 5: Testing": {"status": "PENDING", "start": None, "end": None, "results": []},
            "Phase 6: Completion": {"status": "PENDING", "start": None, "end": None, "results": []}
        }
        self.current_phase = None
        self._create_diary()
    
    def _create_diary(self):
        """Create initial diary file"""
        header = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Add phase checkboxes
        for phase_id, phase_info in self.phases.items():
            header += f"- [ ] {phase_id}\n"
            
        header += "\n## Current Status\n**Active Phase**: Starting\n"
        header += f"**Status**: INITIALIZING\n"
        header += f"**Last Updated**: {datetime.now().isoformat()}\n\n"
        header += "## What's Next Task List\nInitializing...\n\n"
        header += "## Detailed Progress\n\n"
        
        self._write_diary(header)
        
    def _write_diary(self, content: str):
        """Write content to diary file"""
        if self.drive_service and GOOGLE_DRIVE_AVAILABLE:
            self.drive_service.write_file(
                self.diary_filename,
                content.encode('utf-8'),
                'project_documentation',
                mime_type='text/markdown'
            )
        else:
            # Fallback to local file
            diary_path = Path(f'/content/trading_system/{self.diary_filename}')
            with open(diary_path, 'w') as f:
                f.write(content)
                
    def start_phase(self, phase_name: str):
        """Mark a phase as started"""
        self.current_phase = phase_name
        self.phases[phase_name]["status"] = "IN_PROGRESS"
        self.phases[phase_name]["start"] = datetime.now()
        self._update_diary()
        
    def complete_phase(self, phase_name: str, results: List[str]):
        """Mark a phase as completed"""
        self.phases[phase_name]["status"] = "COMPLETED"
        self.phases[phase_name]["end"] = datetime.now()
        self.phases[phase_name]["results"] = results
        self._update_diary()
        
    def fail_phase(self, phase_name: str, error: str):
        """Mark a phase as failed"""
        self.phases[phase_name]["status"] = "FAILED"
        self.phases[phase_name]["end"] = datetime.now()
        self.phases[phase_name]["results"] = [f"ERROR: {error}"]
        self._update_diary()
        
    def _update_diary(self):
        """Update the diary with current state"""
        content = f"""# {self.diary_filename.replace('.md', '')}

**Document**: Change Diary
**Implementation ID**: {self.implementation_id}
**Related Implementation Plan**: {self.plan_name}
**Date Created**: {self.start_time.strftime("%Y-%m-%d")}
**Author**: Trading System Development Team

## Implementation Summary
Automated update process executing approved implementation plan.

## Phase Progress
"""
        
        # Update phase checkboxes
        for phase_id, phase_info in self.phases.items():
            check = "x" if phase_info["status"] == "COMPLETED" else " "
            status_icon = "‚úì" if phase_info["status"] == "COMPLETED" else "‚úó" if phase_info["status"] == "FAILED" else "‚è≥" if phase_info["status"] == "IN_PROGRESS" else ""
            content += f"- [{check}] {phase_id} {status_icon}\n"
            
        content += f"\n## Current Status\n**Active Phase**: {self.current_phase or 'None'}\n"
        
        # Find overall status
        if any(p["status"] == "FAILED" for p in self.phases.values()):
            overall_status = "FAILED - Intervention Required"
        elif all(p["status"] == "COMPLETED" for p in self.phases.values()):
            overall_status = "COMPLETED - Success"
        elif any(p["status"] == "IN_PROGRESS" for p in self.phases.values()):
            overall_status = "IN PROGRESS"
        else:
            overall_status = "PENDING"
            
        content += f"**Status**: {overall_status}\n"
        content += f"**Last Updated**: {datetime.now().isoformat()}\n\n"
        
        # Generate What's Next Task List
        content += "## What's Next Task List\n"
        content += self._generate_whats_next()
        
        content += "\n## Detailed Progress\n\n"
        
        # Add detailed phase information
        for phase_id, phase_info in self.phases.items():
            content += f"### {phase_id}\n"
            content += f"- Status: {phase_info['status']}\n"
            if phase_info['start']:
                content += f"- Started: {phase_info['start'].isoformat()}\n"
            if phase_info['end']:
                content += f"- Ended: {phase_info['end'].isoformat()}\n"
                duration = (phase_info['end'] - phase_info['start']).total_seconds()
                content += f"- Duration: {duration:.1f} seconds\n"
            if phase_info['results']:
                content += "- Results:\n"
                for result in phase_info['results']:
                    content += f"  - {result}\n"
            content += "\n"
            
        self._write_diary(content)
        
    def _generate_whats_next(self) -> str:
        """Generate the What's Next Task List based on current state"""
        task_list = []
        python_cmd = "!python" if RUNNING_IN_JUPYTER else "python"
        
        # Find current phase status
        current_phase_info = self.phases.get(self.current_phase, {})
        phase_status = current_phase_info.get("status", "PENDING")
        
        if phase_status == "IN_PROGRESS":
            task_list.append(f"**Current Phase**: {self.current_phase} - IN PROGRESS ‚è≥\n")
            task_list.append("Please wait for phase to complete...")
            
        elif phase_status == "COMPLETED":
            # Find next phase
            phase_list = list(self.phases.keys())
            current_index = phase_list.index(self.current_phase)
            
            if current_index < len(phase_list) - 1:
                next_phase = phase_list[current_index + 1]
                task_list.append(f"**Phase**: {self.current_phase} - COMPLETED ‚úì")
                task_list.append(f"**Next**: {next_phase}\n")
                task_list.append("Actions Required:")
                
                # Phase-specific instructions
                if "Discovery" in self.current_phase:
                    task_list.extend([
                        f"1. Review Discovery results in Change Diary",
                        f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                        f"3. Verify identified updates match implementation plan"
                    ])
                elif "Documentation" in self.current_phase:
                    task_list.extend([
                        f"1. Review documentation created",
                        f"2. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue",
                        f"3. Prepare for system backup"
                    ])
                elif "Preparation" in self.current_phase:
                    task_list.extend([
                        f"1. Verify backup completed successfully",
                        f"2. Check backup location: /content/backups/",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
                elif "Implementation" in self.current_phase:
                    task_list.extend([
                        f"1. Review applied updates",
                        f"2. Check for any failed updates",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
                elif "Testing" in self.current_phase:
                    task_list.extend([
                        f"1. Review test results",
                        f"2. Verify all services are healthy",
                        f"3. Continue with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --continue"
                    ])
            else:
                # All phases completed
                task_list.extend([
                    "‚úÖ **UPDATE PROCESS COMPLETED SUCCESSFULLY**\n",
                    "Post-Implementation Tasks:",
                    "1. Review complete Change Diary",
                    "2. Verify all services operational",
                    "3. Archive implementation plan",
                    "4. Monitor system for 24 hours"
                ])
                
        elif phase_status == "FAILED":
            task_list.append(f"**Phase**: {self.current_phase} - FAILED ‚úó\n")
            task_list.append("**Intervention Required**\n")
            task_list.append("Recovery Options:")
            task_list.extend([
                f"1. Review error details above",
                f"2. Run diagnostics: {python_cmd} diagnostic_toolkit.py --report",
                f"3. Fix identified issues",
                f"4. Retry phase: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --retry",
                f"5. OR Rollback: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --rollback"
            ])
            
        else:
            # No phase started yet
            task_list.append("Ready to begin implementation process")
            task_list.append(f"Start with: {python_cmd} TRADING_SYSTEM_AUTOMATED_UPDATE_PROCESS.py --plan \"{self.plan_name}\"")
            
        return "\n".join(task_list) + "\n"

class TradingSystemUpdater:
    """Main updater class that orchestrates the update process"""
    
    def __init__(self, implementation_plan: ImplementationPlan):
        self.plan = implementation_plan
        self.implementation_id = implementation_plan.implementation_id
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize Google Drive service
        if GOOGLE_DRIVE_AVAILABLE:
            self.drive_service = get_drive_service()
        else:
            self.drive_service = None
            self.logger.warning("Google Drive service not available - using local files only")
        
        # Initialize Change Diary
        self.diary = ChangeDiary(self.implementation_id, self.plan.plan_name, self.drive_service)
        
        # Paths
        self.base_path = Path('/content/trading_system')
        self.backup_path = Path('/content/backups')
        self.update_path = Path('/content/updates')
        self.temp_path = Path('/content/temp_updates')
        
        # Create required directories
        self._create_required_directories()
        
        # State management
        self.state_file = self.base_path / '.update_state.json'
        self.state = self._load_state()
        
    def _create_required_directories(self):
        """Ensure all required directories exist"""
        directories = [
            self.base_path,
            self.backup_path,
            self.update_path,
            self.temp_path,
            self.base_path / 'logs',
            self.base_path / 'project_documentation'
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Ensured directory exists: {directory}")
            
    def _setup_logging(self):
        """Setup logging configuration"""
        log_dir = Path('/content/logs')
        log_dir.mkdir(exist_ok=True)
        
        logger = logging.getLogger('TradingSystemUpdater')
        logger.setLevel(logging.INFO)
        
        # File handler
        fh = logging.FileHandler(log_dir / 'update_process.log')
        fh.setLevel(logging.INFO)
        
        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        ch.setFormatter(formatter)
        
        logger.addHandler(fh)
        logger.addHandler(ch)
        
        return logger
        
    def _load_state(self) -> Dict:
        """Load saved state from file"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return {
            'implementation_id': self.implementation_id,
            'plan_name': self.plan.plan_name,
            'current_phase': None,
            'completed_phases': [],
            'status': 'PENDING',
            'last_update': None
        }
        
    def _save_state(self):
        """Save current state to file"""
        self.state['last_update'] = datetime.now().isoformat()
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
            
        # Also save to Google Drive if available
        if self.drive_service and GOOGLE_DRIVE_AVAILABLE:
            self.drive_service.save_json(self.state, '.update_state.json', 'coordination')
            
    def execute_update(self, continue_from: Optional[str] = None) -> bool:
        """Execute the update process"""
        try:
            phases = [
                ("Phase 1: Discovery", self._discovery_phase),
                ("Phase 2: Documentation", self._documentation_phase),
                ("Phase 3: Preparation", self._preparation_phase),
                ("Phase 4: Implementation", self._implementation_phase),
                ("Phase 5: Testing", self._testing_phase),
                ("Phase 6: Completion", self._completion_phase)
            ]
            
            # Determine starting point
            start_index = 0
            if continue_from:
                for i, (phase_name, _) in enumerate(phases):
                    if phase_name == continue_from:
                        start_index = i
                        break
                        
            # Execute phases
            for phase_name, phase_func in phases[start_index:]:
                self.logger.info(f"Starting {phase_name}")
                self.diary.start_phase(phase_name)
                
                self.state['current_phase'] = phase_name
                self.state['status'] = 'IN_PROGRESS'
                self._save_state()
                
                try:
                    results = phase_func()
                    if results.get('success', False):
                        self.diary.complete_phase(phase_name, results.get('results', []))
                        self.state['completed_phases'].append(phase_name)
                        self.logger.info(f"Completed {phase_name}")
                    else:
                        error_msg = results.get('error', 'Unknown error')
                        self.diary.fail_phase(phase_name, error_msg)
                        self.state['status'] = 'FAILED'
                        self._save_state()
                        self.logger.error(f"Failed {phase_name}: {error_msg}")
                        return False
                        
                except Exception as e:
                    self.diary.fail_phase(phase_name, str(e))
                    self.state['status'] = 'FAILED'
                    self._save_state()
                    self.logger.exception(f"Exception in {phase_name}")
                    return False
                    
                self._save_state()
                
            # All phases completed
            self.state['status'] = 'COMPLETED'
            self._save_state()
            return True
            
        except Exception as e:
            self.logger.exception("Critical error in update process")
            return False
            
    def _discovery_phase(self) -> Dict:
        """Phase 1: Discover available updates"""
        results = []
        
        try:
            # Check for updates based on implementation plan
            updates_found = len(self.plan.files_to_update)
            results.append(f"Found {updates_found} files to update from implementation plan")
            
            # List files
            for file_info in self.plan.files_to_update:
                results.append(f"- {file_info['filename']} -> {file_info['target']}")
                
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _documentation_phase(self) -> Dict:
        """Phase 2: Create documentation"""
        results = []
        
        try:
            # Documentation is handled by Change Diary
            results.append("Change Diary created and updated")
            results.append(f"Diary name: {self.diary.diary_filename}")
            results.append("Implementation plan referenced")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _preparation_phase(self) -> Dict:
        """Phase 3: Prepare for update (backup)"""
        results = []
        
        try:
            # Create backup
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = self.backup_path / f"backup_{timestamp}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # Backup current services
            services_backed_up = 0
            for service_file in self.base_path.glob("*.py"):
                if service_file.name.endswith("_service.py"):
                    shutil.copy2(service_file, backup_dir)
                    services_backed_up += 1
                    
            results.append(f"Created backup at: {backup_dir}")
            results.append(f"Backed up {services_backed_up} service files")
            
            # Stop running services (if any)
            # In production, this would actually stop services
            results.append("Services prepared for update")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _implementation_phase(self) -> Dict:
        """Phase 4: Apply updates"""
        results = []
        applied = []
        failed = []
        
        try:
            for update in self.plan.files_to_update:
                try:
                    source = update['source']
                    target = update['target']
                    
                    # Try multiple locations for source file
                    source_locations = [
                        self.update_path / source,
                        self.temp_path / source,
                        self.base_path / 'project_documentation' / source,
                        Path(f'/content/drive/MyDrive/TradingSystem/project_documentation/{source}')
                    ]
                    
                    source_found = None
                    for loc in source_locations:
                        if loc.exists():
                            source_found = loc
                            break
                            
                    if source_found:
                        target_path = self.base_path / target
                        shutil.copy2(source_found, target_path)
                        applied.append(f"{source} -> {target}")
                        self.logger.info(f"Applied update: {source} -> {target}")
                    else:
                        # If source not found, create placeholder for simulation
                        target_path = self.base_path / target
                        if not target_path.exists():
                            # Create minimal placeholder
                            with open(target_path, 'w') as f:
                                f.write(f"# {target} - Placeholder created by update process\n")
                                f.write(f"# Source file {source} not found during update\n")
                                f.write(f"# Created: {datetime.now().isoformat()}\n")
                            applied.append(f"{source} -> {target} (placeholder)")
                            self.logger.warning(f"Created placeholder for missing source: {source}")
                        else:
                            failed.append(f"{source} (source not found)")
                            
                except Exception as e:
                    failed.append(f"{update['filename']}: {str(e)}")
                    
            results.append(f"Applied {len(applied)} updates successfully")
            if applied:
                results.extend([f"  {a}" for a in applied])
            if failed:
                results.append(f"Failed to apply {len(failed)} updates:")
                results.extend([f"  {f}" for f in failed])
                
            return {'success': len(failed) == 0, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _testing_phase(self) -> Dict:
        """Phase 5: Test the system"""
        results = []
        
        try:
            # Basic health checks
            results.append("Running system health checks...")
            
            # Check if key files exist
            key_files = ['coordination_service.py', 'web_dashboard_service.py']
            for file_name in key_files:
                if (self.base_path / file_name).exists():
                    results.append(f"‚úì {file_name} exists")
                else:
                    results.append(f"‚úó {file_name} missing")
                    
            # In production, would run actual service health checks
            results.append("Health checks completed")
            
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def _completion_phase(self) -> Dict:
        """Phase 6: Complete the update"""
        results = []
        
        try:
            # Finalize update
            results.append("Update process completed successfully")
            results.append(f"Implementation ID: {self.implementation_id}")
            results.append(f"Total time: {(datetime.now() - self.diary.start_time).total_seconds():.1f} seconds")
            
            # Clean up state file
            if self.state_file.exists():
                self.state_file.unlink()
                results.append("Cleaned up state file")
                
            return {'success': True, 'results': results}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
            
    def get_status(self) -> Dict:
        """Get current implementation status"""
        return {
            'Implementation ID': self.state.get('implementation_id', 'None'),
            'Plan': self.state.get('plan_name', 'None'),
            'Current Phase': self.state.get('current_phase', 'None'),
            'Status': self.state.get('status', 'None'),
            'Completed Phases': ', '.join(self.state.get('completed_phases', [])),
            'Last Update': self.state.get('last_update', 'Never')
        }
        
    def rollback(self) -> bool:
        """Rollback the current implementation"""
        try:
            self.logger.info("Starting rollback process")
            
            # Find latest backup
            backups = sorted(self.backup_path.glob("backup_*"))
            if not backups:
                self.logger.error("No backups found to rollback to")
                return False
                
            latest_backup = backups[-1]
            self.logger.info(f"Rolling back from: {latest_backup}")
            
            # Restore files
            restored = 0
            for backup_file in latest_backup.glob("*.py"):
                target = self.base_path / backup_file.name
                shutil.copy2(backup_file, target)
                restored += 1
                
            self.logger.info(f"Restored {restored} files")
            
            # Update state
            self.state['status'] = 'ROLLED_BACK'
            self.state['current_phase'] = None
            self._save_state()
            
            # Update diary
            self.diary.current_phase = "Rollback"
            self.diary._update_diary()
            
            return True
            
        except Exception as e:
            self.logger.exception("Rollback failed")
            return False

def main():
    """Main entry point"""
    # Setup argument parser
    parser = argparse.ArgumentParser(
        description='Trading System Automated Update Process v2.0.6',
        epilog=f"{'Use ! prefix for commands' if RUNNING_IN_JUPYTER else ''}"
    )
    
    parser.add_argument('--plan', type=str, help='Implementation plan to execute')
    parser.add_argument('--continue', action='store_true', dest='continue_update', 
                       help='Continue from last checkpoint')
    parser.add_argument('--status', action='store_true', help='Check implementation status')
    parser.add_argument('--rollback', action='store_true', help='Rollback current implementation')
    parser.add_argument('--check-only', action='store_true', help='Check for updates without applying')
    parser.add_argument('--retry', action='store_true', help='Retry failed phase')
    
    # Use parse_known_args to handle Jupyter kernel arguments
    args, unknown = parser.parse_known_args()
    
    # Initialize return code
    return_code = 0
    
    try:
        # Handle different command options
        if args.status:
            # Check status
            state_file = Path('/content/trading_system/.update_state.json')
            if state_file.exists():
                with open(state_file, 'r') as f:
                    state = json.load(f)
                    
                print("\n=== IMPLEMENTATION STATUS ===")
                print(f"Implementation ID: {state.get('implementation_id', 'None')}")
                print(f"Plan: {state.get('plan_name', 'None')}")
                print(f"Current Phase: {state.get('current_phase', 'None')}")
                print(f"Status: {state.get('status', 'None')}")
                print(f"Completed Phases: {', '.join(state.get('completed_phases', []))}")
                print(f"Last Update: {state.get('last_update', 'Never')}")
            else:
                print("No implementation in progress")
                
        elif args.plan:
            # Execute implementation plan
            if GOOGLE_DRIVE_AVAILABLE:
                drive_service = get_drive_service()
                
                # Try to read plan from Google Drive
                plan_content = drive_service.read_file(args.plan, 'project_documentation')
                if not plan_content:
                    print(f"‚ùå Implementation plan not found in Google Drive: {args.plan}")
                    return_code = 1
                else:
                    plan = ImplementationPlan.from_drive_content(
                        args.plan,
                        plan_content.decode('utf-8')
                    )
                    
                    print(f"‚úÖ Loaded plan: {plan.implementation_id}")
                    
                    # Execute update
                    updater = TradingSystemUpdater(plan)
                    success = updater.execute_update()
                    return_code = 0 if success else 1
            else:
                print("‚ùå Google Drive service not available")
                return_code = 1
                
        elif args.continue_update or args.retry:
            # Continue or retry from saved state
            if GOOGLE_DRIVE_AVAILABLE:
                drive_service = get_drive_service()
                state = drive_service.load_json('.update_state.json', 'coordination')
                
                if not state:
                    # Try local file
                    state_file = Path('/content/trading_system/.update_state.json')
                    if state_file.exists():
                        with open(state_file, 'r') as f:
                            state = json.load(f)
                            
                if not state:
                    print("‚ùå No implementation in progress to continue")
                    return_code = 1
                else:
                    # Reload implementation plan
                    plan_content = drive_service.read_file(state['plan_name'], 'project_documentation')
                    if not plan_content:
                        print(f"‚ùå Original plan not found: {state['plan_name']}")
                        return_code = 1
                    else:
                        plan = ImplementationPlan.from_drive_content(
                            state['plan_name'],
                            plan_content.decode('utf-8')
                        )
                        
                        print(f"‚úÖ Resuming implementation: {plan.implementation_id}")
                        
                        # Continue from saved phase
                        updater = TradingSystemUpdater(plan)
                        
                        if args.retry and state['status'] == 'FAILED':
                            # Retry the failed phase
                            success = updater.execute_update(continue_from=state['current_phase'])
                        else:
                            # Continue from next phase
                            phases = ["Phase 1: Discovery", "Phase 2: Documentation", 
                                    "Phase 3: Preparation", "Phase 4: Implementation",
                                    "Phase 5: Testing", "Phase 6: Completion"]
                            current_index = phases.index(state['current_phase'])
                            if current_index < len(phases) - 1:
                                success = updater.execute_update(continue_from=phases[current_index + 1])
                            else:
                                print("‚úÖ Implementation already completed")
                                success = True
                                
                        return_code = 0 if success else 1
            else:
                print("‚ùå Google Drive service not available")
                return_code = 1
                
        elif args.rollback:
            # Rollback implementation
            state_file = Path('/content/trading_system/.update_state.json')
            if not state_file.exists():
                print("‚ùå No implementation in progress to rollback")
                return_code = 1
            else:
                # Create minimal plan for rollback
                plan = ImplementationPlan(
                    implementation_id="ROLLBACK",
                    plan_name="Rollback Operation",
                    date_created=datetime.now().strftime("%Y-%m-%d"),
                    phases=[],
                    files_to_update=[]
                )
                updater = TradingSystemUpdater(plan)
                success = updater.rollback()
                return_code = 0 if success else 1
                
        elif args.check_only:
            # Check for updates
            print("Checking for available updates...")
            # In production, this would scan for updates
            print("Check-only mode: Would scan project documentation for new implementation plans")
            print("No actual changes made")
            
        else:
            # No arguments provided
            parser.print_help()
            
    except Exception as e:
        print(f"‚ùå Critical error: {str(e)}")
        logging.exception("Critical error in main")
        return_code = 1
        
    sys.exit(return_code)

if __name__ == "__main__":
    main()

================================================================================
FILE: ./.devcontainer/devcontainer-config.json
================================================================================

{
  "name": "Trading Application",
  "image": "mcr.microsoft.com/devcontainers/python:3.11",
  "features": {
    "ghcr.io/devcontainers/features/python:1": {
      "version": "3.11"
    }
  },
  "forwardPorts": [5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5011, 8080],
  "portsAttributes": {
    "5000": {"label": "Coordination Service"},
    "5001": {"label": "Security Scanner"},
    "5002": {"label": "Pattern Analysis"},
    "5003": {"label": "Technical Analysis"},
    "5005": {"label": "Paper Trading"},
    "5006": {"label": "Pattern Recognition"},
    "5008": {"label": "News Service"},
    "5009": {"label": "Reporting Service"},
    "5011": {"label": "Trading Scheduler"},
    "8080": {"label": "Web Dashboard", "onAutoForward": "openBrowser"}
  },
  "postCreateCommand": "pip install -r requirements.txt && python setup_codespace.py",
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance",
        "ms-python.debugpy",
        "ms-toolsai.jupyter"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "python.linting.enabled": true,
        "python.linting.pylintEnabled": true,
        "python.formatting.provider": "black",
        "python.terminal.activateEnvironment": true
      }
    }
  },
  "remoteEnv": {
    "PYTHONPATH": "${containerWorkspaceFolder}"
  }
}
